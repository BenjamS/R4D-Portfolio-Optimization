---
title: "Risk-adjusted optimization of agricultural research for development portfolios"
author:
  - name: Benjamin Schiek
    email: b.schiek@cgiar.org
    affiliation: CIAT
address:
  - code: CIAT
    address: Alliance Bioversity-CIAT, Km 17 Recta Cali-Palmira, Valle del Cauca, Colombia
abstract: |
 Conventional agricultural research for development (AR4D) priority setting exercises have long been implemented to build insight and consensus around the strengths and weaknesses of individual agricultural research proposals in a given portfolio, but stop short of providing tools that can translate collective insights into optimal resource allocation shares. They also generally exclude from the allocation decision any rigorous accounting of the risk involved in each proposal, or of tradeoffs and synergies between proposals. These methodological lacunae have repeatedly exposed resource allocation processes to ad hoc, politically driven decisionmaking, thereby contributing to growing toxicity in donor-researcher relations. Here I explore the possibility of removing the methodological basis for this discord by introducing a rigorous allocation method, adapted from financial contexts, that allocates precise, optimal budget shares to each proposal based on its expected impact, risk, and synergies/tradeoffs with the other proposals in the portfolio.
keywords: Risk-adjusted portfolio optmization, Mean-Variance Analysis, AR4D, ex-ante impact assessment, foresight
journal: Alliance Bioversity-CIAT Working Paper
bibliography: AR4D Portfolio Optimization.bib
header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{H}
#linenumbers: true
numbersections: true
output:
  rticles::elsevier_article:
    latex_engine: xelatex
mainfont: Garamond
---
<!-- https://tug.org/FontCatalogue/ -->
<!-- Source Serif Pro Light -->
<!-- rmarkdown::draft("Test.Rmd", template = "elsevier_article", package = "rticles") -->
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE)
#devtools::install_github("thomasp85/patchwork")
library(plyr)
library(tidyverse)
library(zoo)
library(tidyquant)
library(patchwork)
```

# Introduction

## Mills' missing fifth step

Long ago, Bradford Mills articulated the four steps of priority setting in agricultural research for development (AR4D) as follows [-@Mills1998]:

>Step one is the development of an information base. Step two is the establishment of research alternatives. Step three is the evaluation of the potential impact of research alternatives. Step four, often considered the final output of a priority-setting exercise, is the ranking of alternatives into research priorities.

However, he went on to observe that these steps were insufficient. An as of yet non-existent

>fifth step, the development of guidelines for translating priorities into planning and resource allocation decisions, is necessary to establish direct links with planning and resource allocation activities [@Mills1998].

Mills identified this lacuna at a time when AR4D centers were just beginning to come under pressure from public and private donors to "do a lot more with a lot less" [@Alston1995]. Methodological work in response to this pressure focused on Steps 1-4. The emergence and refinement of numerous ex-ante impact assessment models for the evaluation of individual research alternatives, in particular, is remarkable (see, for example, @Alston1995; @Antle1999; @Antle2015; Mills, 1998; and @nelson2014modeling). However, work has still not begun on Mills' missing fifth step.

```{r, fig.show = 'hold', out.width="15cm", fig.align='left'}
knitr::include_graphics("Mills_missing_step5.png")
```
In hindsight, Mills' concern was well founded. Budgetary pressure on AR4D centers to "prove their relevance" [@Braunschweig2000], "show value for money" [@yet2016bayesian], and otherwise demonstrate "more efficient spending of resources" [@petsakos2018comparing] has only increased since the 1990s. Between Step 4 and the final allocation of funds, stakeholder politics, institutional inertia, ad hoc procedures and criteria, and other subjective forces have repeatedly undercut any careful, objective rigor occurring in Steps 1-4 [@Birner2016; @mccalla2014cgiar]. This inconsistency, opacity, and subjectivity in resource allocation decisions has, in turn, aggravated scientists' already longstanding distrust of budget allocation mechanisms. Relations between donors and the research programs they fund have, as a result, reached an historic level of toxicity [@Birner2016; @leeuwis2018reforming; @mccalla2014cgiar].

A rigorous tool _at the portfolio level_, along the lines suggested by Mills over twenty years ago, could go a long ways in guaranteeing the consistency, transparency, and objectivity that is, in large part, responsible for this toxicity. [has been lacking from the resource allocation process.] It would also be nice if such a method took explicit account of the risk associated with each research alternative, and were quick and inexpensive to implement relative to consensus building mechanisms such as the Analytical Hierarchy Process [@Braunschweig2000]. In this paper, I explore the possibility of building such a tool based on risk-adjusted portfolio optimization techniques developed in the financial context.

## AR4D mean-variance analysis

In particular, I explore the possibility of adapting Mean-Variance (MV) Analysis, first conceptualized by Markowitz [@markowitz1952portfolio] and later formalized and refined by Merton [@merton1972analytic], to the AR4D context. In stock market investment contexts, MV Analysis is used to optimize the investor's return on investment in a portfolio of $n$ risky assets, given the investor's level of risk tolerance. On input, MV Analysis takes the expected returns and risk of each asset in the portfolio, and outputs the precise resource allocation that must be invested in each of the portfolio assets in order to achieve the risk adjusted maximum return. The optimal solution changes with the investor's risk tolerance. The locus of all solutions across all levels of risk tolerance is called the "efficient frontier". In the conventional approach, the math works out such that this frontier takes the shape of a parabola (Figure \ref{fig:basic_illust}). Parabolae give two solutions for each x-axis input. The investor is of course interested only in the greater of these, given by the upper branch of the parabola. The lower branch is therefore omitted.

```{r, fig.show = 'hold', fig.width=4, fig.height=3, fig.align='center', fig.cap="\\label{fig:basic_illust}The efficient frontier. Each point on the frontier indicates the highest expected return that can be obtained for a given risk tolerance.", echo=FALSE}
y <- seq(0, 1, length.out = 50)
a <- 1 / 2
b <- 1 / 3
x <- a^2 * (y^2 / b^2 + 1)
gg <- ggplot(data.frame(x, y), aes(x = x, y = y))
gg <- gg + geom_point()
gg <- gg + labs(x = "Portfolio Risk (standard deviation)", y = "Portfolio expected return")
gg <- gg + theme(axis.text = element_blank(),
                 axis.ticks = element_blank())
gg

```

Of course, the AR4D context differs from the financial context in many important respects. In particular: 1) the data required to calculate a portfolio covariance matrix do not exist in the AR4D context. 2) Investments in AR4D projects do not scale linearly as they do in the stock market, but rather exhibit diminishing marginal returns. And 3) Budget shares must be positive (there is no AR4D analogue to short-selling).

<!-- _Scalability:_ In the financial context, returns scale linearly with investment. That is to say, the return on an investment of $\$200$ is twice as much as the return on an investment of $\$100$. Investments in the AR4D context, on the other hand, are subject to the law of diminishing marginal returns. Donors cannot double their impact merely by doubling their investments. Such returns are said to scale sub-linearly with investment. -->
<!-- [linear vs. subliear graphic?] -->

<!-- _Negative budget shares:_ To each point along the efficient frontier in Figure \ref{fig:basic_illust} corresponds an optimal budget allocation. MV Analysis allows that individual budget shares in this allocation can be, and frequently are, negative. A negative sign on a budget share indicates that the investor should invest in the inverse of the corresponding portfolio item. In the financial context, this is possible through short selling, or through investment in financial products that track the inverse of the given asset price.^[However, even in the financial context, negative weights are viewed by many as a methodological nuissance. See, for example, Boyle [-@boyle2014positive]. One of the main nuissances is that a portfolio with both negative and positive weights implies that the investor must borrow beyond their budget.] In the AR4D context, there is no analogue to either one of these conventions. -->

<!-- _Lack of a covariance matrix:_ MV Analysis requires information regarding the tradeoffs and synergies between portfolio items. Another word for these tradeoffs and synergies is covariance. A positive covariance may be considered a synergy, while a negative covariance is a tradeoff. In the financial context, covariances can be calculated from data. In the AR4D context, no such data exists. -->

However, there is a very important sense in which MV analysis _does_ reflect the kind of thinking that goes (or used to go) into AR4D bugdet allocation decisions. Emeritus scientists at the International Center for Tropical Agriculture, for example, recall that in the early years of the institution---around the time that Merton was writing his seminal paper---it was common practice to informally plot research proposals in a risk reward space such as that in Figure \ref{fig:basic_illust}, and to thereby guide resource allocation decisions [@JCock_perscomm; @lynam2017forever]. In this paper, I am primarily concerned with the possibility of resuscitating this practice, and of moving it onto a rigorous methodological footing. To achieve this, I propose an adaptation of MV analysis to the AR4D context, redressing each of the three methodological challenges enumerated in the previous paragraph. Before tackling these challenges, it is first necessary to walk through a brief introduction to MV analysis in its native financial context.


<!-- After a brief introduction to MV Analysis in the following section, I redress each of these issues. In Section 3, I redress both the scalability and negative budget share issues by replacing the linear returns function used in MV Analysis with a logarithmic form derived from the law of diminishing marginal returns. In Section 4, I draw on principal components analysis to show how a lack of data can be compensated by domain knowledge in order to "reverse engineer" a covariance matrix. The deduced matrix is useful for orienting stakeholder discussions, but cannot be used in portfolio optimization. To find the optimal risk adjusted resource allocation, I introduce the "signals portfolio", a dimensionally reduced portfolio of principal components. The optimal resource allocation to individual portfolio items can be disaggregated from the optimal signals portfolio based on each item's correlation with each given signal. Finally I walk through a an illustrative example of what this AR4D-adapted version of MV Analysis might look like in practice. -->
<!-- [These asjustments may be of interest to the financial context... The introduction of diminishing marginal returns, which forces positive budget weights, compares favorably to the unmodified approach on an ROI basis...and in a backtest.] -->
<!-- Alston and Norton acknowledged in 1995 that the treatment of risk in impact assessment models was "rudimentary and in need of further refinement" [-@Alston1995]. This remains true today. -->

# Brief introduction to risk adjusted portfolio optimization

In financial contexts, the goal of MV Analysis is to find the optimal investment across a portfolio of risky assets such that risk is minimized for a given budget constraint and portfolio return target. More formally, this can be expressed

\begin{equation}
\max_{\mathbf{w}}\:R - C \:\:\:\:s.t. \:\:\: C=\bar{C} \:, \:\:\: \sigma^2 = \bar{\sigma}^2
\end{equation}

where $\mathbf{w}$ is the vector of budget shares to be invested in each asset, $R$ is the expected portfolio return, defined $R = \mathbf{w} \cdot \boldsymbol{\mu}$, where $\boldsymbol{\mu}$ is the vector of expected returns of each asset; $C$ is the cost, defined $C = \mathbf{w} \cdot \mathbf{1}$, constrained to sum to the budget $\bar{C}$; and $\sigma^2$ is the portfolio variance (a measure of the portfolio risk), defined $\sigma^2 = \mathbf{w} \cdot \Sigma \cdot \mathbf{w}$, where $\Sigma$ is the asset covariance matrix; and $\sigma^2$ is constrained to equal a tolerance $\bar{\sigma}^2$ set by the investor.^[In the financial context, the problem is usually formulated as a risk minimization problem subject to a budget constraint and return target. Both approaches yield the same outputs. In the financial context, the problem includes the option to invest in one risk free asset. There is no risk free investment in the AR4D context, so the set up here focuses on optimization of the risky portfolio only.]

The Lagrangian is then

\begin{equation}
\mathcal{L} = R - C + \lambda_C(C - \bar{C}) + \lambda_{\sigma}(\sigma^2 - \bar{\sigma}^2)
\end{equation}

with first order conditions

\begin{equation}
\nabla \mathcal{L} = \boldsymbol{\mu} - \gamma \mathbf{1} + 2 \lambda_{\sigma} \Sigma \cdot \mathbf{w} = \mathbf{0} \:\:;\:\:\: \gamma = 1-\lambda_C
\end{equation}

and second order conditions

\begin{equation}
\mathbf{w} \cdot \nabla^2 \mathcal{L} \cdot \mathbf{w} = 2 \lambda_{\sigma} \sigma^2 < 0
\end{equation}

where $\nabla^2 \mathcal{L}$ is the Hessian matrix of $\mathcal{L}$. Since $\Sigma$ is symmetric and variances are, by definition, positive, then $\Sigma$ is positive semi-definite. Hence, a maximum is guaranteed so long as $\lambda_{\sigma}$ is negative.

Dotting the first order conditions through by $\mathbf{w}$ gives the equation for the efficient frontier.

\begin{equation}
R^* = \lambda_C \bar{C} + 2 \lambda_{\sigma} \bar{\sigma}^2
\label{eq:rStar}
\end{equation}

where the asterisk on $R$ indicates that this is the maximum portfolio return given budget constraint $\bar{C}$ and risk tolerance $\bar{\sigma}^2$.

Note, in passing, that equation \ref{eq:rStar} implies that the risk shadow price is proportional to the expected reward to risk ratio.

\begin{equation}
\lambda_{\sigma} = \frac{\psi}{2\bar{\sigma}^2} \: ; \:\:\: \psi = R^* - \gamma \bar{C}
\end{equation}

where $\psi = R^* - \gamma \bar{C}$ is the expected reward, i.e., the expected net revenue, adjusted by the budget shadow price $\gamma$.

Now, dotting the first order conditions through by $\Sigma^{-1}$ and rearranging gives an equation for the optimal budget shares.

\begin{equation}
\mathbf{w}^* = -\frac{1}{2 \lambda_{\sigma}} \Sigma^{-1} \cdot \nabla \psi \: ; \:\:\: \nabla \psi = \boldsymbol{\mu} - \gamma \mathbf{1}
\label{eq:budgetShares}
\end{equation}

Note, in passing, that dotting this through by $\nabla \psi$ gives another instructive equation for $\lambda_{\sigma}$.

\begin{equation}
\lambda_{\sigma} = \frac{d_m^2}{2 \psi} \: ; \:\:\:\:d_m^2 = \nabla \psi \cdot \Sigma^{-1} \cdot \nabla \psi
\end{equation}

This says that the risk shadow price is inversely proportional to the ratio of the squared Mahalanobis distance ($d_m^2$) of the portfolio net reward gradient to the net reward. In this setting, the Mahalanobis distance reflects how improbable a given portfolio is. Note, then, that portfolio improbability goes to zero (and hence probability is greater) as $\nabla \psi = \mathbf{0}$, which occurs only when every component in the vector of asset returns ($\boldsymbol{\mu}$) equals the budget shadow price ($\gamma$). Moreover, note that this can be combined with the previous expression for $\lambda_{\sigma}$ to obtain

\begin{equation}
{1 \over d_m}  &=& {\bar{\sigma} \over \psi}
\end{equation}

which says that the probability of the optimal portfolio equals its risk to reward ratio.

Returning to the task at hand, evaluation of the optimal budget shares or the frontier in equations \ref{eq:rStar} and \ref{eq:budgetShares}, requires first solving for the cost and risk shadow prices $\gamma$ and $\lambda_{\sigma}$.

To do this, first note that the budget shares equation can be rewritten as follows:

\begin{equation}
\mathbf{w}^* = -\frac{1}{2 \lambda_{\sigma}} \Sigma^{-1} \cdot [\boldsymbol{\mu}, -\mathbf{1}] \left[\begin{matrix} 1 \\ \gamma \end{matrix} \right]
\end{equation}

Now, dotting through by $[\boldsymbol{\mu}, -\mathbf{1}]$ gives

\begin{equation}
\left[\begin{matrix}
R \\
-\bar{C} \\
\end{matrix} \right]  = -\frac{1}{2 \lambda_{\sigma}} M\left[\begin{matrix}
1 \\
\gamma
\end{matrix} \right]
\end{equation}

where $M$ has been introduced to stand for the matrix

\begin{equation}
M = [\boldsymbol{\mu}, \: -\mathbf{1}]' \cdot \Sigma^{-1} \cdot [\boldsymbol{\mu}, \: -\mathbf{1}]
\end{equation}

Let $M$ be called the "Merton matrix", after the author in whose footsteps I am now following [@merton1972analytic]. Pre-multiplying both sides of the previous equation by the inverse Merton matrix and rearranging gives the following expression: 

\begin{equation}
-2 M^{-1}\left[\begin{matrix}
R \\
-\bar{C} \\
\end{matrix} \right]  = \left[\begin{matrix}
1/\lambda_V \\
\gamma / \lambda_{\sigma} \\
\end{matrix} \right]
\end{equation}

For any given return target $R$ and budget $\bar{C}$, then, the cost and risk shadow prices are given by this equation. With values for $\gamma$ and $\lambda_{\sigma}$ in hand, the budget shares and risk associated with the chosen return target can be evaluated.

The portfolio with the minimum possible risk can be found by differentiating the frontier (\ref{eq:rStar}) with respect to expected return and then setting $\frac{\partial \sigma}{\partial R}=0$. This gives the expected return for the portfolio with minimum possible risk. This corresponds to the portfolio at the vertex of the frontier parabola.

\begin{equation}
R\Bigr|_{\sigma = \sigma_{min}} = {m_{12} \over m_{22}}C
\end{equation}


```{r, fig.width=14, fig.height=14, fig.align='center', echo = FALSE, include = FALSE}
#options(warn = -1); options(scipen = 999)
spy_sector_symbs <- c("XLF", "XLC", "XLY", "XLP", "XLV", "XLK", "RWR",
                      "XLU", "XLI", "XBI", "IYT") #"TTEK"
spy_sector_detail <- c("Financials", "Communications", "Luxury goods", "Consumer\nGoods",
                       "Healthcare", "Technology", "Real estate", "Utilities", "Industrial",
                       "Biotechnology", "Transportation") #"Gov. foreign aid"
minerals_symbs <- c("GLD", "SLV", "PPLT", "JJC", "JJM") #"XME"
minerals_detail <- c("Gold", "Silver", "Platinum", "Copper", "Industrial metals") #"US metals and mining"
agriculture_symbs <- c("JJG", "BAL", "SOYB", "SGG", "JO", "NIB", "DBA")
agriculture_detail <- c("Grains", "Cotton", "Soybean", "Sugar", "Coffee", "Cacao", "General agriculture")
energy_symbs <- c("WTI", "FUE", "WOOD", "ICLN", "KOL", "UNG")
energy_detail <- c("Oil (W&T Offshore Inc.)", "Biofuels", "Timber", "Clean energy", "Coal", "US natural gas")
#currency_symbs <- c("EMLC", "UUP", "FXE", "FXY", "FXF", "FXC", "FXB", "FXA")
#currency_detail <- c("Emerging mkt currencies", "USD", "EUR", "JPY", "CHF", "CND", "GBP", "AUD")
currency_symbs <- c("EURUSD=X", "JPY=X", "CHF=X", "CAD=X",
                    "GBPUSD=X", "AUDUSD=X", "INR=X")
currency_detail <- c("EUR/USD", "USD/JPY",
                     "USD/CHF", "USD/CAD", "GBP/USD", "AUD/USD", "USD/INR")
emerg_mkt_symbs <- c("EMLC", "ELD", "BKF", "VWOB")
emerg_mkt_detail <- c("Emerg mkts debt", "Emerg mkts gov. bonds", "BRIC countries", "Emerging mkt currencies")
crypto_symbs <- c("BLOK", "LEGR", "BCNA")
crypto_detail <- c("Blockchain tech.", "Blockchain companies", "Blockchain in China")
Tbond_symbs <- c("IEI", "IEF", "TLT")#, "BIL"
Tbond_detail <- c("T-bond 3-7 yrs", "T-bond 7-10 yrs", "T-bond 20+ yrs") #"T-bond 1-3 months"

ts_symb_vec <- c(spy_sector_symbs, minerals_symbs, agriculture_symbs, energy_symbs,
                 currency_symbs, emerg_mkt_symbs, crypto_symbs, Tbond_symbs)
ts_detail_vec <- c(spy_sector_detail, minerals_detail, agriculture_detail, energy_detail,
                   currency_detail, emerg_mkt_detail, crypto_detail, Tbond_detail)
df_symb_detail <- data.frame(symbol = ts_symb_vec, detail = ts_detail_vec)
#--
#length(ts_symb_vec)
#--
# list_groups <- list(spy_sector_symbs, minerals_symbs, agriculture_symbs, energy_symbs,
#                      currency_symbs, emerg_mkt_symbs, crypto_symbs, Tbond_symbs)
#-------------------------------------------
# Create grouping info for plots (required input into functions)
list_groups <- list(spy_sector_detail, minerals_detail, agriculture_detail, energy_detail,
                    currency_detail, emerg_mkt_detail, Tbond_detail) #crypto_detail,
group_names <- c("US Sectors", "Minerals", "Agriculture", "Energy", "Major Currency Pairs",
                 "Emerging Markets", "T-Bonds") #"Cryptocurrencies/\nBlockchain"
n_groups <- length(list_groups)
#group_colors <- RColorBrewer::brewer.pal(n = n_groups, "Dark2")
# "Darjeeling"
# group_colors <- wesanderson::wes_palette("Darjeeling1", n = n_groups, type = "continuous")
bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_groups)
group_colors <- sample(bag_of_colors, n_groups)
#--------------------------------------------------------------
# Define function to order data by group
group_fn <- function(group_info){
  list_groups <- group_info[[1]]
  group_names <- group_info[[2]]
  group_colors <- group_info[[3]]
  varNames_ordered <- do.call(c, list_groups)
  n_groups <- length(group_names)
  n_items <- length(varNames_ordered)
  if(is.na(group_colors)){
    bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_groups)
    group_colors <- sample(bag_of_colors, n_groups)
    #group_colors <- viridis::viridis_pal(option = "D")(length(group_names))
  }
  #if(reverse_order){group_colors <- rev(group_colors)}
  #varNames_ordered <- colnames(mat_pctDiff)
  group_vec <- rep(NA, n_items)
  group_color_vec <- rep(NA, n_items)
  for(i in 1:n_groups){
    this_group_vec <- list_groups[[i]]
    this_group_name <- group_names[i]
    this_group_color <- group_colors[i]
    group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
    group_color_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_color
  }
  ind_ordered_cols <- order(factor(group_vec))
  cols_ordered_by_group <- as.character(varNames_ordered[ind_ordered_cols])
  group_color_vec <- group_color_vec[ind_ordered_cols]
  group_vec_ordered <- group_vec[ind_ordered_cols]
  out_list <- list(cols_ordered_by_group, group_color_vec, group_vec_ordered, ind_ordered_cols, group_vec)
  return(out_list)
}
#--------------------------------------------------------------
group_info <- list(list_groups, group_names, group_colors)
  outlist <- group_fn(group_info)
  cols_ordered_by_group <- outlist[[1]]
  group_color_vec <- outlist[[2]]
  group_vec_ordered <- outlist[[3]]
  ind_ordered_cols <- outlist[[4]]
  df_match_group <- data.frame(Item = cols_ordered_by_group, Group = group_vec_ordered)
#--------------------------------------------------------------
#per_ema <- 13
#fromdate = Sys.Date() - 360
fromdate = "2018-06-20"
todate = "2019-08-09"
tbl_ohlcv <- tq_get(ts_symb_vec, get = "stock.prices", from = fromdate, to = todate)
df_ohlcv <- as.data.frame(tbl_ohlcv)
#write.csv(df_ohlcv, "Financial data backup.csv")
#--
# Crypto ts can have duplicate date entries sometimes (BTC-USD)
# Get rid of these duplicates, if any
# df_ohlcv <- as.data.frame(df_ohlcv %>% group_by(symbol) %>% mutate(dup = duplicated(date)))
# df_ohlcv <- subset(df_ohlcv, dup == F)
# df_ohlcv$dup <- NULL
#--

#df_ohlcv$p <- rowSums(df_ohlcv[, c(4:6)]) / 3
#--
#df <- df_ohlcv[, c("date", "symbol", "p")]
# df <- df_ohlcv %>% group_by(symbol) %>% tq_transmute(select = adjusted, 
#                      mutate_fun = periodReturn, 
#                      period     = "monthly")
#--
df <- df_ohlcv[, c("date", "symbol", "adjusted")]
df <- df %>% spread(symbol, adjusted)
ind_shift <- which(colnames(df) %in% currency_symbs)
df[, ind_shift] <- rbind(rep(NA, length(ind_shift)), df[-nrow(df), ind_shift])
df <- df[-c(1, nrow(df)), ]
date_vec <- df$date
mat_ts_dy <- na.approx(df[, -1])
row.names(mat_ts_dy) <- as.character(date_vec)
#o <- apply(mat_ts_dy, 2, function(x) length(which(is.na(x))))
#table(o)
#which(o==1)
#xts_ts_dy <- xts(mat_ts_dy, date_vec)
#date_vec <- index(xts_ts_dy)
#-----
mat_pctDiff_dy <- diff(mat_ts_dy) / mat_ts_dy[-nrow(mat_ts_dy), ] * 100
#mat_pctDiff_dy <- diff(log(mat_ts_dy))
row.names(mat_pctDiff_dy) <- as.character(date_vec[-1])
ts_avg_dy <- rowMeans(mat_pctDiff_dy)
mu_ret_dy <- colMeans(mat_pctDiff_dy)
sd_ret_dy <- apply(mat_pctDiff_dy, 2, sd)

mat_ts_wk <- to.weekly(xts(mat_ts_dy, date_vec), OHLC = F)
mat_pctDiff_wk <- diff(mat_ts_wk) / mat_ts_wk[-nrow(mat_ts_wk), ] * 100
mat_pctDiff_wk <- as.matrix(mat_pctDiff_wk[-1, ])
ts_avg_wk <- rowMeans(mat_pctDiff_wk)
mu_ret_wk <- colMeans(mat_pctDiff_wk)
sd_ret_wk <- apply(mat_pctDiff_wk, 2, sd)

mat_ts_mo <- to.monthly(xts(mat_ts_dy, date_vec), OHLC = F)
mat_pctDiff_mo <- diff(mat_ts_mo) / mat_ts_mo[-nrow(mat_ts_mo), ] * 100
mat_pctDiff_mo <- as.matrix(mat_pctDiff_mo[-1, ])
ts_avg_mo <- rowMeans(mat_pctDiff_mo)
mu_ret_mo <- colMeans(mat_pctDiff_mo)
sd_ret_mo <- apply(mat_pctDiff_mo, 2, sd)
#----------------------------------------------
# mu_pctRet <- apply(10^-2 * mat_pctDiff_dy, 2, function(x) prod(1 + x)) - 1
# mu_pctRet_check <- (mat_ts_dy[nrow(mat_ts_dy), ] - mat_ts_dy[1, ]) / mat_ts_dy[1, ]
# mu_pctRet / mu_pctRet_check
#----------------------------------------------
# Compare histograms of daily, weekly, and monthly returns
# df_plot <- data.frame(ts_id = names(mu_ret_dy), mu_ret_dy, mu_ret_wk, mu_ret_mo)
# colnames(df_plot)[-1] <- c("Mean Daily Return", "Mean Weekly Return", "Mean Monthly Return")
# gathercols <- colnames(df_plot)[-1]
# df_plot <- df_plot %>% gather_("Return Type", "Value", gathercols)
# #gg <- ggplot(df_plot, aes(Value, fill = `Return Type`))
# gg <- ggplot(df_plot, aes(Value))
# gg <- gg + geom_density(alpha = 0.4)
# gg <- gg + facet_wrap(~`Return Type`, nrow = 1, scales = "free")
# #gg <- gg + coord_cartesian(xlim = c(-0.02, 0.02))
# gg
#----------------------------------------------
mat_ts <- mat_ts_dy
colnames(mat_ts) <- as.character(df_symb_detail$detail[order(df_symb_detail$symbol)])
#----------------------------------------------
mat_pctDiff <- mat_pctDiff_dy
#mat_pctDiff <- mat_pctDiff_wk
date_vec <- row.names(mat_pctDiff)
#length(date_vec)
row.names(mat_pctDiff) <- as.character(date_vec)
colnames(mat_pctDiff) <- as.character(df_symb_detail$detail[order(df_symb_detail$symbol)])
#----------------------------------------------
check_on_data <- F
if(check_on_data){
  df_plot <- as.data.frame(mat_pctDiff)
  gathercols <- colnames(df_plot)
  df_plot$Date <- date_vec
  df_plot <- df_plot %>% gather_("Security", "Weekly change (%)", gathercols)
  df_plot$Type <- NA
  df_plot$Type[which(df_plot$Security %in% spy_sector_detail)] <- "US Sectors"
  df_plot$Type[which(df_plot$Security %in% minerals_detail)] <- "Minerals"
  df_plot$Type[which(df_plot$Security %in% agriculture_detail)] <- "Agriculture"
  df_plot$Type[which(df_plot$Security %in% energy_detail)] <- "Energy"
  df_plot$Type[which(df_plot$Security %in% currency_detail)] <- "Major Currency Pairs"
  df_plot$Type[which(df_plot$Security %in% emerg_mkt_detail)] <- "Emerging Markets"
  df_plot$Type[which(df_plot$Security %in% crypto_detail)] <- "Cryptocurrencies/\nBlockchain"
  df_plot$Type[which(df_plot$Security %in% Tbond_detail)] <- "T-bonds"
  #------------------------------------------------------------
  # gg <- ggplot(df_plot, aes(x = Year, y = Value, group = Security, color = Security))
  # gg <- gg + geom_line()
  # gg <- gg + facet_wrap(~ Type, nrow = 3, scales = "free")
  # gg <- gg + theme(legend.position = "none")
  # gg
  #------------------------------------------------------------
  this_type <- "US Sectors"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Minerals"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Agriculture"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Energy"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Major Currency Pairs"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Emerging Markets"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Cryptocurrencies/Blockchain"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "T-bonds"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
}

```


```{r, fig.show = 'hold', fig.cap="\\label{fig:hReturns}Historical (top) daily returns and (bottom) period return, financial data.", fig.width=6, fig.height=6, fig.align='left', echo=FALSE}
#=======================================================================
# Define function to plot period returns as barchart with colors
plot_returns_barchart <- function(df_pctRet, group_colors, list_graph_options, graph_on = T){
  if(is.null(list_graph_options)){
    fig_title <- NULL
    legend_position <- NULL
    ylab <- NULL
  }else{
    fig_title <- list_graph_options[["fig_title"]]
    legend_position <- list_graph_options[["legend_position"]]
    ylab <- list_graph_options[["ylab"]]
  }

  if(is.null(group_colors)){
    n_groups <- length(unique(df_pctRet$Group))
    bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_groups)
    group_colors <- sample(bag_of_colors, n_groups)
  }
  
  colnames(df_pctRet)[1] <- "Period Return"
  df_plot <- df_pctRet
  gg <- ggplot(df_plot, aes(x = Item, y = `Period Return`, fill = Group))
  #gg <- gg + scale_color_brewer(palette = "Dark2")
  gg <- gg + scale_fill_manual(values = group_colors)
  gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
  if("Dataset" %in% colnames(df_plot)){
      gg <- gg + facet_wrap(~Dataset, ncol = 1)
  }
  if(!is.null(fig_title)){
      gg <- gg + labs(title = fig_title)
gg <- gg + theme(plot.title = element_text(face = "bold", size = 9))
  }
  if(!is.null(ylab)){
    gg <- gg + labs(y = ylab)
  }else{
    gg <- gg + theme(axis.title.y = element_blank())
  }
  if(!is.null(legend_position)){
    gg <- gg + theme(legend.position = legend_position)
  }else{
    gg <- gg + theme(legend.position = "bottom")
  }
  gg <- gg + theme(axis.text.x = element_text(size = 9, angle = 60, hjust = 1),
                   #axis.text.x = element_blank(),
                   axis.title.x = element_blank(),
                   axis.text.y = element_text(size = 9),
                   #axis.title.y = element_text(size = 9),
                   legend.title = element_blank(),
                   legend.text = element_text(size = 9))
  #  gg <- gg + coord_equal()
  #  gg <- gg + coord_flip()
  if(graph_on){print(gg)}
  return(gg)

}

#=======================================================================
# Define function to plot daily returns box and whiskers chart 
# plot_boxchart <- function(df_pctDiff, group_colors, list_graph_options, graph_on = T){
#   if(is.null(list_graph_options)){
#     legend_position <- "right" #"none"
#     axis_titles <- "on" #"off"
#     axis_text <- "on" #"y only"
#     fig_title <- NULL
#   }else{
#     legend_position <- list_graph_options[["legend_position"]]
#     axis_titles <- list_graph_options[["axis_titles"]]
#     axis_text <- list_graph_options[["axis_text"]]
#     fig_title <- list_graph_options[["fig_title"]]
#   }
#   
#   df_plot <- df_pctDiff
#     gg <- ggplot(df_plot, aes(x = Item, y = `Daily Returns`, fill = Group))
#   #gg <- gg + scale_color_brewer(palette = "Dark2")
#   gg <- gg + scale_fill_manual(values = group_colors)
#   #gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
#   gg <- gg + geom_boxplot()
#   if("Dataset" %in% colnames(df_plot)){
#     gg <- gg + facet_wrap(~Dataset, ncol = 1)
#   }
#   if(!is.null(fig_title)){
#     gg <- gg + labs(title = fig_title)
#     gg <- gg + theme(plot.title = element_text(face = "bold", size = 9))
#   }
#   if(axis_titles == "off"){
#     gg <- gg + theme(axis.title = element_blank())
#   }
#   if(axis_text == "y only"){
#     gg <- gg + theme(axis.text.x = element_blank(),
#                      axis.text.y = element_text(size = 9))
#   }
#   if(axis_text == "on"){
#     gg <- gg + theme(axis.text.y = element_text(size = 9),
#                      axis.text.x = element_text(size = 9, angle = 60, hjust = 1))
#   }
#     gg <- gg + theme(legend.position = legend_position)
#     if(graph_on){print(gg)}
#     return(gg)
# 
# }
#=======================================================================
# Define function to order data frames by group for period returns barchart and daily returns box and whiskers plots
get_hist_dfs_ready <- function(mat_ts_in, group_info){
  #------------------------------------------------------------
  # Group the vars according to provided group info
  #varNames_ordered <- do.call(c, group_info[[1]])
  outlist <- group_fn(group_info)
  cols_ordered_by_group <- outlist[[1]]
  group_color_vec <- outlist[[2]]
  group_vec_ordered <- outlist[[3]]
  ind_ordered_cols <- outlist[[4]]
  df_match_group <- data.frame(Item = cols_ordered_by_group, Group = group_vec_ordered)
  mat_ts_ordered <- mat_ts_in[, cols_ordered_by_group]
  #------------------------------------------------------------
  # Get date info
  from_date <- row.names(mat_ts_ordered)[1]
  to_date <- row.names(mat_ts_ordered)[nrow(mat_ts_ordered)]
  from_date <- gsub("-", "/", from_date)
  to_date <- gsub("-", "/", to_date)
  date_interval <- paste(from_date, to_date, sep = " - ")
  #------------------------------------------------------------
  # Get returns barchart df organized by group
  nab_pctRet <- 100 * (mat_ts_ordered[nrow(mat_ts_ordered), ] - mat_ts_ordered[1, ]) / mat_ts_ordered[1, ]
  #---
  # check
  mat_pctDiff <- 100 * diff(mat_ts) / mat_ts[-nrow(mat_ts), ]
  nab_pctRet_check <- 100 * apply(10^-2 * mat_pctDiff, 2, function(x) prod(1 + x)) - 1
  nab_pctRet / nab_pctRet_check
  #---
  df_pctRet <- data.frame(Returns = nab_pctRet)
  df_pctRet$Item <- row.names(df_pctRet)
  df_pctRet$Group <- factor(group_vec_ordered)
  df_pctRet$Item <- factor(df_pctRet$Item, levels = cols_ordered_by_group)
  #------------------------------------------------------------
  # Get boxplot df organized by group
    # df_pctDiff <- as.data.frame(mat_pctDiff_ordered)
    # df_pctDiff <- df_pctDiff %>% gather_("Item", "Daily Returns", colnames(df_pctDiff))
    # df_match_group <- data.frame(Item = cols_ordered_by_group, Group = group_vec_ordered)
    # df_pctDiff <- merge(df_pctDiff, df_match_group, by = "Item")
    # df_pctDiff$Item <- factor(df_pctDiff$Item, levels = cols_ordered_by_group)
    # df_pctDiff$Group <- factor(df_pctDiff$Group)
  #------------------------------------------------------------
  out_list <- list(df_pctRet, date_interval)
  return(out_list)
}


#=======================================================================
# Define function to plot covariance/correlation matrices
plot_covmat <- function(covmat, fig_title = "Covariance Matrix", graph_on = T){
  covmat[upper.tri(covmat)] <- NA
  df_plot <- covmat %>% tbl_df()
  these_levels <- colnames(df_plot)
  df_plot$ItemX <- colnames(df_plot)
  gathercols <- colnames(df_plot)[-ncol(df_plot)]
  df_plot <- df_plot %>% gather_("ItemY", "Value", gathercols)
  df_plot$ItemX <- factor(df_plot$ItemX, levels = these_levels)
  df_plot$ItemY <- factor(df_plot$ItemY, levels = these_levels)

  #midpoint <- min(covmat) + (max(covmat) - min(covmat)) / 2
  midpoint <- 0
  gg <- ggplot(df_plot, aes((ItemX), (ItemY)))
  gg <- gg + geom_tile(aes(fill = Value))#, width = 4, height = 4)
  gg <- gg + geom_text(aes(label = round(Value, 2)), size = 3)
  if(!is.null(fig_title)){
    gg <- gg + labs(title = fig_title)
  }
  gg <- gg + theme_bw()
  gg <- gg + theme(axis.text.x = element_text(angle = 60, hjust = 1),
                 axis.title = element_blank(),
                 legend.title = element_blank(),
                 plot.title = element_text(face = "bold", size = 9))
  gg <- gg + scale_fill_gradient2(low = "khaki", mid = "cyan", high = "magenta", midpoint, na.value = "white")
  if(graph_on){print(gg)}
  return(gg)

}
#=======================================================================
# End function definition
#=======================================================================
# Separate out a reduced matrix for the basic MV Analysis demonstration
# group_names
demonstration_group <- c("US Sectors")
ind_group <- which(group_names %in% demonstration_group)
these_items <- do.call(c, list_groups[ind_group])
ind_keep <- which(colnames(mat_ts) %in% these_items)
mat_ts_eg <- mat_ts[, ind_keep]
#-----------------------------------------------------------------------
# For a good illustrative example of portfolio optimization, select a financial period over which the returns in the portfolio are as varied as possible yet with the minimum return being >= 2%. This can be achieved by adjusting "buffer". Also'd be nice to have all signal period returns > 0, but this does not seem possible in this data set.
#-----------------------------------------------------------------------
# ind_train <- 1:round(nrow(mat_pctDiff) * 1 / 2)
# ind_test <- setdiff(1:nrow(mat_pctDiff), ind_train)
length_Q <- 67 # Length of a financial quarter (3 months)
buffer <- 5#36 # Select to give high cv of returns but with min return >=1-2%
#------------------------------------------------------------
# min_pctRet <- c()
# cv_pctRet <- c()
# buffr <- c()
# n_negRet_sigs_train <- c()
# n_negRet_sigs_test <- c()
# for(i in 1:110){
# buffer <- 2+i
#------------------------------------------------------------
ind_train <- (nrow(mat_ts_eg) - 2 * length_Q - buffer + 1):(nrow(mat_ts_eg) -  length_Q - buffer)
ind_test <- (nrow(mat_ts_eg) - length_Q - buffer + 1):(nrow(mat_ts_eg) - buffer)
mat_ts_eg_train <- mat_ts_eg[ind_train, ]
mat_ts_eg_test <- mat_ts_eg[ind_test, ]
  #------------------------------------------------------------
  # Get returns barchart df organized by group
  nab_decRet_eg_train <- (mat_ts_eg_train[nrow(mat_ts_eg_train), ] - mat_ts_eg_train[1, ]) / mat_ts_eg_train[1, ]
  nab_decRet_eg_test <- (mat_ts_eg_test[nrow(mat_ts_eg_test), ] - mat_ts_eg_test[1, ]) / mat_ts_eg_test[1, ]
  nab_pctRet_eg_train <- 100 * nab_decRet_eg_train
  nab_pctRet_eg_test <- 100 * nab_decRet_eg_test
  #---
  # check
  # mat_pctDiff <- 100 * diff(mat_ts_eg_train) / mat_ts_eg_train[-nrow(mat_ts_eg_train), ]
  # nab_pctRet_check <- 100 * (apply(10^-2 * mat_pctDiff, 2, function(x) prod(1 + x)) - 1)
  # nab_pctRet / nab_pctRet_check
  #------------------------------------------------------------
mat_decDiff_eg_train <- diff(mat_ts_eg_train) / mat_ts_eg_train[-nrow(mat_ts_eg_train), ]
mat_decDiff_eg_test <- diff(mat_ts_eg_test) / mat_ts_eg_test[-nrow(mat_ts_eg_test), ]
mat_pctDiff_eg_train <- 100 * mat_decDiff_eg_train
mat_pctDiff_eg_test <- 100 * mat_decDiff_eg_test
covmat_decDiff_train <- cov(mat_decDiff_eg_train)
covmat_decDiff_test <- cov(mat_decDiff_eg_train)
covmat_pctDiff_train <- cov(mat_pctDiff_eg_train)
covmat_pctDiff_test <- cov(mat_pctDiff_eg_train)
#------------------------------------------------------------
# mat_P <- eigen(covmat_decDiff_train)$vectors
# mat_S <- mat_decDiff_eg_train %*% mat_P
# mat_P_test <- eigen(covmat_decDiff_test)$vectors
# mat_S_test <- mat_decDiff_eg_test %*% mat_P_test
# nab_decRet_sigs_train <- (apply(mat_S, 2, function(x) prod(1 + x)) - 1)
# nab_decRet_sigs_test <- (apply(mat_S_test, 2, function(x) prod(1 + x)) - 1)
# n_negRet_sigs_train[i] <- length(which(nab_decRet_sigs_train[1:4] < 0))
# n_negRet_sigs_test[i] <- length(which(nab_decRet_sigs_test[1:4] < 0))
# min_pctRet[i] <- min(nab_pctRet)
# cv_pctRet[i] <- sd(nab_pctRet) / mean(nab_pctRet)
# buffr[i] <- buffer
# }
# View(data.frame(buffr, min_pctRet, cv_pctRet, n_negRet_sigs_train, n_negRet_sigs_test))
#----------------------------------------------------------
train_start_date <- row.names(mat_pctDiff_eg_train)[1]
train_stop_date <- row.names(mat_pctDiff_eg_train)[nrow(mat_pctDiff_eg_train)]
test_start_date <- row.names(mat_pctDiff_eg_test)[1]
test_stop_date <- row.names(mat_pctDiff_eg_test)[nrow(mat_pctDiff_eg_test)]
#----------------------------------------------------------

```
An example application of MV Analysis is presented below using daily financial data for `r ncol(mat_pctDiff_eg_train)` securities over the period `r train_start_date` to `r train_stop_date`, downloaded from yahoo finance using the R tidyquant package. These securities were chosen so as to be broadly representative of the U.S. economy. Details can be found in the appendix.

<!-- They are mostly exchange traded funds tracking broad categories of stocks, bonds, markets, and commodities, plus major currency pairs traded on the foreign exchange market. -->

<!-- [Table here detailing the ETF names, what they track, and my category name] -->

The expected returns $\boldsymbol{\mu}$ are calculated from the data as

$$
\mu_i = \frac{p_{iT} - p_{i1}}{p_{i1}}
$$
where $\mathbf{p}_i$ is the $i^{th}$ asset's price time series, and $T$ is the length of the time series. These period returns are presented in the bottom panel of Figure \ref{fig:hReturns}.

The covariance matrix $\Sigma$ is calculated from the data as

$$
\Sigma=\frac{1}{1 - n} X'X \:\:; \:\:\:\:\:\:X_{it} = r_{it}-\bar{r} \:, \:\:\:\:\: \bar{r} = \frac{1}{T}\sum_{t=1}^Tr_{it}
$$
where $n$ is the number of assets, and $r_{it}$ is the $i^{th}$ asset's daily return, calculated
$$
r_{it} = \frac{p_{it} - p_{it-1}}{p_{it-1}}
$$
for any given day $t$ in the series. As a practical matter, note that the period returns can also be calculated from the daily returns as follows.
$$
\mu_i = \prod_{t = 2}^T (1 + r_{it}) - 1
$$
Application of MV Analysis to this data yields the optimal frontier and budget shares displayed in Figure \ref{fig:mvConv}.

```{r, fig.show = 'hold', fig.cap="\\label{fig:hReturns}Historical (top) daily returns and (bottom) period return, financial data.", fig.width=6, fig.height=6, fig.align='left', echo=FALSE}
# Plot historical returns for period and items defined above
fig_title <- "Period Returns (%)"
list_graph_options <- list()
list_graph_options[["fig_title"]] <- fig_title
list_graph_options[["legend_position"]] <- "none"
list_graph_options[["ylab"]] <- NULL
df_pctRet <- data.frame(Value = nab_pctRet_eg_train, Item = names(nab_pctRet_eg_train), Group = demonstration_group)
gg_barchart <- plot_returns_barchart(df_pctRet, group_colors, list_graph_options, graph_on = F)
#----------------------------------------------------------
# Covariance matrix plot
gg_covmat <- plot_covmat(covmat_pctDiff_train, graph_on = F)
#----------------------------------------------------------
#gg_boxplot + gg_barchart + plot_layout(ncol = 1, heights = c(2, 1))
gg_barchart + gg_covmat + plot_layout(ncol = 1, heights = c(1, 2))
#----------------------------------------------------------


```

\pagebreak


```{r, fig.show = "hold", fig.width = 5, fig.height=5, fig.align='left', fig.cap="\\label{fig:mvConv}Optimal portfolio returns frontier and budget shares, financial data.", echo = FALSE}
#=======================================================================
# Define portfolio optimization function
optimize_portfolio <- function(M_inv, covmat_inv, covmat, mat_nab, targ_vec,
                               backtest_info = NULL){
  #print(M)
  lambdas <- -2 * M_inv %*% targ_vec
  # Return shadow price
  l_R <- lambdas[1]
  # Budget shadow price
  l_C <- lambdas[2]
  # Optimal budget shares
  wStar <- -1 / 2 * covmat_inv %*% mat_nab %*% lambdas
  #print(sum(wStar))
  # Portfolio variance
  Vtarg <- t(wStar) %*% covmat %*% wStar
  # Rtarg <- targ_vec[1]
  Rtarg <- t(wStar) %*% mat_nab[, 1]
  #----------------------------------------------------
  # check <- t(lambdas) %*% targ_vec + 2*l_V * V
  # print(check)
  #----------------------------------------------------
  sd_targ <- sqrt(Vtarg)
  #if(l_V > 0){wStar <- rep(NA, length(wStar))}
  #----------------------------------------------------
  if(!is.null(backtest_info)){
    nab_decRet_test <- backtest_info[["nab_decRet_test"]]
    covmat_test <- backtest_info[["covmat_test"]]
    Rtest <- t(wStar) %*% nab_decRet_test
    Vtest <- t(wStar) %*% covmat_test %*% wStar
    sd_test <- sqrt(Vtest)
  }else{
    Rtest <- NA
    Vtest <- NA
    sd_test <- NA
  }
  #----------------------------------------------------
  frontier_vec <- c(Rtarg, sd_targ, l_R, l_C, Rtest, sd_test)
  list_out <- list(wStar, frontier_vec)
  return(list_out)
}
#=======================================================================
# Define function for budget shares plot
plot_budgetShares <- function(df_wStar, group_small = NULL, color_vec = NULL, graph_on = T, list_graph_options = NULL){
    # Budget shares plot
  # Note df_w_in should have just the target risk column (i.e. don't also include the backtest risk column). At any rate, the budget risk column should be the same as the one used in the frontier plot.
  #df_plot <- df_wStar_prop
  #------------------------------------
  if(!is.null(list_graph_options)){
    legend_position <- list_graph_options[["legend_position"]]
    fig_title <- list_graph_options[["fig_title"]]
    axis_titles <- list_graph_options[["axis_titles"]]
    Xaxis_numbers_off <- list_graph_options[["Xaxis_numbers_off"]]
  }else{
      legend_position = "bottom"
      fig_title = NULL
      axis_titles = "on"
      Xaxis_numbers_off = F
  }
  
  #------------------------------------
  df_plot <- df_wStar
  gathercols <- colnames(df_plot)[-1]
  #------------------------------------
  if(!is.null(group_small)){
    mat_plot <- as.matrix(df_plot[, -1])
    mu_vec <- apply(mat_plot, 2, mean)
    ind_group_small <- which(mu_vec < 10^-2)
    other_col <- rowSums(mat_plot[, ind_group_small])
    
    mat_plot <- mat_plot[, -ind_group_small]
    df_plot <- as.data.frame(mat_plot)
    df_plot$Other <- other_col
    df_plot$`Risk (standard deviation)` <- df_wStar$`Risk (standard deviation)`
    gathercols <- colnames(df_plot)[-ncol(df_plot)]
  }
  #------------------------------------
  df_plot$portfolio_id <- 1:nrow(df_wStar)
  df_match_V <- df_plot[, c("portfolio_id", "Risk (standard deviation)")]
  df_plot <- df_plot %>% gather_("Item", "Budget shares", gathercols)
  df_plot <- df_plot %>% group_by(Item) %>% 
    mutate(mu = median(`Budget shares`)) %>% 
    as.data.frame()
  df_plot <- df_plot[order(df_plot$mu, decreasing = T), ]
  #ind_order_mu <- order(df_plot$mu, df_plot$Item, decreasing = T)
  df_plot$Item <- factor(df_plot$Item,
                         levels = unique(df_plot$Item),
                         ordered = T)
    #------------------------------------
if(is.null(color_vec)){
  # Randomly assign a color to each portfolio item if none assigned
  n_items <- ncol(df_wStar) - 1
    bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_items)
    color_vec <- sample(bag_of_colors, n_items)
}
    #------------------------------------
#  df_plot$`Risk (standard deviation)` <- as.factor(df_plot$`Risk (standard deviation)`)
  gg <- ggplot(df_plot, aes(x = `Risk (standard deviation)`, y = `Budget shares`, fill = Item))
  #legend_position <- "right"
  gg <- gg + geom_area(position = "stack")
 # gg <- gg + geom_bar(stat = "identity")
  gg <- gg + scale_fill_manual(values = color_vec)

  gg <- gg + theme(legend.title = element_blank(),
                   legend.position = legend_position,
                   legend.text = element_text(size = 8),
                   axis.title = element_text(size = 8),
                   axis.text = element_text(size = 8)
                   )
  if(axis_titles == "off"){
    gg <- gg + theme(axis.title = element_blank())
  }
  if(axis_titles == "x only"){
    gg <- gg + theme(axis.title.y = element_blank())
  }
  if(Xaxis_numbers_off){
    gg <- gg + theme(axis.text.x = element_blank())
  }
  if(!is.null(fig_title)){
    gg <- gg + labs(title = fig_title)
    gg <- gg + theme(plot.title = element_text(size = 8))
  }
  if(length(unique(df_plot$Item)) > 15){
    gg <- gg + theme(legend.position = "none")
  }
  gg_weights <- gg
  
  if(graph_on){print(gg)}
  
  return(gg_weights)
  
}
#=========================================================
plot_frontier <- function(df_frontier, ROI_basis = T, list_graph_options = NULL, graph_on = T){
  if(!is.null(list_graph_options)){
    fig_title <- list_graph_options[["fig_title"]]
  }else{
    fig_title <- NULL
  }
  
    df_plot <- df_frontier
  # if(ncol(df_plot) == 3){
  #   gathercols <- colnames(df_plot)[2:3]
  #   df_plot <- df_plot %>% gather_("Type", "Value", gathercols)
  #   gg <- ggplot(df_plot, aes(x = `Risk (standard deviation)`, y = Value, group = Type, color = Type))
  # }else{
  #   gg <- ggplot(df_plot, aes(x = `Risk (standard deviation)`, y = `Return target`))
  # }
  if(ROI_basis){
    gg <- ggplot(df_plot, aes(x = `Risk (standard deviation)`, y = `ROI target`))
  }else{
    gg <- ggplot(df_plot, aes(x = `Risk (standard deviation)`, y = `Return target`))
  }
  if(!is.null(fig_title)){
    gg <- gg + labs(title = fig_title)
    gg <- gg + theme(plot.title = element_text(face = "bold", size = 10))
  }
  gg <- gg + theme(axis.title.x = element_blank(),
                   axis.text.x = element_blank(),
                   axis.title.y = element_text(size = 8),
                   axis.text.y = element_text(size = 8))
  gg <- gg + geom_point()
  if(graph_on){print(gg)}
  return(gg)
}
#=======================================================================
get_optimal_frontier <- function(covmat, mat_nab,
                                 fun_env = NULL){
  #-------------------------------------------
  if(is.null(fun_env)){
    n_points_on_frontier = 50
    R_range = 0.3
    backtest_info = NULL
    C_targ = 1
    dimRet = F
  }else{
    n_points_on_frontier = fun_env[["n_points_on_frontier"]]
    R_range <- fun_env[["R_range"]]
    backtest_info = fun_env[["backtest_info"]]
    C_targ = fun_env[["C_targ"]]
    dimRet = fun_env[["dimRet"]]
  }
  #-------------------------------------------
      covmat_inv <- solve(covmat)
  M <- t(mat_nab) %*% covmat_inv %*% mat_nab # Merton matrix
  M_inv <- solve(M)
  R_at_minRisk <- M[1, 2] / M[2, 2] * C_targ
  #minRisk <- 
  Rtarg_vec <- seq(R_at_minRisk, R_at_minRisk + R_range, length.out = n_points_on_frontier)
  #-------------------------------------------
  list_wStar <- list()
  list_frontier <- list()
  for(i in 1:length(Rtarg_vec)){
    this_Rtarg <- Rtarg_vec[i]
    targ_vec <- c(this_Rtarg, C_targ)
        #-------------------------------------------
          list_out <- optimize_portfolio(M_inv, covmat_inv, covmat, mat_nab, targ_vec,
                                   backtest_info)
      #-------------------------------------------
    list_wStar[[i]] <- list_out[[1]]
    list_frontier[[i]] <- list_out[[2]]
  }
  #-------------------------------------------
  df_frontier <- data.frame(do.call(rbind, list_frontier))
  colnames(df_frontier) <- c("Return target",
                             "Risk (standard deviation)",
                             "Risk shadow price",
                             "Budget shadow price",
                             "Return backtest",
                             "Risk backtest")
  df_wStar <- data.frame(df_frontier$`Risk (standard deviation)`, t(do.call(cbind, list_wStar)))
  varNames_ordered <- row.names(mat_nab)
  colnames(df_wStar) <- c("Risk (standard deviation)", varNames_ordered)
  
  df_frontier$`Tot. investment` <- rowSums(abs(df_wStar[, -1]))
  df_frontier$`ROI target` <- df_frontier$`Return target` / df_frontier$`Tot. investment`
  df_frontier$`ROI backtest` <- df_frontier$`Return backtest` / df_frontier$`Tot. investment`
  #--------------------------------------
  list_out <- list(df_wStar, df_frontier, R_at_minRisk)
  return(list_out)
}
#=======================================================================
#=======================================================================
# End function definitions
#=======================================================================
#=======================================================================
# Conventional risk-reward frontier
n_items <- ncol(mat_ts_eg_train)
C_targ <- 1
nab_C <- rep(1, n_items)
utility_interpretation <- F
#--------------------------------------------------------------
# Covariance matrix
covmat_train <- covmat_decDiff_train #diag(eigen(covmat_decDiff_train)$values)
covmat_test <- covmat_decDiff_test #diag(eigen(covmat_decDiff_test)$values)
#--------------------------------------------------------------
# Expected returns vector
nab_decRet_train <- nab_decRet_eg_train
nab_decRet_test <- nab_decRet_eg_test
  #---
  # check
# nab_decRet_train <- (mat_ts_eg_train[nrow(mat_ts_eg_train), ] - mat_ts_eg_train[1, ]) / mat_ts_eg_train[1, ]
# nab_decRet_test <- (mat_ts_eg_test[nrow(mat_ts_eg_test), ] - mat_ts_eg_test[1, ]) / mat_ts_eg_test[1, ]
  # mat_pctDiff <- 100 * diff(mat_ts_eg_train) / mat_ts_eg_train[-nrow(mat_ts_eg_train), ]
  # nab_pctRet_check <- 100 * (apply(10^-2 * mat_pctDiff, 2, function(x) prod(1 + x)) - 1)
  # nab_pctRet / nab_pctRet_check
#--------------------------------------------------------------
mat_nab <- cbind(nab_decRet_train, nab_C)
n_points_on_frontier <- 50
R_range <- 0.05
backtest_info <- list()
backtest_info[["nab_decRet_test"]] <- nab_decRet_test
backtest_info[["covmat_test"]] <- covmat_test
#--------------------------------------------------------------
fun_env_getOptFront <- list()
fun_env_getOptFront[["n_points_on_frontier"]] <- n_points_on_frontier
fun_env_getOptFront[["R_range"]] <- R_range
fun_env_getOptFront[["backtest_info"]] <- backtest_info
fun_env_getOptFront[["C_targ"]] <- C_targ
fun_env_getOptFront[["dimRet"]] <- F
#--------------------------------------------------------------
list_out <- get_optimal_frontier(covmat_train, mat_nab,
                                 fun_env = fun_env_getOptFront)
df_wStar <- list_out[[1]]
df_frontier <- list_out[[2]]
R_at_minRisk <- list_out[[3]]
#--------------------------------------------------------------
gg_frontier <- plot_frontier(df_frontier, ROI_basis = F, list_graph_options = NULL, graph_on = F)
#--------------------------------------------------------------
# df_wStar_agg <- df_wStar %>% gather_("Item", "Budget Share", colnames(df_wStar)[-1])
# df_wStar_agg <- merge(df_wStar_agg, df_match_group, by = "Item")
# df_wStar_agg <- df_wStar_agg %>% group_by(`Risk (standard deviation)`, Group) %>% summarise(`Budget Share` = sum(`Budget Share`)) %>% spread(Group, `Budget Share`)
#gg_budget <- plot_budgetShares(df_wStar_agg, group_small = NULL, color_vec = group_colors, graph_on = F, list_graph_options = NULL)
n_items <- ncol(df_wStar) - 1
bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_items)
color_vec_mv_eg <- sample(bag_of_colors, n_items)
gg_budget <- plot_budgetShares(df_wStar, color_vec = color_vec_mv_eg, graph_on = F, list_graph_options = NULL)
#--------------------------------------------------------------
gg_frontier + gg_budget + plot_layout(ncol = 1)
#https://gotellilab.github.io/GotelliLabMeetingHacks/NickGotelli/ggplotPatchwork.html
#--------------------------------------------------------------

```
Note how the budget allocation changes with risk tolerance, and that some budget shares are negative. Negative budget shares mean that the investor should invest in the inverse of the corresponding asset. In the financial context, this is possible through short selling. Even so, many financial managers find negative shares to be something of a nuissance, since they imply borrowing, and thus effectively defeat the purpose of including a budget constraint in the first place ^[see Boyle]. Negative shares also present a somewhat misleading picture of portfolio returns. The high returns along the frontier in Figure \ref{fig:mvConv} are a result not so much of shrewd budget allocation as they are of leveraging. When expressed as a return _on investment_ (ROI), the outlook is more sobering (Figure \ref{fig:mvROI_levg_btest}, left panel).

```{r, fig.show = "hold", fig.width = 6, fig.height=3, fig.align='left', fig.cap="\\label{fig:mvROI_levg_btest}\\textit{(Left)} Optimal portfolio return on investment (ROI) frontier and leveraging; \\textit{(Right)} Backtest.", echo = FALSE}
df_frontier$Leveraging <- df_frontier$`Tot. investment` - 1

df_frontier_conv <- df_frontier
df_wStar_conv <- df_wStar

df_plot <- df_frontier_conv
gg <- ggplot(df_plot, aes(x = `Risk (standard deviation)`, y = `ROI target`))
gg <- gg + geom_point()
gg <- gg + theme(axis.title.x = element_blank(),
                 axis.text.x = element_blank())
gg_roi <- gg

gg <- ggplot(df_plot, aes(x = `Risk (standard deviation)`, y = Leveraging))
gg <- gg + geom_area(fill = "coral", alpha = 0.5)
gg_levg <- gg

gg_roiLevg <- gg_roi + gg_levg + plot_layout(ncol = 1, heights = c(2 / 3, 1 / 3))

gg <- ggplot(df_plot, aes(x = `Risk backtest`, y = `ROI backtest`))
gg <- gg + geom_point()
gg_backtest <- gg

gg_roiLevg | gg_backtest

```

Finally, note that the real ROI to the optimized portfolios over the investment period turn out to be much lower than that indicated by the efficient frontier (Figure \ref{fig:mvROI_levg_btest}, right panel). This is a common problem encountered in MV Analysis. Optimal solutions are highly sensitive to noise in the covariance matrix, often resulting in efficient frontiers that overstate returns and/or understate risk [@Michaud...].

For these reasons, MV Analysis is still a work in progress, even in its native financial context. The task of adaptation thus requires a degree of innovation in redressing longstanding issues.

# Adapting MV analysis to the AR4D context

From the exposition above, it is readily evident that a number of adjustments must be made to the method introduced above for it to be applicable in the AR4D context. To begin with, AR4D proposals do not generate data analogous to price time series found in the financial context. It is thus not possible to calculate an AR4D proposal variance-covariance matrix in a straightforward manner. Secondly, adjustments must be made to the portfolio return function $R$ in order to reflect the diminishing returns to investment in AR4D proposals. And, thirdly, budget shares must be positive in the AR4D context. (Finally, it would also be nice if the resulting solutions more closely resembled the real returns...)

I begin with the covariance matrix....drawing on PCA...dimensionally reduced matrix, map back to the portfolio items. Then...the diminishing returns to investment form derived from a... The issue of weights then falls into place without any further adjustments. A backtest suggests that the solutions are more accurate than in the plain vanilla case.


Below, I make adjustments to the MV Analysis method introduced above in order to adapt it to the AR4D context. As mentioned in the introduction, these adjustments redress the lack of a covariance matrix, the diminishing returns to investments, and the positive budget shares found in the AR4D context. I begin with the issue of the covariance matrix.

It turns out that these two adjustments then force budget shares to be positive.

## "Reverse engineering" the covariance matrix from domain knowledge


Jump diffusion process...Phillips corp. Merton. Here I take the alternate route of deducing the 

<!-- Pennings, E., & Lint, O. (1997). The option value of advanced R & D. European Journal of Operational Research, 103(1), 8394. http://doi.org/10.1016/S0377-2217(96)00283-4

Merton, R. C. (1976). Option pricing when underlying stock returns are discontinuous. Journal of Financial Economics, 3(1), 125144.
-->


In principal components analysis, a dataset $X$ containing $t$ observations of $n$ variables is distilled into a dataset $S$ of just $k<n$ variables that capture the main tendencies and structure in the data.^[The data is always centered. If the variables in $X$ follow diverse scalings and/or units of measurement (i.e. if apples are being compared to oranges), then $X$ should also be scaled to unit variance. In this exposition, the variables are all of the same type, and so $X$ is centered but not scaled. See [@AbdiPCA] for an introduction to principal components analysis.] The distilled matrix $S$ is defined

\begin{equation}
S = X \tilde{P}
\label{eq:sigs_def}
\end{equation}

where $\tilde{P}$ is a matrix containing the $k$ leading eigenvectors of the full set of eigenvectors $P$, which is taken from the eigendecomposition of the data covariance matrix $\Sigma_{XX}$ (equation \ref{eq:eigDecomp}).

\begin{equation}
\Sigma_{XX} = P\Gamma P'
\label{eq:eigDecomp}
\end{equation}

where $\Gamma$ is the diagonal matrix of eigenvalues of $\Sigma_{XX}$. 

From the definition (\ref{eq:sigs_def}), it follows that the signals are uncorrelated with each other, and that their variance is given by the eigenvalues of the data covariance matrix.

\begin{eqnarray}
\Sigma_{SS} &=& \frac{1}{n-1}S'S \\
&=& \frac{1}{n-1} \tilde{P}'X'X\tilde{P} \\
&=& \tilde{P}'\Sigma_{XX}\tilde{P} \\
&=& \tilde{P}'P \Gamma P'\tilde{P}=\tilde{\Gamma}
\label{eq:covmat_SS}
\end{eqnarray}

The columns of the distilled matrix $S$ are referred to as the principal components (PC), or the PC scores, or the factor scores. When dealing with noisy time series, as in this paper, they might just as well be reffered to as the "signals", in the sense that they are signals extracted from noise.

The dataset is thus effectively reduced in complexity from $n$ to $k$ dimensions. There then remains the question of what essential process these dimensions or signals describe. This can be interpreted based on how correlated they are with the variables in the original dataset. These correlations ($K_{XS}$) are found by first finding their corresponding covariances ($\Sigma_{XS}$).

\begin{eqnarray}
\Sigma_{XS} &=& \frac{1}{n-1}X'S \\
&=& \frac{1}{n-1}X'XP \\
&=& KP = P \Gamma P'P \\
&=& P \Gamma
\end{eqnarray}

The correlation matrix $K_{XS}$ then follows as

\begin{eqnarray}
K_{XS} &=& D(\boldsymbol{\sigma}_X)^{-1} \Sigma_{XS} D(\boldsymbol{\sigma}_S)^{-1} \\
&=&D(\boldsymbol{\sigma}_X)^{-1} P \Gamma D(\boldsymbol{\sigma}_S)^{-1}
\end{eqnarray}

But the standard deviations of the signals are just the square roots of the eigenvalues (recall equation \ref{eq:covmat_SS}), so this reduces to

\begin{eqnarray}
K_{XS} &=& D(\boldsymbol{\sigma}_X)^{-1}\Sigma_{XS} \Gamma ^{-{1 \over 2}} \\
&=& D(\boldsymbol{\sigma}_X)^{-1}P \Gamma \Gamma^{-{1 \over 2}} \\
&=& D(\boldsymbol{\sigma}_X)^{-1}P \Gamma ^{1 \over 2}
\end{eqnarray}

Note that if $X$ were scaled to unit variance, then this would reduce further to

\begin{equation}
K_{XS} = P \Gamma^{1 \over 2}
\end{equation}

The correlations matrix $K_{XS}$ is sometimes referred to as the "loadings" matrix, in the sense that it indicates how much each variable in $X$ loads onto a given signal (or, vice versa, how much each signal loads onto a given variable).^[Although many prefer to call $P$ the loadings.] In keeping with this convention, and in order to reduce notational clutter, $K_{XS}$ is henceforth relabeled $L$.

The correlations between the financial assets from the example above and their four leading signals are presented in Figure \ref{fig:corrXS_barchart}. Concrete meaning can be attributed to each of these otherwise abstract signals by examining how correlated they are with the portfolio items.


```{r, fig.show='hold', fig.width=6, fig.height=3, fig.align='right', fig.cap="\\label{fig:corrXS_barchart}Correlation of portfolio items with leading signals."}


# Define function to calculate variable-signal correlations, financial data
get_S_and_corrXS <- function(mat_X_in){
  # mat_P = eigenvectors of the data correlation matrix
  # mat_U = corresponding eigenvalues
  mat_X_centered <- scale(mat_X_in, scale = F)
  # out_svd <- svd(mat_X_centered)
  # sing_values <- out_svd$d
  # n_obs <- nrow(mat_X_centered)
  # eig_values <- sing_values^2 / (n_obs - 1)
  # mat_P <- out_svd$v

mat_P <- eigen(cov(mat_X_in))$vectors
if(mean(mat_P[, 1]) < 0){mat_P <- -mat_P}
eig_values <- eigen(cov(mat_X_in))$values
mat_U <- diag(eig_values)
  
#mat_P_sigs <- mat_P[, 1:n_signals]
# eig_values[1:n_signals] / eigen(cov(mat_X_centered))$values[1:n_signals] #check
# mat_P / eigen(cov(mat_X_in))$vectors #check

#mat_U <- diag(eig_values)

#mat_U_sigs <- matU[, 1:n_signals]
#---------------------------------------------
sd_X <- apply(mat_X_in, 2, sd)
D_sdX_inv <- diag(1 / sd_X)
cormat_XS <- D_sdX_inv %*% mat_P %*% sqrt(mat_U)
row.names(cormat_XS) <- colnames(mat_X_in)
mat_L <- cormat_XS
#mat_L <- diag(1 / apply(mat_X_in, 2, sd)) %*% mat_P %*% sqrt(mat_U)
  #---------------------------------------------------------
  # Set sign of eigenvectors such that signals best conform to their most highly correlated items
# First have to get average of highest correlated items for each signal
corrThresh <- 0.55
n_items <- ncol(mat_L)
list_X_hiCorr_avg <- list()
  for(i in 1:n_items){
    this_loadvec <- mat_L[, i]
    ind_tracks <- which(abs(this_loadvec) >= corrThresh)
    if(length(ind_tracks) == 0){
        ind_tracks <- which(abs(this_loadvec) == max(abs(this_loadvec)))
    }
    if(length(ind_tracks) == 1){
              list_X_hiCorr_avg[[i]] <- mat_X_centered[, ind_tracks]
    }else{
          loadvec_kept <- this_loadvec[ind_tracks]
    list_X_hiCorr_avg[[i]] <- rowMeans(mat_X_centered[, ind_tracks])

    }
}
mat_X_hiCorr_avg <- do.call(cbind, list_X_hiCorr_avg)
mat_S_all <- mat_X_centered %*% mat_P
#mat_S_all <- mat_X_in %*% mat_P
for(i in 1:n_items){
  this_S <- mat_S_all[, i]
  this_X_hiCorr_avg <- mat_X_hiCorr_avg[, i]
  mse <- mean((this_S - this_X_hiCorr_avg)^2)
  mse_neg <- mean((-this_S - this_X_hiCorr_avg)^2)
  if(mse_neg < mse){
        mat_P[, i] <- -mat_P[, i]
      }
    }
cormat_XS <- D_sdX_inv %*% mat_P %*% sqrt(mat_U)
row.names(cormat_XS) <- colnames(mat_X_in)
mat_L <- cormat_XS
mat_S_all <- mat_X_centered %*% mat_P
#---------------------------------------------
# res <- FactoMineR::PCA(mat_pctDiff_in, scale.unit = F, ncp = ncol(mat_pctDiff_in), graph = F)
# mat_L_FactoMiner <- res$var$coord
# mat_L / mat_L_FactoMiner

list_out <- list(mat_S_all, cormat_XS, eig_values, mat_P)
return(list_out)
}
#====================================================
# Barchart of variable-signal correlations
plot_corrXS_barchart <- function(mat_L, group_info = NULL, xAxis_title = NULL, sigNames = NULL){
  
  n_signals <- ncol(mat_L)
    df_plot <- data.frame(Item = row.names(mat_L), mat_L)
    df_plot$Item <- as.character(df_plot$Item)
    #-------------------------------------------------------
    if(is.null(sigNames)){
  signal_id <- paste("Signal", 1:n_signals)
}else{
  signal_id <- paste("Signal", 1:n_signals, "-", sigNames)
}
  colnames(df_plot)[2:(n_signals + 1)] <- signal_id
  #-------------------------------------------------------
  gathercols <- as.character(signal_id) 
  df_plot <- gather_(df_plot, "Signal", "Correlation", gathercols)
    df_plot <- transform(df_plot,
                       Signal = factor(Signal, levels = gathercols))

  if(!is.null(group_info)){
  outlist <- group_fn(group_info)
  cols_ordered_by_group <- outlist[[1]]
  group_color_vec <- outlist[[2]]
  group_vec_ordered <- outlist[[3]]
  df_match_group <- data.frame(Item = cols_ordered_by_group, Group = group_vec_ordered)
  df_plot <- merge(df_plot, df_match_group, by = "Item")
  df_plot <- df_plot[order(df_plot$Group), ]
  df_plot$Item <- factor(df_plot$Item, levels = unique(df_plot$Item))
     gg <- ggplot(df_plot, aes(x = Item, y = Correlation, fill = Group))
     gg <- gg + scale_fill_manual(values = unique(group_color_vec))
    }else{
      gg <- ggplot(df_plot, aes(x = Item, y = Correlation))
  }
  gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
  gg <- gg + ylim(limits = c(-1, 1))
  gg <- gg + facet_wrap(~ Signal, nrow = 1)
  if(!is.null(xAxis_title)){
    gg <- gg + labs(y = xAxis_title)
  }
  gg <- gg + theme(axis.text = element_text(size = 8),
                   axis.title.x = element_text(size = 8),
                   axis.title.y = element_blank(),
                   legend.title = element_blank(),
                   legend.text = element_text(size = 8),
                   strip.text = element_text(size = 8))
  gg <- gg + coord_equal()
  gg <- gg + coord_flip()
  gg

}

#====================================================
n_signals <- 4
mat_X_in <- mat_decDiff_eg_train
#mat_X_in <- mat_ts_eg_train
list_out <- get_S_and_corrXS(mat_X_in)
mat_S_all <- list_out[[1]]
cormat_XS <- list_out[[2]]
eig_values <- list_out[[3]]
mat_P <- list_out[[4]]
mat_S <- mat_S_all[, 1:n_signals]
mat_L <- cormat_XS[, 1:n_signals]

#eigen(cov(mat_S))$values / eig_values[1:n_signals]

plot_corrXS_barchart(mat_L)

```

Clearly, Signal 4 represents movements in communications. Signal 3 is concerned with Utilities and Real Estate, and so might be called the "Housing & Urban Development Signal". The interpretation of Signals 1 and 2 is not so straightforward, since they are correlated with many portfolio items. In such cases, it is often useful to apply an orthogonal rotation to $L$ in order to clarify the picture. That is to say, instead of examining $L$, one examines $L_{\circlearrowleft}$.

\begin{equation}
L_{\circlearrowleft} = LB
\label{eq:Lrot}
\end{equation}

where $B$ is the orthogonal rotation matrix, such that $B'B = I$ and $BB' = I$.
<!-- Note that a rotation of $L$ implies a rotation of the signals, since -->

<!-- \begin{equation} -->
<!-- SR = XP -->
<!-- \end{equation} -->

In Figure \ref{fig:corrXS_barchart_rot} a special kind of orthogonal rotation, called a varimax rotation, is applied to $L$. Varimax rotations flesh out structure by maximizing sparseness in the rotated matrix. After applying this rotation, it becomes clear that Signal 1 is representative of Biotechnology and Healthcare; and so Signal 1 might be called the "Pharmaceutical Signal". Signal 2 loadings are now more pronounced, especially Financials, Industrial, and Transportation. Signal 2 might thus be called the "Financial and Physical Infrastructure Signal." In Figure \ref{fig:signals_with_hiCorr_items}, it becomes particularly evident how each signal hews closely to its most highly correlated items, offering visual confirmation of these interpretations. 

```{r, fig.show='hold', fig.width=6, fig.height=3, fig.align='left', fig.cap="\\label{fig:corrXS_barchart_rot}Varimax rotated correlation of portfolio items with leading signals.", echo=F}

mat_Lrot <- varimax(mat_L)[[1]]
mat_Lrot <- matrix(as.numeric(mat_Lrot),
                          attributes(mat_Lrot)$dim,
                          dimnames = attributes(mat_Lrot)$dimnames)
mat_R <- varimax(mat_L)[[2]]
mat_R <- matrix(as.numeric(mat_R),
                          attributes(mat_R)$dim,
                          dimnames = attributes(mat_R)$dimnames)

xAxis_title <- "Varimax Rotated Correlation"
plot_corrXS_barchart(mat_Lrot, group_info = NULL, xAxis_title, sigNames = NULL)


```


```{r, fig.show='hold', fig.width=6, fig.height=5, fig.align='left', fig.cap="\\label{fig:signals_with_hiCorr_items}Signals plotted together with their most highly correlated assets. The signals are plotted as thick grey lines."}
# Define function to plot signals with their highest loading items.
plot_sigs_wHiLoad_items <- function(mat_X_in, mat_S, mat_L, color_vec, sigNames = NULL, load_threshold = 0.55, n_display_max = 4){
n_signals <- ncol(mat_L)
if(is.null(sigNames)){
  fig_title_vec <- paste("Signal", 1:n_signals)
}else{
  fig_title_vec <- paste("Signal", 1:n_signals, "-", sigNames)
}
# Plot signals against highest correlated variables
  date_vec <- row.names(mat_S)
  xAxis_labels <- date_vec[seq(1, nrow(mat_S), length.out = 5)]
  list_gg <- list()
  for(i in 1:n_signals){
    this_loadvec <- mat_L[, i]
    ind_tracks <- which(abs(this_loadvec) >= load_threshold)
    if(length(ind_tracks) == 0){
        ind_tracks <- which(abs(this_loadvec) == max(abs(this_loadvec)))
    }

    n_display <- length(ind_tracks)
    loadvec_kept <- this_loadvec[ind_tracks]
    if(n_display > n_display_max){
      n_to_omit <- n_display - n_display_max
      #random_omission <- sample(1:n_display, n_to_omit)
      #mat_pctDiff_tracks <- mat_pctDiff_tracks[, -random_omission]
      ind_omit <- order(loadvec_kept)[c(1:n_to_omit)]
      ind_tracks <- ind_tracks[-ind_omit]
      loadvec_kept <- this_loadvec[ind_tracks]
    }
    mat_X_tracks <- mat_X_in[, ind_tracks]
    color_vec_tracks <- color_vec[ind_tracks]
    #------------
    df_plot_sig <- data.frame(Date = date_vec, Value = mat_S[, i])
    df_plot_tracks <- data.frame(Date = date_vec, mat_X_tracks)
    if(ncol(df_plot_tracks) > 2){
      gathercols <- colnames(mat_X_tracks)
      colnames(df_plot_tracks)[-1] <- gathercols
      df_plot_tracks <- df_plot_tracks %>% gather_("ts", "Value", gathercols)
    }else{
      colnames(df_plot_tracks)[-1] <- "Value"
      df_plot_tracks$ts <- colnames(mat_X_in)[ind_tracks]
    }
    #------------
    gg <- ggplot()
    gg <- gg + geom_line(data = df_plot_sig, aes(x = Date, y = Value, group = 1), color = "grey", lwd = 1.3)
    gg <- gg + geom_line(data = df_plot_tracks, aes(x = Date, y = Value, group = ts, color = ts))
    gg <- gg + scale_color_manual(values = color_vec_tracks)
    gg <- gg + scale_x_discrete(breaks = xAxis_labels)
    gg <- gg + labs(title = fig_title_vec[i])

    if(i == n_signals){
      #gg <- gg + labs(title = paste("Signal", i), caption = fig_title)
      gg <- gg + theme(#axis.text.x = element_text(angle = 60, hjust = 1),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.title = element_blank(),
        plot.title = element_text(size = 9)
        #plot.caption = element_text(hjust = 0, size = 10)
      )
    }else{
      gg <- gg + theme(axis.text.x = element_blank(),
                       axis.title.x = element_blank(),
                       axis.title.y = element_blank(),
                       legend.title = element_blank(),
                       plot.title = element_text(size = 9))
    }
    
    list_gg[[i]] <- gg
    
  }
  
  gg_all <- wrap_plots(list_gg) + plot_layout(ncol = 1)
  gg_all
}

# Name signals based on correlations
# sigNames <- c("Portfolio average\n(Especially Pharmaceutical, Technology, and Luxury Goods)",
#               "Transportation and Industrial",
#               "Housing & Urban Development (Inverse)",
#               "Communications (Inverse)")
# 
# mat_X_in <- mat_pctDiff_in
# plot_sigs_wHiLoad_items(mat_X_in, mat_S, mat_L, sigNames, load_threshold = 0.55, n_display_max = 4)
sigNames <- c("Pharmaceutical",
              "Financial and Physical Infrastructure",
              "Housing & Urban Development",
              "Communications")
  

color_vec <- color_vec_mv_eg
plot_sigs_wHiLoad_items(mat_X_in, mat_S, mat_Lrot, color_vec, sigNames, load_threshold = 0.65, n_display_max = 4)
  
```  

In practice, the number of signals that should be distilled from the original data set $X$ depends upon how much of the variance in $X$ the researcher wishes to capture or reflect in the signals, and how many signals are required to reach this subjectively determined threshold. The portion of the system's evolution reflected in any given signal ($c_i$) is defined as the signal's variance divided by the sum of all signal variances. Recalling from equation \ref{eq:covmat_SS} that a signal's variance is just the corresponding eigenvalue $u_i$ extracted from $\Sigma_{XX}$, this is expressed

\begin{equation}
c_i = \frac{u_i}{\sum_{i = 1}^n u_i}
\end{equation}

The cumulative variance captured by a group of $k$ signals is then

\begin{equation}
c_k <- \frac{\sum_{i=1}^k u_i}{\sum_{i = 1}^n u_i}
\end{equation}

The individual and cumulative portions explained by each signal are plotted in Figure \ref{fig:varExplained_barchart}. Customarily, researchers like to retain signals such that at least 90% of the variance in the original data is explained. The horizontal dashed line in the plot marks this subjective threshold.

```{r, fig.show='hold', fig.width=5, fig.height=2, fig.align='center', fig.cap="\\label{fig:varExplained_barchart}Plot of the individual and cumulative portions of variance explained by each signal.", echo=F}
c_vec <- eig_values / sum(eig_values)
ck_vec <- cumsum(c_vec)
  df_plot <- data.frame(Signal = paste("Signal", 1:length(eig_values)), Portion_explained = c_vec, Portion_explained_cumul = ck_vec)
  colnames(df_plot)[2:3] <- c("Individually", "Cumulatively")
  gathercols <- colnames(df_plot)[2:3]
  df_plot <- gather_(df_plot, "Portion explained", "Value", gathercols)
  df_plot$Signal <- factor(df_plot$Signal,
                           levels = unique(df_plot$Signal),
                           ordered = T)
    gg <- ggplot(df_plot, aes(x = Signal, y = Value, fill = `Portion explained`))
  gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
  gg <- gg + scale_fill_manual(values = c("wheat3", "paleturquoise"))
  gg <- gg + geom_hline(yintercept = 0.9, color = "coral", size = 1, linetype = "dashed")
  gg <- gg + theme(axis.text.y = element_text(size = 9),
                   axis.text.x = element_text(size = 9, angle = 60, hjust = 1),
                   axis.title = element_blank(),
                   legend.title = element_text(size = 9),
                   legend.text = element_text(size = 9))

  gg
  
  #====================================================
n_signals <- which(ck_vec > 0.9)[1]
#n_signals <- 3
#====================================================

  
```

The plot thus shows that the leading `r n_signals` signals are sufficient to meet this criterion. Note that the data correlation matrix can then be approximated from the retained signals, as their outer product.

\begin{equation}
\hat{\Sigma}_{XX} = \tilde{L}_{\circlearrowleft}\tilde{L}_{\circlearrowleft}'
\label{eq:covmat_from_Lrot}
\end{equation}

This is true for any orthogonal rotation $B$, since

\begin{equation}
\tilde{L}_{\circlearrowleft}\tilde{L}_{\circlearrowleft}' = (X \tilde{P}B)(X \tilde{P} B)' \\
&=& (X\tilde{P})BB'(X\tilde{P})'
\end{equation}

But $BB' = I$ by orthogonality, thereby dropping out of the equation.

The difference between the financial data correlation matrix and the signals derived correlation matrix is shown in Figure \ref{fig:compareCorMats}. The signals derived correlation matrix is approximate in the sense that it approximates the data correlation matrix, but it should not necessarily be considered inferior in terms of accuracy. To the extent that the original data are contaminated by noise, the signals derived correlation matrix may even be more accurate with respect to the "true process" that generates the data.

```{r, fig.show='hold', fig.width=6, fig.height=4, fig.align='center', fig.cap="\\label{fig:compareCovMats}The data covariance matrix minus the covariance matrix derived from the retained signals."}
mat_S <- mat_S_all[, 1:n_signals]
mat_L <- cormat_XS[, 1:n_signals]
eig_values_sigs <- eig_values[1:n_signals]

covmat_XX <- cov(mat_pctDiff_eg_train)
covmat_XX_derived <- mat_L %*% t(mat_L)
mat_diff_covmats <- covmat_XX - covmat_XX_derived

plot_covmat(mat_diff_covmats, fig_title = NULL, graph_on = F)

#plot_covmat(covmat_XX_derived, fig_title = NULL, graph_on = F)


# cormat_XX <- cor(mat_pctDiff_train_mv)
# D_sdX <- diag(apply(mat_X_in, 2, sd))
# mat_L_cor <- D_sdX %*% mat_L
# cormat_XX_derived <- mat_L_cor %*% t(mat_L_cor)
# cormat_XX <- cor(mat_pctDiff_train_mv)
# mat_diff_cormats <- cormat_XX - covmat_XX_derived
# plot_covmat(mat_diff_cormats, fig_title = NULL, graph_on = F)

```

Now, consider that, in the absence of data, the retained signals could be identified based on expert knowledge. That is, the key signals in \ref{fig:signals_with_hiCorr_items} could be deduced through consultation with, say, market experts. The same domain knowledge could then be tapped to determine each portfolio asset's correlation with each of these key signals. The end result of this consultation would be a crowdsourced, orthogonally rotated loadings matrix. The assets covariance matrix could then be calculated from this information via equation \ref{eq:covmat_from_Lrot}.

In this sense, a covariance matrix can be reverse engineered from domain knowledge when there is no data. The crowdsourced covariance matrix may differ from the data covariance matrix; but it need not be considered inferior to the data covariance matrix in terms of accuracy. In noisy contexts, crowdsourced covariance matrices may prove more accurate than their data based counterparts. As a rule of thumb, the appropriateness of the crowdsourcing approach to covariance matrix estimation may be assessed by meditating upon the conceptual ratio $\nu$ defined in equation \ref{eq:ratio_conf}. 
<!-- Regardless of accuracy, the crowdsourced covariance matrix has value in that it provides a picture...consistent with what we think we know about the system.-->

\begin{equation}
\nu = \frac{\text{confidence in domain knowledge}}{\text{confidence in data}}
\label{eq:ratio_conf}
\end{equation}

As $\nu$ is higher, the crowdsource approach to covariance matrix estimation is more suitable. As $\nu$ is lower, it becomes more appropriate to estimate the covariance matrix on the basis of data. For values of $\nu$ close to 1, one might consider a mixture of the two approaches.

[Maybe in intro to this section: below show how, in the absence of data, domain knowledge of the key dimensions of the problem space, and of how the various moving parts of the problem (i.e. the portfolio items) map onto these dimensions, is sufficient to estimate the data covariance matrix.]

## The crowdsourced signals portfolio

The deduced data covariance matrix may be useful in orienting stakeholder discussions, but it cannot be used for optimization since, by definition, it contains eigenvalues that are equal to zero.^[In the previous section we saw that optimization requires that the covariance matrix be invertible. A matrix with eigenvalues equal to zero is not invertible.]

However, we can optimize over a portfolio of signals, and then map the optimal signal budget shares back to individual asset shares based on their correlation with the signals. To do this, we need the signals covariance matrix $\Sigma_{SS}$. Recall from equation $\ref{eq:covmat_SS}$ that the signals covariance matrix is a diagonal matrix containing the leading eigenvalues of $\Sigma_{XX}$. Note that $\Sigma{SS}$ can also be defined in terms of the loadings matrix $L$ as follows.

\begin{equation}
\Sigma{SS} = L'L
&=& (X \tilde{P})' X \tilde{P} \\
&=& \tilde{P}' X'X \tilde{P} \\
&=& \tilde{P}' \Sigma_{XX} \tilde{P} \\
&=& \tilde{P}' P \Gamma P' \tilde{P} \\
&=& \tilde{\Gamma} \\
\label{eq:covmat_SS_from_L}
\end{equation}

When loadings are ellicited from domain knowledge, as opposed to calculated from data, they will generally not be orthogonal, and so must be interpreted as rotated loadings ($L_{\circlearrowleft}$). Applying steps \ref{eq:covmat_SS_from_L} then results in a rotated signals covariance matrix.

\begin{eqnarray}
\Sigma_{SS \circlearrowleft} = \tilde{L}_{\circlearrowleft}'\tilde{L}_{\circlearrowleft} \\
&=& (X \tilde{P} B)' X \tilde{P} B \\
&=& B' \tilde{P}' X'X \tilde{P} B \\
&=& B' \tilde{P}' \Sigma_{XX} \tilde{P} B \\
&=& B' \tilde{P}' P \Gamma P' \tilde{P} B \\
&=& B' \tilde{\Gamma} B \\
\end{eqnarray}

However, note here that $B'$ is equal to the eigenvectors of $\Sigma_{SS \circlearrowleft}$. This means that, given crowdsourced, rotated loadings $L_{\circlearrowleft}$, the signals covariance matrix can be isolated from the implicit rotation $B$ by taking the eigendecomposition of $L_{\circlearrowleft}'L_{\circlearrowleft}$. The eigenvalues of this decomposition are $\Gamma$, which is equal to the unrotated signals covariance matrix.

## All data as rotated signals? Always discard eigenvectors?

This implies something interesting. (By the line of reasoning above) Any dataset may itself be considered a set of orthogonally rotated signals constructed from some higher dimensional dataset. Eigenvectors are always, in some sense, extraneously applied as an aid in interpretation, and hence not inherent to the construction of the signals.... eigenvectors are always, in some sense, suprfluous to their construction, whereas the eigenvalues remain the same throughout successive dimensional reductions. 



Since the rotation $B$ is not inherent to the construction of the signals, but rather applied extraneously as an aid in their interpretation, it stands to reason that they should be discarded insofar as we are interested in acquiring a "pure" expression of the the signals covariance matrix for purposes of optimization. This leaves us with a diagonal signals covariance matrix.

\begin{equation}
\Sigma_{SS} = \tilde{\Gamma}
\end{equation}




In the AR4D context, $\nu$ has a high value.

Now, when deducing the covariance matrix based on domain knowledge, the rotation represents a perspective.  In the identification of signals it was shown how the loadings can be rotated to aid in interpretation.  Covariance matrices are only defined up to orthogonal rotation. 

The signals covariance matrix is just the diagonal matrix of the `r n_signals` leading eigenvalues ($U$) corresponding to the `r n_signals` retained signals (recall equation \ref{eq:}). The matrix $U$ can be derived from the elicited loadings $L_{\circlearrowleft}$ and project risk $\boldsymbol{\sigma}$ as follows. First, define a new matrix $Q$.

\begin{equation}
Q = D(\boldsymbol{\sigma}_X) L_{\circlearrowleft}
\end{equation}

By equation \ref{eq:} it follows that 

\begin{equation}
Q = P U^{1 \over 2} R
\end{equation}

and hence

\begin{eqnarray}
Q'Q &=& L_{\circlearrowleft}' D(\boldsymbol{\sigma}^2_X) L_{\circlearrowleft} \\
&=& R'U R
\end{eqnarray}

where $R$ is the orthogonal rotation matrix. It follows, then, that $U$ (and $R$) can be obtained through the eigendecomposition of $Q'Q$.

## Diminishing returns

In financial contexts, investments scale linearly. A doubling of the investment doubles expected return. In the AR4D context, on the other hand, returns to investment are marginally diminishing. In other words, the additional return resulting from any small increase in investment in a given portfolio item will be inversely proportionate to the quantity already invested in that portfolio item. This statement can be formalized as follows.

\begin{equation}
\frac{\partial \ln(R)}{\partial \ln(w_i)} \sim \frac{d \ln (w_i)}{dw_i}
\label{eq:dimRetDefine}
\end{equation}

Equation \ref{eq:dimRetDefine} can be used to arrive at an expression for $R$ as follows:

$$
\begin{eqnarray}
\ln R &=& \int \nabla_{\ln \mathbf{w}} \ln R \cdot d\mathbf{w} \\
&=& \int \nabla_{\mathbf{w}} \ln\mathbf{w} \cdot d\ln\mathbf{w} \\
&=& \int \mathbf{w}^{-2} \cdot d\mathbf{w} \\
&=& \boldsymbol{\alpha} \cdot \mathbf{w}^{-1} + k \\
\end{eqnarray}
$$
Substituting $\bar{R}=\exp(k)$, this can be rewritten

$$
\begin{equation}
R(\mathbf{w}; \boldsymbol{\alpha}, \bar{R}) = \bar{R}\exp(\boldsymbol{\alpha} \cdot \mathbf{w}^{-1})
\end{equation}
$$
where the proportionality constants $\boldsymbol{\alpha}$ are determined by context. In the present setting, the constants $\boldsymbol{\alpha}$ correspond to the vector $\boldsymbol{\mu}$. The constant of integration, $\bar{R}$, represents the ceiling on $R$, i.e.,

$$
\begin{equation}
\bar{R} = \lim_{\mathbf{w} \rightarrow \infty} R
\end{equation}
$$

and can be set to 1 by convention, resulting in

$$
\begin{equation}
R(\mathbf{w}; \boldsymbol{\mu}) = \exp(\boldsymbol{\mu} \cdot \mathbf{w}^{-1})
\end{equation}
$$

## Enforcing positive weights

\begin{equation}
L = \sigma^2 - \lambda_R(R) (\ln R - \ln \hat{R}) + \lambda_C (C - \hat{C})
\end{equation}

Where here

\begin{equation}
\sigma^2 = Var(\ln R) = \mathbf{w}^{-1}' \Sigma \mathbf{w}^{-1}
\end{equation}

And, since only using eigenvalues...

\begin{equation}
\sigma^2 = Var(\ln R) = \mathbf{w}^{-1}' D(\boldsymbol{\gamma}) \mathbf{w}^{-1}
\end{equation}


\begin{equation}
\nabla_{\mathbf{w}} L = -2 D(\boldsymbol{\gamma}) \mathbf{w}^{-3} - \lambda_R D(\boldsymbol{\mu}) \mathbf{w}^{-2} + \lambda_C \mathbf{1} = \mathbf{0}
\end{equation}

which can be rearranged into the following cubic equation

\begin{equation}
-2 D(\boldsymbol{\gamma}) + \lambda_R D(\boldsymbol{\mu}) \mathbf{w} + \lambda_C \mathbf{w}^{3} = \mathbf{0}
\label{eq:rootfn}
\end{equation}

By Descartes' rule of signs, each budget share $w_i$ is guaranteed to have one positive real root and two complex roots. WxMaxima returns the following solution for equation \ref{eq:rootfn}.

\begin{equation}
w_i = 1 \over \sqrt{3} (\lambda_C^{-(1 \over 3)} a^{1 \over 3} - \lambda_C^{-(2 \over 3)} \lambda_R \mu_i a^{-(1 \over 3)})
\label{eq:w_sol}
\end{equation}

where

\begin{equation}
a_i = \sqrt{27 \gamma_i^2 + \frac{\lambda_R^3 \mu_i^3}{\lambda_C}} + \sqrt{27}\gamma_i
\end{equation}

The domain of $w_i$ is not readily apparent from this expression. However, it can coerced into a more instructive and compact form as follows. First, define $b_i = \frac{\lambda_C}{\lambda_R \mu_i}$ and $u_i = \frac{\gamma_i}{\lambda_R \mu_i}$, so that the term $a_i$ can be expressed 

\begin{equation}
a_i = \sqrt{27 u_i^2 - 1\over b_i} + \sqrt{27} u_i
\end{equation}

Now, note that $a_i$ can be factored as follows.

\begin{equation}
a_i = \sqrt{1 \over b_i} \left( j \sqrt{1 - 27 u_i^2 b_i} + \sqrt{27 b_i} u_i \right)
\end{equation}

where $j$ is the imaginary number $j = \sqrt{-1}$.

Now, note that this expression describes a triangle and can thus be rewritten in terms of sine and cosine.

\begin{equation}
a_i = \sqrt{1 \over b_i} \left( j \sin{\theta_i} + \cos{\theta_i} \right) \:\:; \:\:\:\: \theta_i = \acos{\sqrt{27 b_i} u_i}
\end{equation}

By Euler's formula, moreover,

\begin{equation}
a_i = \sqrt{1 \over b_i} e^{j \theta_i}
\end{equation}

Substituting this into equation \ref{eq:w_sol} then reduces the budget share equation to

\begin{equation}
w_i = \frac{1}{b_i \sqrt{3}} \left(e^{j \theta_i \over 3} + e^{-j \theta_i \over 3} \right)
\end{equation}

which further reduces to

\begin{equation}
w_i = 2 \over \sqrt{3} \sqrt{\frac{\lambda_R \mu_i}{\lambda_C}} \cos{\theta_i \over 3} \:\:; \:\:\:\: \theta_i = \acos{3 \sqrt{3} \sqrt{\frac{\lambda_C \gamma_i^3}{\lambda_R^3 \mu_i^3}}}
\end{equation}

Since $-1<\cos{t}<1$ for any angle $t$, the budget share domain readily follows as

\begin{equation}
-\frac{2}{\sqrt{3}} \sqrt{\frac{\lambda_R \mu_i}{\lambda_C}} < w_i < \frac{2}{\sqrt{3}} \sqrt{\frac{\lambda_R \mu_i}{\lambda_C}}
\end{equation}

Only cases where $\lambda_R > 0, \: \lambda_C > 0$ are relevant, further restricting this to

\begin{equation}
0 < w_i < \frac{2}{\sqrt{3}} \sqrt{\frac{\lambda_R \mu_i}{\lambda_C}}
\end{equation}

hence confirming that weights are always positive.

...

For curiosity's sake, how does this apply to the fin data...

```{r, fig.show='hold', fig.width=6, fig.height=3, fig.align='left', fig.cap="\\label{fig:corrXS_barchart_rot}Varimax rotated correlation of portfolio items with leading signals, backtest data.", echo=F}
mat_X_in_test <- mat_decDiff_eg_test
list_out <- get_S_and_corrXS(mat_X_in_test)
mat_S_test_all <- list_out[[1]]
cormat_XS_test <- list_out[[2]]
eig_values_test <- list_out[[3]]
mat_P_test <- list_out[[4]]
#----

mat_L_test <- cormat_XS_test[, 1:n_signals]
mat_Lrot_test <- varimax(mat_L_test)[[1]]
mat_Lrot_test <- matrix(as.numeric(mat_Lrot_test),
                          attributes(mat_Lrot_test)$dim,
                          dimnames = attributes(mat_Lrot_test)$dimnames)
mat_R_test <- varimax(mat_L_test)[[2]]
mat_R_test <- matrix(as.numeric(mat_R_test),
                          attributes(mat_R_test)$dim,
                          dimnames = attributes(mat_R_test)$dimnames)

xAxis_title <- "Varimax Rotated Correlation (test data)"
plot_corrXS_barchart(mat_Lrot_test[, 1:4], group_info = NULL, xAxis_title, sigNames = NULL)

```


```{r, fig.show = "hold", fig.width = 5, fig.height=5, fig.align='left', fig.cap="\\label{fig:mvSigs}Optimal frontier and budget shares, signals portfolio.", echo = FALSE}
ind_rearrange <- c(2, 1, 3:n_signals)
#=============================================================================
# To convince yourself that sign of eigenvectors can be changed with impunity
# M <- matrix(runif(4*4), 4, 4)
# M <- round(t(M) %*% M, 5)
# P <- eigen(M)$vectors
# U <- diag(eigen(M)$values)
# these_cols <- c(2, 4)
# P_new <- P
# P_new[, these_cols] <- -P[, these_cols]
# round(M - P %*% U %*% t(P), 4)
# round(M - P_new %*% U %*% t(P_new), 4)
#=============================================================================

# rootfn_lambdas <- function(lambdas_in, Rtarg, Ctarg, nab_decRet, eigvals){
#   l_R_in <- lambdas_in[1]
#   l_C_in <- lambdas_in[2]
#   k_in <- -l_C_in / l_R_in
#   lRtarg <- log(Rtarg)
#   #k <- get_k_forlRtarg(lRtarg, Ctarg, nab_decRet, eigvals)
#   wStar <- optimal_portfolio_wgts_dimRet(k_in, nab_decRet, eigvals, Ctarg)
#   mu_vec <- nab_decRet
#   u_vec <- eigvals
#   mat_nab <- cbind(mu_vec, wStar^2)
#   M <- t(mat_nab) %*% diag(1 / u_vec) %*% mat_nab
#   M_inv <- solve(M)
#   targ_vec <- c(-lRtarg, Ctarg)
#   lambdas_out <- 2 * (M_inv %*% targ_vec)
# lambdas_slack <- lambdas_in - lambdas_out
# return(lambdas_slack)
#   
# }
# 
# Rtarg <- 0.0357
# l_R_in <- -runif(1) * 1000
# l_C_in <- runif(1)
# lambdas_in <- c(l_R_in, l_C_in)
# out_nleqslv <- nleqslv::nleqslv(lambdas_in, rootfn_lambdas, jac = NULL, Rtarg, Ctarg, nab_decRet, eigvals)
# out_nleqslv$x
# out_nleqslv$fvec
# out_nleqslv$message

# rho_fun <- function(k, u_vec, mu_vec, x_vec, n){
#   a_vec <- sqrt(27 * u_vec^2 + k * mu_vec^3) + sqrt(27) * u_vec
#   beta_vec <- a_vec^(1 / 3) - sign(k) * abs(k)^(1 / 3) * mu_vec * a_vec^(-1 / 3)
#   rho <- as.numeric(t(x_vec) %*% beta_vec^n)
#   return(list(rho, beta_vec))
# }

# k_lo <- 27 * min(-eigvals^2 / nab_decRet^3)
# k_lo <- 27 * max(-eigvals^2 / nab_decRet^3)
#k_lo <- min(-nab_decRet^3 / (27 * eigvals^2))

# k_lo <- max(-nab_decRet^3 / (27 * eigvals^2))
# k_up <- -0.001

optimize_portfolio_dimRet <- function(k, nab_decRet, eigvals, Ctarg){
  u_vec <- as.numeric(eigvals)
  mu_vec <- as.numeric(nab_decRet)
  term <- -k * u_vec^2 / mu_vec^3
  
  theta_vec <- acos(3 * sqrt(3) * sqrt(term))
  rho_vec <- 2 / sqrt(3) * (diag(sqrt(mu_vec)) %*% as.matrix(cos(theta_vec / 3)))
  #---
  # a_vec <- sqrt((-k) * mu_vec^3) * (sqrt(27 * u_vec^2 / ((-k) * mu_vec^3) - 1) + sqrt(27 * u_vec^2 / ((-k) * mu_vec^3)))
  # k_to1o3 <- sign(k) * abs(k)^(1 / 3)
  # a_vec <- sqrt(27 * u_vec^2 + k * mu_vec^3) + sqrt(27) * u_vec
  # rho_vec <- 1 / sqrt(3) * (a_vec^(1 / 3) - k_to1o3 * mu_vec * a_vec^(-1 / 3))
#---
  ones_vec <- rep(1, length(u_vec))
  rho_C <- as.numeric(t(ones_vec) %*% rho_vec)
  # l_C <- (rho_C / Ctarg)^3
  # l_R <- k_to1o3 * l_C^(1 / 3)
  # wStar <- l_C^(-1 / 3) * rho_vec
  r_lRlC <- (Ctarg / rho_C)^2
  wStar <- sqrt(r_lRlC) * rho_vec
  R <- exp(-mu_vec %*% wStar^-1)
  s <- sqrt(u_vec %*% wStar^-2)
  l_R <- -1 / sqrt(-k * r_lRlC)
  l_C <- -l_R / r_lRlC
  #--
  Rreal <- mu_vec %*% wStar
  sreal <- sqrt(u_vec %*% wStar^2)
  #--
  frontier_vec <- c(R, s, Rreal, sreal, l_R, l_C)
  outlist <- list(frontier_vec, wStar)
  return(outlist)
}


#=========================================================================

get_optimal_frontier_dimRet <- function(nab_decRet, eigvals, n_points_on_frontier, backtest_info = NULL){
k_lo <- max(-nab_decRet^3 / (27 * eigvals^2))
k_up <- k_lo * 10^-1
k_vec <- seq(k_lo, k_up, length.out = n_points_on_frontier)
list_frontier <- list()
list_wStar <- list()
for(i in 1:n_points_on_frontier){
  k <- k_vec[i]
  outlist <- optimize_portfolio_dimRet(k, nab_decRet, eigvals, Ctarg)
  frontier_vec <- outlist[[1]]
  wStar <- outlist[[2]]
  
  if(!is.null(backtest_info)){
    nab_decRet_test <- backtest_info[["nab_decRet_test"]]
    covmat_test <- backtest_info[["covmat_test"]]
    Rtest <- exp(-nab_decRet_test %*% wStar^-1)
    stest <- sqrt(t(wStar^-1) %*% covmat_test %*% wStar^-1)
    Rreal_test <- nab_decRet_test %*% wStar
    sreal_test <- t(wStar) %*% covmat_test %*% wStar
    frontier_vec <- c(frontier_vec, Rtest, stest, Rreal_test, sreal_test)
  }else{
    frontier_vec <- c(frontier_vec, rep(NA, 4))
  }
  
  list_frontier[[i]] <- frontier_vec
  list_wStar[[i]] <- wStar
}

df_frontier <- as.data.frame(do.call(rbind, list_frontier))
colnames(df_frontier) <- c("Obj fn target",
                           "Obj fn risk",
                           "ROI target",
                            "Risk (standard deviation)",
                             "ROI shadow price",
                             "Budget shadow price",
                           "Obj fn backtest",
                           "Obj fn risk backtest",
                             "ROI backtest",
                             "Risk backtest")
  df_wStar <- data.frame(df_frontier$`Risk (standard deviation)`, t(do.call(cbind, list_wStar)))
  varNames_ordered <- names(nab_decRet)
  colnames(df_wStar) <- c("Risk (standard deviation)", varNames_ordered)
  outlist <- list(df_frontier, df_wStar)
  
}




#=========================================================================
#=========================================================================
#=========================================================================
#=========================================================================
mat_P_sigs <- mat_P[, 1:n_signals]
mat_Sx_train <- mat_decDiff_eg_train %*% mat_P_sigs
#----
mat_P_sigs_test <- mat_P_test[, 1:n_signals]
mat_Sx_test <- mat_decDiff_eg_test %*% mat_P_sigs_test
eig_values_sigs_test <- eig_values_test[1:n_signals]
#----
nab_decRet_sigs_train <- (apply(mat_Sx_train, 2, function(x) prod(1 + x)) - 1)
names(nab_decRet_sigs_train) <- paste("Signal", 1:n_signals)
nab_decRet_sigs_test <- (apply(mat_Sx_test, 2, function(x) prod(1 + x)) - 1)
#----
nab_decRet_sigs_test <- nab_decRet_sigs_test[ind_rearrange]
#----
mat_U <- diag(eig_values_sigs)
mat_U_test <- diag(eig_values_sigs_test[ind_rearrange])
#=========================================================================
# dimRet:
Ctarg <- 1
covmat <- mat_U
eigvals <- diag(covmat)
covmat_test <- mat_U_test
nab_decRet <- nab_decRet_sigs_train
nab_decRet_test <- nab_decRet_sigs_test
# covmat <- diag(eig_values)
# covmat_test <- diag(eig_values_test)
# nab_decRet <- nab_decRet_eg_train
# nab_decRet_test <- nab_decRet_eg_test
#---------------------------------------
backtest_info <- list()
backtest_info[["nab_decRet_test"]] <- nab_decRet_test
backtest_info[["covmat_test"]] <- covmat_test
#-------------------------------------------------------------------------
# eigvals <- eigen(covmat_decDiff_train)$values
# nab_decRet <- nab_decRet_eg_train
# backtest_info[["nab_decRet_test"]] <- nab_decRet_eg_test
# backtest_info[["covmat_test"]] <- covmat_decDiff_test
outlist <- get_optimal_frontier_dimRet(nab_decRet, eigvals, n_points_on_frontier, backtest_info)
df_frontier <- outlist[[1]]
df_wStar <- outlist[[2]]
#df_frontier <- round(df_frontier, 4)

gg <- plot_frontier(df_frontier, ROI_basis = F, list_graph_options = NULL, graph_on = F)
gg_frontier <- gg
#--------------------------------------------------------------
n_items <- ncol(df_wStar) - 1
bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_items)
color_vec_mv_eg <- sample(bag_of_colors, n_items)
gg_budget <- plot_budgetShares(df_wStar, color_vec = color_vec_mv_eg, graph_on = F, list_graph_options = NULL)
#--------------------------------------------------------------
gg_frontier + gg_budget + plot_layout(ncol = 1)


```

mapping signal shares back to individual portfolio item shares
...must preserve the ordering of the signals, and reflect each item's contribution to each signal, and meet the following conditions:

wStar > 0
C = sum(wStar) = sum(wStar_sigs)
Rreal = nab_decRet_sigs %*% wStar_sigs = nab_decRet %*% wStar

One candidate is readily found by recalling the definition of signals (eq ....) and extrapolating

wStar = Psquig %*% wStar_sigs
[Actually: wStar = Psquig %*% diag(betas) %*% wStar_sigs]

This has the benefit that, by definition, condition 3 is fulfilled.

Psquig may contain positive and negative entries, meaning that wStar may contain negative budget shares. However, recall that the eigenvectors in Psqug are only defined up to scaling. This allows one to perform an operation on wStar that forces all of its entries to be positive (while also meeting conditions 2 and 3). This then fixes the free parameters in the signals definition.


<!-- Inaccuracy in the efficient frontier due to noisy covariance matrices severely limits the usefulness of MV Analysis in everyday practice [@Michaud...]. A backtest of the efficient frontiers from the example pursued in this paper reaffirms this (Figure \ref{fig:backtest}). It stands to reason that replacement of the data covariance matrix with a signals derived covariance matrix could improve the accuracy of the efficient frontier. But the signals derived covariance matrix contains eigenvalues equal to zero, and is thus is not invertible; and hence cannot be used in MV Analysis. Nonetheless, there is an alternative route to the same goal of reducing noise in MV Analysis: the portfolio of assets can be replaced with a portfolio of signals. -->

<!-- That is to say, instead of optimizing over a portfolio of $n$ assets, the problem is reduced to one of optimizing over just the $k$ retained signals. Instead of using the data covariance matrix in MV Analysis, the signals covariance matrix is used, which is just the first $k$ rows and columns of the diagonal eigenvalue matrix $\tilde{U}$ (recall equation \ref{eq:covmat_SS}). The vector of expected returns to signals $\boldsymbol{\mu}_S$ can be aggregated up from the asset expected returns based on the . -->
<!-- <!-- The expected return to each signal can be calculated as the sum of the expected returns to each asset weighted by their loadings onto the given signal.  --> -->

<!-- \begin{equation} -->
<!-- \tilde{\mu} = \frac{\mathbf{p}_T' \mathbf{e} - \mathbf{p}_1' \mathbf{e}}{\mathbf{p}_1' \mathbf{e}} -->
<!-- \end{equation} -->

<!-- To test this idea, the signals portfolio efficient frontier and budget shares are calculated using the financial data, and displayed in Figure \ref{fig:sigPortfolio}. The backtest on the right side of the Figure indicates that the signals portfolio frontier is far more accurate than that of the conventional approach in Figure \ref{fig:backtest}. The real ROI even exceeds the expected ROI over a substantial part of the frontier. -->


<!-- MV Analysis then outputs the optimal budget shares to be allocated to each signal ($\mathbf{\tilde{w}}^*$), rather than to each asset. The disaggregation of these signal budget shares down to allocations to each individual portfolio item ($\mathbf{w}^*$) follows from the definition of signals in equation \ref{eq:sigs_def}. -->

<!-- \begin{equation} -->
<!-- \mathbf{w}^* = P\mathbf{\tilde{w}}^* -->
<!-- \label{eq:w_disagg} -->
<!-- \end{equation} -->


```{r, fig.show='hold', fig.width=5, fig.height=5, fig.align='center', fig.cap="\\label{fig:wDisagg}The asset budget shares derived from the signals."}

# Map the signal weights back to the assets
mat_wStar <- t(as.matrix(df_wStar[, -1]))
#colSums(mat_L %*% diag(1 / colSums(mat_L)))
mat_wStar_assets <- (mat_P_sigs %*% mat_wStar^-1)^-1
#mat_wStar_assets <- exp(mat_wStar_assets)
mat_wStar_assets <- apply(mat_wStar_assets, 2, function(x) x - min(x) * 1.1)
mat_wStar_assets <- apply(mat_wStar_assets, 2, function(x) x / sum(x))

df_wStar_assets <- data.frame(df_wStar$`Risk (standard deviation)`, t(mat_wStar_assets))
colnames(df_wStar_assets) <- c("Risk (standard deviation)", row.names(mat_Lrot))
gg_budget <- plot_budgetShares(df_wStar_assets, color_vec = color_vec, graph_on = F, list_graph_options = NULL)
gg_budget
# #==========================================================
# 
# 
# 
# # 
# covmat_XX <- diag(eigen(cov(mat_pctDiff_eg_train * 10^-2))$values)
# covmat_XX_test <- diag(eigen((cov(mat_pctDiff_eg_test * 10^-2)))$values)
# Vtarg <- c()
# ROItarg <- c()
# Vtarg_test <- c()
# ROItarg_test <- c()
# #
# for(i in 1:ncol(mat_wStar_assets)){
#   wStar <- mat_wStar_assets[, i]
#   Vtarg[i] <- t(wStar) %*% covmat_XX %*% wStar
#   ROItarg[i] <- wStar %*% nab_decRet_eg_train
#   Vtarg_test[i] <- t(wStar) %*% covmat_XX_test %*% wStar
#   ROItarg_test[i] <- wStar %*% nab_decRet_eg_test
# }
# #
# 
#    df_frontier <- data.frame(sqrt(Vtarg), ROItarg, sqrt(Vtarg_test), ROItarg_test)
#    colnames(df_frontier) <- c("Risk (standard deviation)", "ROI target", "Risk backtest", "ROI backtest")
#   #plot_frontier(df_frontier, ROI_basis = T, graph_on = F)
# 
# df_plot_train <- df_frontier[, c("Risk (standard deviation)", "ROI target")]
# df_plot_test <- df_frontier[, c("Risk backtest", "ROI backtest")]
# df_plot_train$Type = "Efficient frontier"
# df_plot_test$Type = "Backtest"
# colnames(df_plot_train)[1:2] <- c("Risk (standard deviation)", "ROI")
# colnames(df_plot_test)[1:2] <- c("Risk (standard deviation)", "ROI")
# df_plot <- rbind(df_plot_train, df_plot_test)
# 
# gg <- ggplot(df_plot, aes(x = `Risk (standard deviation)`, y = ROI))
# gg <- gg + geom_point()
# gg <- gg + scale_color_manual(values = c("blue", "black"))
# gg <- gg + labs(title = "Backtest")
# gg <- gg + theme(axis.title.y = element_blank(),
#                  legend.title = element_blank(),
#                  legend.position = "bottom",
#                  plot.title = element_text(face = "bold", size = 10))
# gg_backtest <- gg
# gg_backtest
# 
#   



```








# An illustrative example of AR4D risk adjusted portfolio optimization

In practice, AR4D risk adjusted portfolio optimization would come after Step 3 in the resource allocation workflow, either complementing or replacing the ranking of research proposals in order of priority that is slated to occur in Step 4. By this point in the resource allocation workflow, a list of research proposals would have been drawn up, and the expected impact of each proposal assessed and quantified. A hypothetical list of such proposals is presented in Figure \ref{fig:ExpPctRet_Examp}.^[The AR4D proposals are loosely grouped into four categories to facilitate interpretation of the gaphics, but there is no strict rule followed, and clearly some overlap, in the grouping.]
<!-- This quantification of net impacts is calculated over the dimensions of interest to the stakeholders. For example, if the ... then the net impact is an aggregation of the net benefits in each of these categories. -->

```{r, fig.show = "hold", fig.width = 5, fig.height = 3, fig.align = "left", fig.cap = "\\label{fig:ExpPctRet_Examp}A set of hypothetical AR4D proposals and their expected returns."}
#---------------------------------------------
prop_CSA <- c("Heat Tolerant\nBeans", "Coffee Agroforestry", "Digital Agriculture", "Low Emission\nSilvo-Pastoril")
econGrowth_CSA <- c(0.17, 0.28, 0.4, -0.35)
econEquality_CSA <- c(0.85, 0.32, 0.81, 0.27)
envSust_CSA <- c(0.65, 0.42, 0.5, 0.8)
nutrition_CSA <- c(0.25, 0.01, 0.009, 0.012)
#---------------------------------------------
# prop_socialCap <- c("Micro Finance\nand Insurance", "Multi-stakeholder\nPlatforms", "Market Access")
# econGrowth_socialCap <- c(0.2, 0.45, 0.34)
# econEquality_socialCap <- c(0.87, 0.62, 0.79)
# envSust_socialCap <- c(0.1, 0.1, 0)
#---------------------------------------------
prop_smallHolder <- c("Cassava for\nBio-ethanol", "Triple Purpose\nSweet Potato", "Dairy Cooperative", "Multi-stakeholder\nPlatforms")
econGrowth_smallHolder <- c(0.67, -0.31, 0.15, 0.4)
econEquality_smallHolder <- c(0.6, 0.75, 0.83, 0.81)
envSust_smallHolder <- c(0.32, 0.4, -0.52, 0.1)
nutrition_smallHolder <- c(0, 0.76, 0.67, 0.11)
#---------------------------------------------
prop_highYcommod <- c("Mega Maize", "Hyper Rice", "Ultra Cow")#, "Uber Palm")
econGrowth_highYcommod <- c(0.82, 0.86, 0.76)#, 0.88)
econEquality_highYcommod <- c(-0.34, -0.42, -0.63)#, -0.58)
envSust_highYcommod <- c(-0.6, -0.52, -0.67)#, -0.83)
nutrition_highYcommod <- c(0.6, 0.65, 0.68)
#---------------------------------------------
df_CSA <- data.frame(Proposal = prop_CSA, `Economic Growth` = econGrowth_CSA, `Economic Equality` = econEquality_CSA, `Environmental Sustainability` = envSust_CSA, `Nutritional Security` = nutrition_CSA, Group = "Climate Smart\nAgriculture")

# df_socialCap <- data.frame(Proposal = prop_socialCap, `Economic Growth` = econGrowth_socialCap, `Economic Equality` = econEquality_socialCap, `Environmental Sustainability` = envSust_socialCap, Group = "Social Capital")

df_smallHolder <- data.frame(Proposal = prop_smallHolder, `Economic Growth` = econGrowth_smallHolder, `Economic Equality` = econEquality_smallHolder, `Environmental Sustainability` = envSust_smallHolder, `Nutritional Security` = nutrition_smallHolder, Group = "Smallholder\nResilience")

df_highYcommod <- data.frame(Proposal = prop_highYcommod, `Economic Growth` = econGrowth_highYcommod, `Economic Equality` = econEquality_highYcommod, `Environmental Sustainability` = envSust_highYcommod, `Nutritional Security` = nutrition_highYcommod, Group = "High Value Yield\nEnhancement")

list_df <- list(df_highYcommod, df_smallHolder, df_CSA)#, df_socialCap)

df_Lrot <- as.data.frame(do.call(rbind, list_df))
colnames(df_Lrot)[2:4] <- gsub("\\.", " ", colnames(df_Lrot)[2:(ncol(df_Lrot) - 1)])
group_names <- as.character(unique(df_Lrot$Group))
n_groups <- length(group_names)
bag_of_colors <- randomcoloR::distinctColorPalette(k = 2 * n_groups)
group_colors_arb <- sample(bag_of_colors, n_groups)

# Randomly assign an expected pct. return to each AR4D proposal
n_prop <- nrow(df_Lrot)
n_signals <- ncol(df_Lrot) - 2
# nab_pctRet_ar4d <- exp(rnorm(n_prop))
# nab_pctRet_ar4d <- round(nab_pctRet_ar4d / sum(nab_pctRet_ar4d), 3) * 100
nab_pctRet_ar4d <- runif(n_prop)
nab_pctRet_ar4d <- round(nab_pctRet_ar4d / sum(nab_pctRet_ar4d), 3) * 100
df_pctRet <- data.frame(nab_pctRet_ar4d, Item = df_Lrot$Proposal, Group = df_Lrot$Group)
names(nab_pctRet_ar4d) <- df_Lrot$Proposal
# Plot expected returns for each AR4D proposal
group_colors <- group_colors_arb
#plot_returns_barchart(df_pctRet, group_colors)
list_graph_options <- list()
list_graph_options[["fig_title"]] <- NULL
list_graph_options[["ylab"]] <- "Percent"
list_graph_options[["legend_position"]] <- NULL
plot_returns_barchart(df_pctRet, group_colors, list_graph_options, graph_on = F)

```

The data required to calculate an AR4D proposal covariance matrix does not generally exist. However, the diagonal elements of such a matrix, i.e. the variance of each proposal, can be elicited from stakeholders by asking, for example, what they think the maximum, minimum, and most probable impact of each given proposal will be. The proposal impact variance can then be computed on the basis of a triangular distribution as


\begin{equation}
\sigma^2 = \frac{y_{low}^2 + y_{high}^2 + y_{likely}^2 - y_{low}y_{high} - y_{low}y_{likely}-y_{high}y_{likely}}{18}
\end{equation}

[triangular density graphic?]
If, for whatever reason, it is not possible to estimate these variances, then they can be scaled to unity. In the hypothetical resource allocation exercise at hand, the standard deviations were elicited from stakeholders, and appear along the diagonal of the AR4D proposal covariance matrix in Figure .... No estimate is yet available for the covariances, and so these are left blank for now.

```{r, fig.show='hold', fig.width=6, fig.height=4, fig.align='center', fig.cap="\\label{fig:covMat_ar4d}The AR4D proposal covariance matrix, with proposal risk assessments (along the diagonal) elicited from stakeholders. The off-diagonal elements, i.e. the covariances between proposals, remain unknown."}

fig_title <- "Covariances Unknown"
D_sd2X <- matrix(NA, n_prop, n_prop)
row.names(D_sd2X) <- as.character(df_pctRet$Item)
colnames(D_sd2X) <- as.character(df_pctRet$Item)
sd2X_vec <- 1 * runif(n_prop)
diag(D_sd2X) <- sd2X_vec
covmat_XX <- D_sd2X
plot_covmat(covmat_XX, fig_title, graph_on = F)


```

Usually, signals are deduced from data, or from the data covariance matrix. If no data exists, then the analysis is abandoned. However, equation ... effectively means that, in the absence of data, the data covariance matrix can be "reverse engineered" from a (rotated) loadings matrix deduced from domain knowledge. The "reverse engineered" matrix is an approximation of the data covariance matrix, but is not necessarily less accurate than the data covariance matrix. 

In the present hypothetical example, a "signal" can be thought of as a development process that the stakeholders have at the forefront of their minds. Signals can also be thought of as the key dimensions that best describe the evolution of the proposed AR4D activities and outcomes, within a particular problem space that is of interest to the stakeholders. These key dimensions can be elicited by asking stakeholders questions such as "What are our key AR4D objectives? And what are the key metrics by which to measure progress towards those objectives?" In the AR4D context, there is often pre-existing consensus on such questions, which can be found laid out in strategy documents. The Sustainable Development Goals may be considered a set of dimensions defining a very broad problem space. The key dimensions of a particular AR4D portfolio space could be selected as a subset of these. The number of portfolio dimensions must be less than the number of portfolio items.

In the hypothetical example presented here, the stakeholders have decided that Economic Growth, Income Equality, Environmental Sustainability, and Nutritional Security are the dimensions that best register the evolution of the proposals under consideration. The next step is to invite the stakeholders to rate, from $-100$ to $100$, each research proposal in terms of its contribution towards each of these objectives. A positive rating means the proposal contributes toward the objective; a negative rating means the proposal works against the objective; and a rating of zero means that the proposal has no influence over the given objective. The hypothetical results of such an exercise are presented in Figure \ref{fig:loadsRotExamp}. These ratings are then divided by $100$, and the matrix as a whole is interpreted as an orthogonal rotation of the matrix of correlations between AR4D proposals and signals ($L_{\circlearrowleft}$).

```{r, fig.show = "hold", fig.width = 6, fig.height = 3, fig.align = "center", fig.cap = "\\label{fig:loadsRotExamp}A hypothetical example of rotated signal loadings elicited from a stakeholder survey."}
df_x <- df_Lrot
df_x$Group <- NULL
prop_col <- df_x$Proposal
df_x$Proposal <- NULL
mat_Lrot <- as.matrix(df_x)
rownames(mat_Lrot) <- prop_col
list_groups <- list(prop_highYcommod, prop_smallHolder, prop_CSA)
# n_groups <- length(list_groups)
# bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_groups)
# group_colors <- sample(bag_of_colors, n_groups)
group_info <- list(list_groups, group_names, group_colors)

sigNames <- c("GDP Growth", "Economic\nEquality", "Environmental\nSustainability", "Nutritional\nSecurity")
xAxis_title <- "Rotated Correlations"
plot_corrXS_barchart(mat_Lrot, group_info, xAxis_title, sigNames)

```



<!-- This would be the case, for example, of a yield enhancing variety that requires increased use of chemical inputs that degrade soils, pollute water sources, and pose health risks. Such a technology might contribute toward the economic growth objective, but works against the environmental sustainability objective. Likewise, it is customary in AR4D communities to assume that a tradeoff exists between the objectives of economic growth and economic equality [@Alston].^{Recent empirical studies have cast doubt on this idea [@;@;@].} Such tradeoffs are inherent in any research proposal. It is critical that stakeholders acknowledge them.-->


<!-- #```{r} -->
<!-- #D_sdX <- diag(apply(mat_pctDiff_train_mv, 2, sd)) -->
<!-- # n_proj <- 11 -->
<!-- # n_sig <- 4 -->
<!-- # D_sdX <- diag(runif(n_proj)) #stand devs of projects, elicited -->
<!-- # mat_Lrot_eg <- matrix(rnorm(66), n_proj, n_sig) -->
<!-- # mat_Lrot_eg[which(mat_Lrot_eg > 1)] <- 0.98 -->
<!-- # mat_Lrot_eg[which(mat_Lrot_eg < -1)] <- -0.98 #rotated loadings (X<->S correlations), elicited -->
<!-- # #Now derive U, R, P -->
<!-- # Q <- D_sdX %*% mat_Lrot_eg -->
<!-- # R <- t(eigen(t(Q) %*% Q)$vectors) #orthog rotation matrix -->
<!-- # eigvals <- eigen(t(Q) %*% Q)$values #signal variances -->
<!-- # P_dervd <- D_sdX %*% mat_Lrot_eg %*% t(R) %*% diag(1 / eigvals^(1 / 2)) #eigenvectors of covmat of X -->
<!-- # round(t(P_dervd) %*% P_dervd, 3) #check orthogonality -->
<!-- #``` -->

As shown in section 3.2, the implicit covariance matrix ($\hat{\Sigma}$) can then be found by multiplying $L_{\circlearrowleft}L_{\circlearrowleft}'$. The result is presented in Figure \ref{fig:covmatProps}. This crowdsourced approximation to a data covariance matrix offers insight into the implicit covariances---i.e. tradeoffs and synergies---between AR4D proposals in the portfolio. This is remarkable in and of itself and may be useful for orienting stakeholder discussion. For risk adjusted portfolio optimization, we must turn to the signals portfolio.

```{r, fig.show = "hold", fig.width = 6, fig.height = 4, fig.align = "center", fig.cap = "\\label{fig:covmatProps}AR4D proposal covariance matrix derived from the elicited loadings."}

covmat_XX_derived <- mat_Lrot %*% t(mat_Lrot)
fig_title <- "Crowdsourced AR4D Proposal Covariance Matrix"
plot_covmat(covmat_XX_derived, fig_title, graph_on = F)



```

Nonetheless, as discussed in section 3.3, optimal frontier and budget shares can be obtained via another route. The signals portfolio is optimized. Individual AR4D proposal budget shares can then be disaggregated from these    To determine the optimal resource allocation, the signals portfolio is optimized.

In order to optimize the signals portfolio, the signal expected returns and covariance matrix are required.

<!-- Signal expected returns can be aggregated up from the expected returns to each portfolio item via equation \ref{eq:}. -->
...
Having thus deduced the signals covariance matrix $U$ and vector of expected returns $\boldsymbol{\mu}$ from the elicited information $L_{\circlearrowleft}$ and $\boldsymbol{\sigma}$, the optimal frontier and budget shares for the signals portfolio are calculated in Figure \ref{fig:mvAR4D}.

```{r, fig.show='hold', fig.width=5, fig.height=5, fig.align='center', fig.cap="\\label{fig:mvAR4D}Optimal frontier and budget shares for the AR4D signals portfolio.", echo = FALSE}
#--------------------------------------------------------------
# Covariance matrix
# D_sdX_inv <- diag(10^2 / sqrt(sd2X_vec))
D_sdX_inv <- diag(sqrt(1 / diag(covmat_XX_derived)))
mat_Q <- D_sdX_inv %*% mat_Lrot
eig_decomp_QQ <- eigen(t(mat_Q) %*% mat_Q)
eig_values_QQ <- eig_decomp_QQ$values
#eig_values_QQ <- 10^-6 * eig_values_QQ
mat_U <- diag(eig_values_QQ)
covmat_SS <- mat_U
#--------------------------------------------------------
# Get mat_P
mat_U_sqrt_inv <- diag(1 / sqrt(eig_values_QQ))
mat_R <- t(eig_decomp_QQ$vectors) # * 10^3
mat_P <- mat_Q %*% t(mat_R) %*% mat_U_sqrt_inv
if(mean(mat_P[, 1]) < 0){mat_P <- -mat_P}
mat_P_sigs <- mat_P[, 1:n_signals]
#--------------------------------------------------------------
# Expected returns vector
#nab_pctRet_ar4d_sigs <- as.numeric(t(10^-2 * nab_pctRet_ar4d) %*% mat_P_sigs)
nab_pctRet_ar4d_sigs <- as.numeric(t(10^-2 * nab_pctRet_ar4d) %*% mat_Lrot)
names(nab_pctRet_ar4d_sigs) <- sigNames
#--------------------------------------------------------------
mat_nab <- cbind(nab_pctRet_ar4d_sigs, nab_C)
n_points_on_frontier <- 20
Rtarg_limits <- c(0.11, 0.19)
backtest_info <- NULL
#--------------------------------------------------------------
covmat <- covmat_SS
eigvals <- diag(covmat_SS)
#covmat_test <- mat_U_test
nab_decRet <- nab_pctRet_ar4d_sigs
#nab_decRet_test <- nab_decRet_sigs_test
backtest_info <- NULL
#--------------------------------------------------------------
outlist <- get_optimal_frontier_dimRet(nab_decRet, eigvals, n_points_on_frontier, backtest_info)
df_frontier <- outlist[[1]]
df_wStar <- outlist[[2]]
#--------------------------------------------------------------
gg <- plot_frontier(df_frontier, ROI_basis = F, list_graph_options = NULL, graph_on = F)
gg_frontier <- gg
n_items <- ncol(df_wStar) - 1
bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_items)
color_vec_mv_eg <- sample(bag_of_colors, n_items)
gg_budget <- plot_budgetShares(df_wStar, color_vec = color_vec_mv_eg, graph_on = F, list_graph_options = NULL)
#--------------------------------------------------------------
gg_frontier + gg_budget + plot_layout(ncol = 1)
```

The budget allocation to each AR4D proposal is then disaggregated from the signal budget shares by equation \ref{eq:}. The matrix of `r n_signals` leading eigenvectors $P$ of the proposal covariance matrix is required in order to perform this last step. Having deduced $R$ and $U$ above, it is possible to deduce $P$ as follows.

\begin{eqnarray}
Q R' U^{-{1 \over 2}} &=& PU^{1 \over 2}RR'U^{-{1 \over 2}} \\ &=& P
\end{eqnarray}

Having deduced $P$, the optimal AR4D proposal budget allocation is calculated in Figure \ref{fig:wDisagg_ar4d}.

```{r, fig.show='hold', fig.width=5, fig.height=5, fig.align='center', fig.cap="\\label{fig:wDisagg_ar4d}AR4D proposal budget allocation derived from the signal budget shares."}
#--------------------------------------------------------
# Disaggregate budget shares to portfolio items
mat_wStar <- t(as.matrix(df_wStar[, -1]))
#colSums(mat_L %*% diag(1 / colSums(mat_L)))
mat_wStar_assets <- mat_P_sigs %*% mat_wStar
mat_wStar_assets <- exp(mat_wStar_assets)
mat_wStar_assets <- apply(mat_wStar_assets, 2, function(x) x / sum(x))
df_wStar_assets <- data.frame(df_wStar$`Risk (standard deviation)`, t(mat_wStar_assets))
colnames(df_wStar_assets) <- c("Risk (standard deviation)", row.names(mat_Lrot))
gg_budget <- plot_budgetShares(df_wStar_assets, graph_on = F, list_graph_options = NULL)
gg_budget

```


Interpretation of the example. Note that proposals with low expected return do not necessarily correspond to a low budget share...vice versa...Note how the optimal solution changes with risk tolerance. At lower risk tolerances, the optimal investment focuses on economic growth, followed by environmental sustainability and economic equality. However, as risk tolerance increases, the optimal investment dictates that investments in environmental sustainability be displaced by investments in economic growth and, to a lesser extent, economic equality.

-elicitation can be accomplished in a variety of ways (eg.: online surveys, in-person events, consultation of strategy documents, some combination thereof, etc.).

* May not be exactly "accurate", but generates an optimal solution that is consistent with what we think we know about the domain. Moreover, it should kept in mind that data are not infallible sources of information. Data of the sort considered here, in particular, are inherently noisy. Under such circumstances, arguably, it may be preferible to build up the covariance matrix from crowdsourced domain knowledge, in the manner shown above, rather than calculate it from data.





















































The dimension covariance and correlation matrices are given in Figure \ref{fig:covmatDim}.

<!-- #```{r, fig.show = "hold", fig.width = 6, fig.height = 4, fig.align = "center", fig.cap = "\\label{fig:covmatDim}Implicit dimensional covariance and correlation matrices."} -->

<!-- covmat <- t(mat_Lrot) %*% mat_Lrot -->
<!-- sd_vec <- diag(sqrt(covmat)) -->
<!-- cormat <- diag(1 / sd_vec) %*% covmat %*% diag(1 / sd_vec) -->
<!-- rownames(cormat) <- rownames(covmat) -->
<!-- colnames(cormat) <- colnames(covmat) -->


<!-- n_col <- ncol(covmat) -->
<!-- df_plot <- covmat %>% tbl_df() -->
<!-- these_levels <- colnames(df_plot) -->
<!-- df_plot$V1 <- colnames(df_plot) -->
<!-- gathercols <- colnames(df_plot)[-ncol(df_plot)] -->
<!-- df_plot <- df_plot %>% gather_("V2", "Value", gathercols) -->

<!-- df_plot$V1 <- factor(df_plot$V1, levels = these_levels) -->
<!-- df_plot$V2 <- factor(df_plot$V2, levels = these_levels) -->

<!-- #midpoint <- min(covmat) + (max(covmat) - min(covmat)) / 2 -->
<!-- midpoint <- 0 -->
<!-- gg <- ggplot(df_plot, aes(V1, V2)) -->
<!-- gg <- gg + geom_tile(aes(fill = Value))#, width = 4, height = 4) -->
<!-- gg <- gg + geom_text(aes(label = round(Value, 2)), size = 3) -->
<!-- gg <- gg + theme(axis.text.x = element_text(angle = 60, hjust = 1), -->
<!--                  axis.title = element_blank(), -->
<!--                  legend.title = element_blank(), -->
<!--                  legend.position = "top", -->
<!--                  plot.title = element_text(size = 10)) -->
<!-- gg <- gg + labs(title = "Covariance Matrix") -->
<!-- gg <- gg + scale_fill_gradient2(low = "magenta", mid = "khaki", high = "cyan", midpoint) -->
<!-- gg_covmat <- gg -->


<!-- df_plot <- cormat %>% tbl_df() -->
<!-- these_levels <- colnames(df_plot) -->
<!-- df_plot$V1 <- colnames(df_plot) -->
<!-- gathercols <- colnames(df_plot)[-ncol(df_plot)] -->
<!-- df_plot <- df_plot %>% gather_("V2", "Value", gathercols) -->
<!-- df_plot$V1 <- factor(df_plot$V1, levels = these_levels) -->
<!-- df_plot$V2 <- factor(df_plot$V2, levels = these_levels) -->

<!-- #midpoint <- min(covmat) + (max(covmat) - min(covmat)) / 2 -->
<!-- midpoint <- 0 -->
<!-- gg <- ggplot(df_plot, aes(V1, V2)) -->
<!-- gg <- gg + geom_tile(aes(fill = Value))#, width = 4, height = 4) -->
<!-- gg <- gg + geom_text(aes(label = round(Value, 2)), size = 3) -->
<!-- gg <- gg + theme(axis.text.x = element_text(angle = 60, hjust = 1), -->
<!--                  axis.title = element_blank(), -->
<!--                  axis.text.y = element_blank(), -->
<!--                  legend.title = element_blank(), -->
<!--                  legend.position = "top", -->
<!--                  plot.title = element_text(size = 10)) -->
<!-- gg <- gg + labs(title = "Correlation Matrix") -->
<!-- gg <- gg + scale_fill_gradient2(low = "magenta", mid = "khaki", high = "cyan", midpoint) -->
<!-- gg_cormat <- gg -->


<!-- gg_covmat | gg_cormat -->

<!-- #kappa(covmat) -->
<!-- #eigen(covmat)$values -->
<!-- #eigen(covmat)$vectors -->

<!-- # R <- eigen(covmat)$vectors -->
<!-- # mat_L <- mat_Lrot %*% t(R) -->
<!-- #  -->
<!-- # mat_Lrot <- varimax(mat_L)[[1]] -->
<!-- # mat_Lrot <- matrix(as.numeric(mat_Lrot), attributes(mat_Lrot)$dim, dimnames=attributes(mat_Lrot)$dimnames) -->
<!-- #  -->
<!-- # interpret_loadings(mat_Lrot, fun_env) -->


<!-- #``` -->

When approached in this way, the portfolio optimization problem is dimensionally reduced from fourteen proposals to just three crosscutting signals. Each signal may itself be viewed as a _thematic_ AR4D portfolio, i.e. as a set of investments that is weighted towards a particular development theme (in this case, either Economic Growth, Economic Equality, or Environmental Sustainability).

If the return to proposal $i$ in dimension $j$ can be expressedThen the expected return to each signal can be calculatedwhere $n$ is the number of proposals in the portfolio.


<!-- #```{r, fig.show='hold', fig.width=5, fig.height=4, fig.align='center', fig.cap="\\label{fig:AR4DFrontier}Hypothetical example of the optimal frontier and budget shares in the AR4D context.", echo = FALSE} -->

<!-- # sum_Lrows <- as.numeric(mat_Lrot %*% as.matrix(rep(1, n_signals))) -->
<!-- # mat_Lrot_normdByRows <- diag(1 / sum_Lrows) %*% mat_Lrot -->
<!-- # #rowSums(mat_Lrot_normdByRows) -->
<!-- # rownames(mat_Lrot_normdByRows) <- rownames(mat_Lrot) -->
<!-- # mat_mu <- diag(mu_pctRet) %*% mat_Lrot_normdByRows -->
<!-- # #mat_mu %*% as.matrix(rep(1, n_signals)) - as.matrix(mu_pctRet) -->
<!-- # #mu_pctRet_dims <- colSums(mat_mu) -->
<!-- # mu_pctRet_dims <- colMeans(mat_mu) -->
<!-- # #------------------------------------------------------------- -->
<!-- # nab_pctRet <- mu_pctRet_dims -->
<!-- # nab_C <- rep(1, n_signals) -->
<!-- # mat_nab <- cbind(nab_pctRet, nab_C) -->
<!-- #  -->
<!-- # n_points_on_frontier <- 50 -->
<!-- # fun_env <- list(n_points_on_frontier, -->
<!-- #                 utility_interpretation = T, -->
<!-- #                 backtest_info = NULL, -->
<!-- #                 frontier_and_budget_plot = T, -->
<!-- #                 group_info = NULL, -->
<!-- #                 group_colors = NULL, -->
<!-- #                 group_small = NULL, -->
<!-- #                 C_targ = 1, -->
<!-- #                 fig_title = NULL) -->
<!-- # AR4D_eg_rTarg_limits <- c(0.08, .6) -->
<!-- # list_out <- get_optimal_frontier(covmat, mat_nab, -->
<!-- #                                  Rtarg_limits = AR4D_eg_rTarg_limits, -->
<!-- #                                  fun_env) -->
<!-- #  -->
<!-- # df_wStar <- list_out[[1]] -->
<!-- # df_frontier <- list_out[[2]] -->
<!-- # outlist_gg <- list_out[[4]] -->
<!-- # gg_frontier <- outlist_gg[1] -->
<!-- # gg_weights <- outlist_gg[2] -->
<!-- #  -->
<!-- # gg_frontier[[1]] + gg_weights[[1]] + plot_layout(ncol = 1) -->

<!-- #(eigen(covmat)$vectors)^2 %*% as.matrix(mu_pctRet_dims) -->

<!-- #``` -->


The resource allocation to the individual AR4D proposals within each of these dimensions is calculated as follows.,,,The optimal budget allocation to the $i^{th}$ proposal then follows as



<!-- # ```{r, fig.show='hold', fig.width=7, fig.height=5, fig.align='center', fig.cap="\\label{fig:propWgts}dfgg.", echo = FALSE} -->

<!-- # fun_env <- list(n_points_on_frontier, -->
<!-- #                 utility_interpretation = F, -->
<!-- #                 backtest_info = NULL, -->
<!-- #                 frontier_and_budget_plot = T, -->
<!-- #                 group_info = NULL, -->
<!-- #                 group_colors = NULL, -->
<!-- #                 group_small = NULL, -->
<!-- #                 C_targ = 1, -->
<!-- #                 fig_title = NULL) -->
<!-- #  -->
<!-- # list_out <- get_optimal_frontier(covmat, mat_nab, -->
<!-- #                                  Rtarg_limits = AR4D_eg_rTarg_limits, -->
<!-- #                                  fun_env) -->
<!-- #  -->
<!-- # df_wStar <- list_out[[1]] -->
<!-- # #rowSums(df_wStar[, -1]) -->
<!-- #  -->
<!-- # mat_wStar <- as.matrix(df_wStar) -->
<!-- # #mat_Lrot_normdByCols <- mat_Lrot %*% diag(1 / colSums(mat_Lrot)) -->
<!-- # #colSums(mat_Lrot_normdByCols) -->
<!-- # mat_wStar_prop <- mat_Lrot %*% t(mat_wStar[, -1]) -->
<!-- # #colSums(mat_wStar_prop) -->
<!-- # mat_wStar_prop <- mat_wStar_prop %*% diag(1 / colSums(mat_wStar_prop)) -->
<!-- # mat_wStar_prop <- exp(mat_wStar_prop) -->
<!-- # mat_wStar_prop <- mat_wStar_prop %*% diag(1 / colSums(mat_wStar_prop)) -->
<!-- # # mat_wStar_prop <- mat_wStar_prop %*% diag(1 / colSums(mat_wStar_prop)) -->
<!-- # #colSums(mat_wStar_prop) -->
<!-- # mat_wStar_prop <- cbind(df_wStar$`Risk (standard deviation)`, t(mat_wStar_prop)) -->
<!-- # colnames(mat_wStar_prop) <- c(colnames(df_wStar)[1], rownames(mat_Lrot_normd)) -->
<!-- # df_wStar_prop <- as.data.frame(mat_wStar_prop) -->
<!-- #  -->
<!-- # gg_wgts <- plot_budgetShares(df_wStar_prop, n_points_on_frontier, graph_on = T, group_small = NULL, group_info = group_info, group_colors = group_colors_arb, legend_position = "bottom") -->
<!-- #  -->
<!-- # list_df <- list(df_highYcommod, df_CSA, df_socialCap, df_smallHolder) -->
<!-- # list_gg <- list() -->
<!-- # last_i <- length(list_df) -->
<!-- # axis_titles <- "off" -->
<!-- # Xaxis_numbers_off <- T -->
<!-- # for(i in 1:last_i){ -->
<!-- #   df_this <- list_df[[i]] -->
<!-- #   this_group <- unique(df_this$Group) -->
<!-- #   ind_these <- c(1, which(colnames(df_wStar_prop) %in% df_this$Proposal)) -->
<!-- #   df_wStar_prop_thisGroup <- df_wStar_prop[, ind_these] -->
<!-- #   if(i == last_i){ -->
<!-- #     axis_titles <- "x only" -->
<!-- #     Xaxis_numbers_off <- F -->
<!-- #     } -->
<!-- #   gg_wgts_thisGroup <- plot_budgetShares(df_wStar_prop_thisGroup, -->
<!-- #                                      n_points_on_frontier, graph_on = F, -->
<!-- #                                      group_small = NULL, group_info = NULL, -->
<!-- #                                      group_colors = NULL, -->
<!-- #                                      legend_position = "right", -->
<!-- #                                      fig_title = this_group, -->
<!-- #                                      axis_titles, -->
<!-- #                                      Xaxis_numbers_off) -->
<!-- #   list_gg[[i]] <- gg_wgts_thisGroup -->
<!-- # } -->
<!-- #  -->
<!-- # # ind_soCap <- c(1, which(colnames(df_wStar_prop) %in% df_socialCap$Proposal)) -->
<!-- # # df_wStar_prop_soCap <- df_wStar_prop[, ind_soCap] -->
<!-- # # gg_wgts_soCap <- plot_budgetShares(df_wStar_prop_soCap, n_points_on_frontier, graph_on = T, group_small = NULL, group_info = NULL, group_colors = NULL, legend_position = "right") -->
<!-- #  -->
<!-- # gg_wgts | (list_gg[[1]] / list_gg[[2]] / list_gg[[3]] / list_gg[[4]]) | plot_layout(widths = c(2, 1)) -->

<!-- ``` -->


# Discussion/Conclusion

The CGIAR is said to "have a long history of good intentions but limited success in developing appropriate approaches for priority setting" [@Birner2016]. It may be that the problem is cultural. Methodological limitations leave plenty of room for politics etc. to shape the decision... An historic effort begun in 2011 to restructure CGIAR financing around a "portfolio" model proved emblematic of this "limited success", when the results of an extensive priority setting exercise were rejected out of hand in favor of an ad hoc budget allocation. The end result thereby reinforced the very institutional inertia that the effort had intended to disrupt (Birner &amp; Byerlee, 2016). The aim of the present paper is to reduce the methodological basis for such failures as much as possible.   The methodological limitations of conventional priority setting are more to blame than any set of persons... Mill's missing fifth step... My aim is to improve methodological rigor and objectivity ...such that research conglomerates genuinely seeking to optimize resource allocation will have the tools required to achieve this. this is an issue faced across many fields [@physics, @medicine]
"long history of good intentions but limited success in developing appropriate approaches for priority setting". An historic effort begun in 2011 to restructure CGIAR financing around Core Research Programs (CRPs) was ad hoc, thereby reinforcing the very institutional inertia it was designed to disrupt [@Birner2016]. ReDonor attempts to establish criteria and methods for a merit-based allocation of ever-scarcer research funds are seen as invasive characterized as "development at the expense of research" [@Birner2016] or even the "Balkanization" of research [@Petsko2011]. "One of the geniuses" of CG centers is said to be their ability to consistently attract "aid funding of long-term research" while keeping "aid professionals from setting research agendas" [@mccalla2014cgiar]. Scientists respond with ultimatums of their own, demanding a return to the days of "more stable funding" and "budget autonomy" [@leeuwis2018reforming].
"One of the geniuses" of such institutes is said to be their ability to consistently attract "aid funding of long-term research" while keeping "aid professionals from setting research agendas and hiring scientists" (McCalla, 2014). Donor attempts to establish criteria and methods for a merit-based allocation of ever-scarcer research funds are characterized as
development at the expense of research (Birner &amp; Byerlee, 2016) or even the Balkanization of research (Petsko, 2011) .
* The backtest in ... suggests that the eigenvalue approach has the additional benefit of generating more accurate solutions (? not really since Rbar can be arbitrarily changed)









































































































<!-- ## Separating signals from noise, identifying main assets contributing to market movements -->

<!-- MV analysis is, in many ways, still a work in progress, with unresolved methodological issues severely limiting its usefulness even in its native financial context. In particular, covariance matrices estimated from price data tend to contain a lot of noise; and the optimal solutions then tend to diverge considerably from reality as a result of this [@michaud1989markowitz]. In my adaptation below, I propose a new approach to this problem by first dimensionally reducing the portfolio from several dozen assets to just a handful of "signals" constructed from the eigenvectors of the correlation matrix that can be meaningfully distinguished from noise. -->

<!-- The meaningful eigenvectors are separated out from the noisy eigenvectors using an old, but little-utilized technique developed in the study of physical systems. Once signals have been isolated, their meaning is interpreted based on how much the different portfolio items load onto them. A varimax rotation of the loadings matrix turns out to be of great assistance in this task. The interpretation of the signals is then further confirmed and illustrated by constructing the signal time series and then plotting them against their respective highest loading price series. The highest loading price series are seen to track their respective signals closely. -->






<!-- # Discussion and conclusion -->

<!-- Discussion -->

<!-- -AR4D resource allocation has been called for for a long time . -->
<!-- -There is a cultural impasses between donors and research institutes .  -->
<!-- -Much of the cultural impasse is premised upon a highly qualitative resource allocation procedure that is easily influenced by stakeholder pressure. basic lack of precise impartial tools at the portfolio level.  -->
<!-- -It stands to reason, then, that the elaboration of such tools could go a long way towards ameliorating the toxic environment.  -->
<!-- -[BUT] Adaptation of MV analysis to the AR4D context requires clearing some non-trivial methodological hurdles, which have also been of concern in the financial context: addressing the negative budget allocations and noise reduction. Here I have explored the possibility of redressing these issues in a novel ways (...) . -->
<!-- -Here I am mainly concerned with introduction and adaptation of these methods to the AR4D context. and pointing towards their potential. In the financial example, there is enough data to conduct a backtest. The results of the backtest are encouraging, but do not necessarily prove that the methods presented here will always generate portfolio frontiers that are more accurate than the conventional frontier. More empirical tests are necessary for that. -->


<!-- [end] -->

<!-- MV Analysis is still a work in progress even in the financial context where it originated. In my adaptation to the AR4D context, I have attempted to redress the issue of negative budget shares by replacing the revenue and cost functions with utility functions. To redress the issue of noise-induced inaccuracy, I have attempted to purge the noise from the portfolio by dimensionally reducing the portfolio to a handful of signals constructed from those eigenvectors of the data correlation matrix that can be meaningfully distinguished from noise. To isolate these signals, I have applied a criterion developed in the study of phsyical systems, more rigorous than the usual rules of thumb. A backtest of these modifications compares favorably to conventional MV Analysis. -->



<!-- #```{r, fig.show='hold', fig.width=6, fig.height=3, fig.align='center', fig.cap="\\label{fig:sigPortfolio_U}\\textit{(Left) }Optimal frontier and budget shares for the signals portfolio, using the utility approach to account for diminishing returns and to force positive budget shares. \\textit{(Right) } Backtest of the frontier."} -->
<!-- #-------------------------------------------------------------- -->
<!-- utility_interpretation <- T -->
<!-- #-------------------------------------------------------------- -->
<!-- Rtarg_limits <- c(10^(-9), 1.3) -->
<!-- #-------------------------------------------------------------- -->
<!-- fun_env_getOptFront[["Rtarg_limits"]] <- Rtarg_limits -->
<!-- fun_env_getOptFront[["utility_interpretation"]] <- utility_interpretation -->
<!-- #-------------------------------------------------------------- -->
<!-- list_out <- get_optimal_frontier(covmat_train, mat_nab, -->
<!--                                  fun_env = fun_env_getOptFront) -->
<!-- df_wStar <- list_out[[1]] -->
<!-- df_frontier <- list_out[[2]] -->
<!-- #-------------------------------------------------------------- -->
<!-- list_graph_options[["fig_title"]] <- "Efficient frontier and budget shares" -->
<!-- gg_frontier <- plot_frontier(df_frontier, ROI_basis = T, list_graph_options, graph_on = F) -->
<!-- #-------------------------------------------------------------- -->
<!-- # df_wStar_agg <- df_wStar %>% gather_("Item", "Budget Share", colnames(df_wStar)[-1]) -->
<!-- # df_wStar_agg <- merge(df_wStar_agg, df_match_group, by = "Item") -->
<!-- # df_wStar_agg <- df_wStar_agg %>% group_by(`Risk (standard deviation)`, Group) %>% summarise(`Budget Share` = sum(`Budget Share`)) %>% spread(Group, `Budget Share`) -->
<!-- #gg_budget <- plot_budgetShares(df_wStar_agg, group_small = NULL, color_vec = group_colors, graph_on = F, list_graph_options = NULL) -->
<!-- n_items <- ncol(df_wStar) - 1 -->
<!-- gg_budget <- plot_budgetShares(df_wStar, color_vec = color_vec_mv_sigs_eg, graph_on = F, list_graph_options = NULL) -->
<!-- #-------------------------------------------------------------- -->
<!-- df_plot_train <- df_frontier[, c("Risk (standard deviation)", "Return target")] -->
<!-- df_plot_test <- df_frontier[, c("Risk backtest", "Return backtest")] -->
<!-- df_plot_train$Type = "Optimal solution" -->
<!-- df_plot_test$Type = "Backtest" -->
<!-- colnames(df_plot_train)[1:2] <- c("Risk (standard deviation)", "Return") -->
<!-- colnames(df_plot_test)[1:2] <- c("Risk (standard deviation)", "Return") -->
<!-- df_plot <- rbind(df_plot_train, df_plot_test) -->

<!-- gg <- ggplot(df_plot, aes(x = `Risk (standard deviation)`, y = Return, group = Type, color = Type)) -->
<!-- gg <- gg + geom_point() -->
<!-- gg <- gg + scale_color_manual(values = c("blue", "black")) -->
<!-- gg <- gg + labs(title = "Backtest") -->
<!-- gg <- gg + theme(axis.title.y = element_blank(), -->
<!--                  legend.title = element_blank(), -->
<!--                  legend.position = "bottom", -->
<!--                  plot.title = element_text(face = "bold", size = 10)) -->
<!-- gg_backtest <- gg -->
<!-- #-------------------------------------------------------------- -->
<!-- (gg_frontier + gg_budget + plot_layout(ncol = 1)) | gg_backtest -->


<!-- #``` -->




\pagebreak

# References
