---
title: "Risk-adjusted optimization of agricultural research for development portfolios"
author:
  - name: Benjamin Schiek
    email: b.schiek@cgiar.org
    affiliation: CIAT
address:
  - code: CIAT
    address: Alliance Bioversity-CIAT, Km 17 Recta Cali-Palmira, Valle del Cauca, Colombia
abstract: |
 Conventional agricultural research for development (AR4D) priority setting exercises can generate insight into the strengths and weaknesses of individual agricultural research proposals, but they stop short of providing tools that can translate this insight into optimal resource allocation shares across a portfolio containing several such proposals. They also lack tools for risk accounting. Here I explore the possibility of redressing both methodological lacunae in one stroke by adapting Mean-Variance (MV) Analysis, a risk-adjusted portfolio optimization technique developed in financial contexts, to the AR4D portfolio optimization problem.
keywords: Risk-adjusted portfolio optmization, Mean-Variance Analysis, AR4D, ex-ante impact assessment, foresight
journal: Alliance Bioversity-CIAT Working Paper
bibliography: AR4D Portfolio Optimization.bib
header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{H}
#linenumbers: true
numbersections: true
output:
  rticles::elsevier_article:
    latex_engine: xelatex
mainfont: Garamond
---
<!-- https://tug.org/FontCatalogue/ -->
<!-- Source Serif Pro Light -->
<!-- rmarkdown::draft("Test.Rmd", template = "elsevier_article", package = "rticles") -->
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE)
#devtools::install_github("thomasp85/patchwork")
library(plyr)
library(tidyverse)
library(zoo)
library(tidyquant)
library(patchwork)
```

# Introduction

## Mills' missing fifth step

Long ago, Bradford Mills articulated the four steps of priority setting in agricultural research for development (AR4D) as follows [-@Mills1998]:

>Step one is the development of an information base. Step two is the establishment of research alternatives. Step three is the evaluation of the potential impact of research alternatives. Step four, often considered the final output of a priority-setting exercise, is the ranking of alternatives into research priorities.

However, he went on to observe that these steps were insufficient. An as of yet non-existent

>fifth step, the development of guidelines for translating priorities into planning and resource allocation decisions, is necessary to establish direct links with planning and resource allocation activities [@Mills1998].

Mills identified this lacuna at a time when AR4D centers were just beginning to come under pressure from public and private donors to "do a lot more with a lot less" [@Alston1995]. Methodological work in response to this pressure focused on Steps 1-4. The emergence and refinement of numerous ex-ante impact assessment models for the evaluation of individual research alternatives, in particular, is remarkable (see, for example, @Alston1995; @Antle1999; @Antle2015; Mills, 1998; and @nelson2014modeling). However, work has still not begun on Mills' missing fifth step.

```{r, fig.show = 'hold', out.width="15cm", fig.align='left'}
knitr::include_graphics("Mills_missing_step5.png")
```
In hindsight, Mills' concern was well founded. Budgetary pressure on AR4D centers to "prove their relevance" [@Braunschweig2000], "show value for money" [@yet2016bayesian], and otherwise demonstrate "more efficient spending of resources" [@petsakos2018comparing] has only increased since the 1990s. Between Step 4 and the final allocation of funds, stakeholder politics, institutional inertia, ad hoc procedures and criteria, and other subjective forces have repeatedly undercut any careful, objective rigor occurring in Steps 1-4 [@Birner2016; @mccalla2014cgiar]. This  inconsistency, opacity, and subjectivity in resource allocation decisions, has, in turn, aggravated scientists' already longstanding distrust of budget allocation mechanisms. Relations between donors and the research programs they fund have, as a result, reached an historic level of toxicity [@Birner2016; @leeuwis2018reforming; @mccalla2014cgiar].

A rigorous tool _at the portfolio level_, along the lines suggested by Mills over twenty years ago, could go a long ways in guaranteeing the consistency, transparency, and objectivity that is, in large part, responsible for this toxicity. [has been lacking from the resource allocation process.] It would also be nice if such a method took explicit account of the risk associated with each research alternative, and were quick and inexpensive to implement relative to consensus building mechanisms such as the Analytical Hierarchy Process [@Braunschweig2000]. In this paper, I explore the possibility of building such a tool based on risk-adjusted portfolio optimization techniques developed in the financial context.

## AR4D mean-variance analysis

In particular, I explore the possibility of adapting Mean-Variance (MV) Analysis, first conceptualized by Markowitz [@markowitz1952portfolio] and later formalized and refined by Merton [@merton1972analytic], to the AR4D context. In stock market investment contexts, MV Analysis is used to optimize the investor's return on investment in a portfolio of $n$ risky assets, given the investor's level of risk tolerance. On input, MV Analysis takes the expected returns and risk of each asset in the portfolio, and outputs the precise resource allocation that must be invested in each of the portfolio assets in order to achieve the risk adjusted maximum return. The optimal solution changes with the investor's risk tolerance. The locus of all solutions across all levels of risk tolerance is called the "efficient frontier". In the conventional approach, the math works out such that this frontier takes the shape of a parabola (Figure \ref{fig:basic_illust}). Parabolae give two solutions for each x-axis input. The investor is of course interested only in the greater of these, given by the upper branch of the parabola. The lower branch is therefore omitted.

```{r, fig.show = 'hold', fig.width=4, fig.height=3, fig.align='center', fig.cap="\\label{fig:basic_illust}The efficient frontier. Each point on the frontier indicates the highest expected return that can be obtained for a given risk tolerance.", echo=FALSE}
y <- seq(0, 1, length.out = 50)
a <- 1 / 2
b <- 1 / 3
x <- a^2 * (y^2 / b^2 + 1)
gg <- ggplot(data.frame(x, y), aes(x = x, y = y))
gg <- gg + geom_point()
gg <- gg + labs(x = "Portfolio Risk (standard deviation)", y = "Portfolio expected return")
gg <- gg + theme(axis.text = element_blank(),
                 axis.ticks = element_blank())
gg

```

There is an important sense in which this reflects AR4D decisionmaking contexts. Emeritus scientists at the International Center for Tropical Agriculture, for example, recall that in the early years of the institution---around the time that Merton was writing his seminal paper---it was common practice to plot research proposals in a risk reward space such as that in Figure \ref{fig:basic_illust}, and to thereby guide resource allocation decisions [@JCock_perscomm; @lynam2017forever]. In this paper, I am primarily concerned with the possibility of resuscitating this practice, and of moving it onto a rigorous methodological footing.

Of course, the AR4D context also differs from the financial context in many important respects. In  order to adapt MV Analysis to the AR4D context, it is necessary to address three of these divergent areas in particular: scalability, negative budget shares, and the lack of data by which to assess project tradeoffs and synergies.

_Scalability:_ In the financial context, returns scale linearly with investment. That is to say, the return on an investment of $\$200$ is twice as much as the return on an investment of $\$100$. Investments in the AR4D context, on the other hand, are subject to the law of diminishing marginal returns. Donors cannot double their impact merely by doubling their investments. Such returns are said to scale sub-linearly with investment.
[linear vs. subliear graphic?]

_Negative budget shares:_ To each point along the efficient frontier in Figure \ref{fig:basic_illust} corresponds an optimal budget allocation. MV Analysis allows that individual budget shares in this allocation can be, and frequently are, negative. A negative sign on a budget share indicates that the investor should invest in the inverse of the corresponding portfolio item. In the financial context, this is possible through short selling, or through investment in financial products that track the inverse of the given asset price.^[However, even in the financial context, negative weights are viewed by many as a methodological nuissance. See, for example, Boyle [-@boyle2014positive]. One of the main nuissances is that a portfolio with both negative and positive weights implies that the investor must borrow beyond their budget.] In the AR4D context, there is no analogue to either one of these conventions.

_Lack of a covariance matrix:_ MV Analysis requires information regarding the tradeoffs and synergies between portfolio items. Another word for these tradeoffs and synergies is covariance. A positive covariance may be considered a synergy, while a negative covariance is a tradeoff. In the financial context, covariances can be calculated from data. In the AR4D context, no such data exists.

After a brief introduction to MV Analysis in the following section, I redress each of these issues. In Section 3, I redress both the scalability and negative budget share issues by replacing the linear returns function used in MV Analysis with a logarithmic form derived from the law of diminishing marginal returns. In Section 4, I draw on principal components analysis to show how a lack of data can be compensated by domain knowledge in order to "reverse engineer" a covariance matrix. The deduced matrix is useful for orienting stakeholder discussions, but cannot be used in portfolio optimization. To find the optimal risk adjusted resource allocation, I introduce the "signals portfolio", a dimensionally reduced portfolio of principal components. The optimal resource allocation to individual portfolio items can be disaggregated from the optimal signals portfolio based on each item's correlation with each given signal. Finally I walk through a an illustrative example of what this AR4D-adapted version of MV Analysis might look like in practice.
[These asjustments may be of interest to the financial context... The introduction of diminishing marginal returns, which forces positive budget weights, compares favorably to the unmodified approach on an ROI basis...and in a backtest.]
<!-- Alston and Norton acknowledged in 1995 that the treatment of risk in impact assessment models was "rudimentary and in need of further refinement" [-@Alston1995]. This remains true today. -->



# Brief introduction to risk adjusted portfolio optimization

The risk-adjusted optimization problem for a portfolio of $n$ risky assets is formulated as follows. 

\begin{equation}
\max_{\mathbf{w}}\:R - C \:\:\:\:s.t. \:\:\: C=\bar{C} \:, \:\:\: \sigma^2 = \bar{\sigma}^2
\end{equation}

where $\mathbf{w}$ is the vector of budget shares, $R$ is the expected portfolio return, defined $R = \mathbf{w} \cdot \boldsymbol{\mu}$, where $\boldsymbol{\mu}$ is the vector of expected returns of each asset; $C$ is the cost, defined $C = \mathbf{w} \cdot \mathbf{1}$, constrained to sum to the budget $\bar{C}$; and $\sigma^2$ is the portfolio variance (a measure of the portfolio risk), defined $\sigma^2 = \mathbf{w} \cdot \Sigma \cdot \mathbf{w}$, where $\Sigma$ is the asset covariance matrix; and $\sigma^2$ is constrained to equal a tolerance $\bar{\sigma}^2$ set by the investor.^[In the financial context, the problem is usually formulated as a risk minimization problem subject to a budget constraint and return target. Both approaches yield the same outputs. In the financial context, the problem includes the option to invest in one risk free asset. There is no risk free investment in the AR4D context, so the set up here focuses on optimization of the risky portfolio only.]

The Lagrangian is then

\begin{equation}
\mathcal{L} = R - C + \lambda_C(C - \bar{C}) + \lambda_{\sigma}(\sigma^2 - \bar{\sigma}^2)
\end{equation}

with first order conditions

\begin{equation}
\nabla \mathcal{L} = \boldsymbol{\mu} - \gamma \mathbf{1} + 2 \lambda_{\sigma} \Sigma \cdot \mathbf{w} = \mathbf{0} \:\:;\:\:\: \gamma = 1-\lambda_C
\end{equation}

and second order conditions

\begin{equation}
\mathbf{w} \cdot \nabla^2 \mathcal{L} \cdot \mathbf{w} = 2 \lambda_{\sigma} \sigma^2 < 0
\end{equation}

where $\nabla^2 \mathcal{L}$ is the Hessian matrix of $\mathcal{L}$. Since $\Sigma$ is symmetric and variances are, by definition, positive, then $\Sigma$ is positive semi-definite. Hence, a maximum is guaranteed so long as $\lambda_{\sigma}$ is negative.

Dotting the first order conditions through by $\mathbf{w}$ gives the equation for the efficient frontier.

\begin{equation}
R^* = \lambda_C \bar{C} + 2 \lambda_{\sigma} \bar{\sigma}^2
\label{eq:rStar}
\end{equation}

where the asterisk on $R$ indicates that this is the maximum portfolio return given budget constraint $\bar{C}$ and risk tolerance $\bar{\sigma}^2$.

Note, in passing, that equation \ref{eq:rStar} implies that the risk shadow price is proportional to the expected reward to risk ratio.

\begin{equation}
\lambda_{\sigma} = \frac{\psi}{2\bar{\sigma}^2} \: ; \:\:\: \psi = R^* - \gamma \bar{C}
\end{equation}

where $\psi = R^* - \gamma \bar{C}$ is the expected reward, i.e., the expected net revenue, adjusted by the budget shadow price $\gamma$.

Now, dotting the first order conditions through by $\Sigma^{-1}$ and rearranging gives an equation for the optimal budget shares.

\begin{equation}
\mathbf{w}^* = -\frac{1}{2 \lambda_{\sigma}} \Sigma^{-1} \cdot \nabla \psi \: ; \:\:\: \nabla \psi = \boldsymbol{\mu} - \gamma \mathbf{1}
\label{eq:budgetShares}
\end{equation}

Note, in passing, that dotting this through by $\nabla \psi$ gives another instructive equation for $\lambda_{\sigma}$.

\begin{equation}
\lambda_{\sigma} = \frac{d_m^2}{2 \psi} \: ; \:\:\:\:d_m^2 = \nabla \psi \cdot \Sigma^{-1} \cdot \nabla \psi
\end{equation}

This says that the risk shadow price is inversely proportional to the ratio of the squared Mahalanobis distance ($d_m^2$) of the portfolio net reward gradient to the net reward. In this setting, the Mahalanobis distance reflects how improbable a given portfolio is. Note, then, that portfolio improbability goes to zero (and hence probability is greater) as $\nabla \psi = \mathbf{0}$, which occurs only when every component in the vector of asset returns ($\boldsymbol{\mu}$) equals the budget shadow price ($\gamma$). Moreover, note that this can be combined with the previous expression for $\lambda_{\sigma}$ to obtain

\begin{equation}
{1 \over d_m}  &=& {\bar{\sigma} \over \psi}
\end{equation}

which says that the probability of the optimal portfolio equals its risk to reward ratio.

Returning to the task at hand, evaluation of the optimal budget shares or the frontier in equations \ref{eq:rStar} and \ref{eq:budgetShares}, requires first solving for the cost and risk shadow prices $\gamma$ and $\lambda_{\sigma}$.

To do this, first note that the budget shares equation can be rewritten as follows:

\begin{equation}
\mathbf{w}^* = -\frac{1}{2 \lambda_{\sigma}} \Sigma^{-1} \cdot [\boldsymbol{\mu}, -\mathbf{1}] \left[\begin{matrix} 1 \\ \gamma \end{matrix} \right]
\end{equation}

Now, dotting through by $[\boldsymbol{\mu}, -\mathbf{1}]$ gives

\begin{equation}
\left[\begin{matrix}
R \\
-\bar{C} \\
\end{matrix} \right]  = -\frac{1}{2 \lambda_{\sigma}} M\left[\begin{matrix}
1 \\
\gamma
\end{matrix} \right]
\end{equation}

where $M$ has been introduced to stand for the matrix

\begin{equation}
M = [\boldsymbol{\mu}, \: -\mathbf{1}]' \cdot \Sigma^{-1} \cdot [\boldsymbol{\mu}, \: -\mathbf{1}]
\end{equation}

Let $M$ be called the "Merton matrix", after the author in whose footsteps I am now following [@merton1972analytic]. Pre-multiplying both sides of the previous equation by the inverse Merton matrix and rearranging gives the following expression: 

\begin{equation}
-2 M^{-1}\left[\begin{matrix}
R \\
-\bar{C} \\
\end{matrix} \right]  = \left[\begin{matrix}
1/\lambda_V \\
\gamma / \lambda_{\sigma} \\
\end{matrix} \right]
\end{equation}

For any given return target $R$ and budget $\bar{C}$, then, the cost and risk shadow prices are given by this equation. With values for $\gamma$ and $\lambda_{\sigma}$ in hand, the budget shares and risk associated with the chosen return target can be evaluated.


The portfolio with the minimum possible risk can be found by differentiating the frontier (\ref{eq:rStar}) with respect to expected return and then setting $\frac{\partial \sigma}{\partial R}=0$. This gives the expected return for the portfolio with minimum possible risk. This corresponds to the portfolio at the vertex of the frontier parabola.

\begin{equation}
R\Bigr|_{\sigma = \sigma_{min}} = {m_{12} \over m_{22}}C
\end{equation}


```{r, fig.width=14, fig.height=14, fig.align='center', echo = FALSE, include = FALSE}
#options(warn = -1); options(scipen = 999)
spy_sector_symbs <- c("XLF", "XLC", "XLY", "XLP", "XLV", "XLK", "RWR",
                      "XLU", "XLI", "XBI", "IYT") #"TTEK"
spy_sector_detail <- c("Financials", "Communications", "Luxury goods", "Consumer\nGoods",
                       "Healthcare", "Technology", "Real estate", "Utilities", "Industrial",
                       "Biotechnology", "Transportation") #"Gov. foreign aid"
minerals_symbs <- c("GLD", "SLV", "PPLT", "JJC", "JJM") #"XME"
minerals_detail <- c("Gold", "Silver", "Platinum", "Copper", "Industrial metals") #"US metals and mining"
agriculture_symbs <- c("JJG", "BAL", "SOYB", "SGG", "JO", "NIB", "DBA")
agriculture_detail <- c("Grains", "Cotton", "Soybean", "Sugar", "Coffee", "Cacao", "General agriculture")
energy_symbs <- c("WTI", "FUE", "WOOD", "ICLN", "KOL", "UNG")
energy_detail <- c("Oil (W&T Offshore Inc.)", "Biofuels", "Timber", "Clean energy", "Coal", "US natural gas")
#currency_symbs <- c("EMLC", "UUP", "FXE", "FXY", "FXF", "FXC", "FXB", "FXA")
#currency_detail <- c("Emerging mkt currencies", "USD", "EUR", "JPY", "CHF", "CND", "GBP", "AUD")
currency_symbs <- c("EURUSD=X", "JPY=X", "CHF=X", "CAD=X",
                    "GBPUSD=X", "AUDUSD=X", "INR=X")
currency_detail <- c("EUR/USD", "USD/JPY",
                     "USD/CHF", "USD/CAD", "GBP/USD", "AUD/USD", "USD/INR")
emerg_mkt_symbs <- c("EMLC", "ELD", "BKF", "VWOB")
emerg_mkt_detail <- c("Emerg mkts debt", "Emerg mkts gov. bonds", "BRIC countries", "Emerging mkt currencies")
crypto_symbs <- c("BLOK", "LEGR", "BCNA")
crypto_detail <- c("Blockchain tech.", "Blockchain companies", "Blockchain in China")
Tbond_symbs <- c("IEI", "IEF", "TLT")#, "BIL"
Tbond_detail <- c("T-bond 3-7 yrs", "T-bond 7-10 yrs", "T-bond 20+ yrs") #"T-bond 1-3 months"

ts_symb_vec <- c(spy_sector_symbs, minerals_symbs, agriculture_symbs, energy_symbs,
                 currency_symbs, emerg_mkt_symbs, crypto_symbs, Tbond_symbs)
ts_detail_vec <- c(spy_sector_detail, minerals_detail, agriculture_detail, energy_detail,
                   currency_detail, emerg_mkt_detail, crypto_detail, Tbond_detail)
df_symb_detail <- data.frame(symbol = ts_symb_vec, detail = ts_detail_vec)
#--
#length(ts_symb_vec)
#--
# list_groups <- list(spy_sector_symbs, minerals_symbs, agriculture_symbs, energy_symbs,
#                      currency_symbs, emerg_mkt_symbs, crypto_symbs, Tbond_symbs)
#-------------------------------------------
# Create grouping info for plots (required input into functions)
list_groups <- list(spy_sector_detail, minerals_detail, agriculture_detail, energy_detail,
                    currency_detail, emerg_mkt_detail, Tbond_detail) #crypto_detail,
group_names <- c("US Sectors", "Minerals", "Agriculture", "Energy", "Major Currency Pairs",
                 "Emerging Markets", "T-Bonds") #"Cryptocurrencies/\nBlockchain"
n_groups <- length(list_groups)
#group_colors <- RColorBrewer::brewer.pal(n = n_groups, "Dark2")
# "Darjeeling"
# group_colors <- wesanderson::wes_palette("Darjeeling1", n = n_groups, type = "continuous")
bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_groups)
group_colors <- sample(bag_of_colors, n_groups)
#--------------------------------------------------------------
# Define function to order data by group
group_fn <- function(group_info){
  list_groups <- group_info[[1]]
  group_names <- group_info[[2]]
  group_colors <- group_info[[3]]
  varNames_ordered <- do.call(c, list_groups)
  n_groups <- length(group_names)
  n_items <- length(varNames_ordered)
  if(is.na(group_colors)){
    bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_groups)
    group_colors <- sample(bag_of_colors, n_groups)
    #group_colors <- viridis::viridis_pal(option = "D")(length(group_names))
  }
  #if(reverse_order){group_colors <- rev(group_colors)}
  #varNames_ordered <- colnames(mat_pctDiff)
  group_vec <- rep(NA, n_items)
  group_color_vec <- rep(NA, n_items)
  for(i in 1:n_groups){
    this_group_vec <- list_groups[[i]]
    this_group_name <- group_names[i]
    this_group_color <- group_colors[i]
    group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
    group_color_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_color
  }
  ind_ordered_cols <- order(factor(group_vec))
  cols_ordered_by_group <- as.character(varNames_ordered[ind_ordered_cols])
  group_color_vec <- group_color_vec[ind_ordered_cols]
  group_vec_ordered <- group_vec[ind_ordered_cols]
  out_list <- list(cols_ordered_by_group, group_color_vec, group_vec_ordered, ind_ordered_cols, group_vec)
  return(out_list)
}
#--------------------------------------------------------------
group_info <- list(list_groups, group_names, group_colors)
  outlist <- group_fn(group_info)
  cols_ordered_by_group <- outlist[[1]]
  group_color_vec <- outlist[[2]]
  group_vec_ordered <- outlist[[3]]
  ind_ordered_cols <- outlist[[4]]
  df_match_group <- data.frame(Item = cols_ordered_by_group, Group = group_vec_ordered)
#--------------------------------------------------------------
#per_ema <- 13
#fromdate = Sys.Date() - 360
fromdate = "2018-06-20"
todate = "2019-08-09"
tbl_ohlcv <- tq_get(ts_symb_vec, get = "stock.prices", from = fromdate, to = todate)
df_ohlcv <- as.data.frame(tbl_ohlcv)
#write.csv(df_ohlcv, "Financial data backup.csv")
#--
# Crypto ts can have duplicate date entries sometimes (BTC-USD)
# Get rid of these duplicates, if any
# df_ohlcv <- as.data.frame(df_ohlcv %>% group_by(symbol) %>% mutate(dup = duplicated(date)))
# df_ohlcv <- subset(df_ohlcv, dup == F)
# df_ohlcv$dup <- NULL
#--

#df_ohlcv$p <- rowSums(df_ohlcv[, c(4:6)]) / 3
#--
#df <- df_ohlcv[, c("date", "symbol", "p")]
# df <- df_ohlcv %>% group_by(symbol) %>% tq_transmute(select = adjusted, 
#                      mutate_fun = periodReturn, 
#                      period     = "monthly")
#--
df <- df_ohlcv[, c("date", "symbol", "adjusted")]
df <- df %>% spread(symbol, adjusted)
ind_shift <- which(colnames(df) %in% currency_symbs)
df[, ind_shift] <- rbind(rep(NA, length(ind_shift)), df[-nrow(df), ind_shift])
df <- df[-c(1, nrow(df)), ]
date_vec <- df$date
mat_ts_dy <- na.approx(df[, -1])
row.names(mat_ts_dy) <- as.character(date_vec)
#o <- apply(mat_ts_dy, 2, function(x) length(which(is.na(x))))
#table(o)
#which(o==1)
#xts_ts_dy <- xts(mat_ts_dy, date_vec)
#date_vec <- index(xts_ts_dy)
#-----
mat_pctDiff_dy <- diff(mat_ts_dy) / mat_ts_dy[-nrow(mat_ts_dy), ] * 100
#mat_pctDiff_dy <- diff(log(mat_ts_dy))
row.names(mat_pctDiff_dy) <- as.character(date_vec[-1])
ts_avg_dy <- rowMeans(mat_pctDiff_dy)
mu_ret_dy <- colMeans(mat_pctDiff_dy)
sd_ret_dy <- apply(mat_pctDiff_dy, 2, sd)

mat_ts_wk <- to.weekly(xts(mat_ts_dy, date_vec), OHLC = F)
mat_pctDiff_wk <- diff(mat_ts_wk) / mat_ts_wk[-nrow(mat_ts_wk), ] * 100
mat_pctDiff_wk <- as.matrix(mat_pctDiff_wk[-1, ])
ts_avg_wk <- rowMeans(mat_pctDiff_wk)
mu_ret_wk <- colMeans(mat_pctDiff_wk)
sd_ret_wk <- apply(mat_pctDiff_wk, 2, sd)

mat_ts_mo <- to.monthly(xts(mat_ts_dy, date_vec), OHLC = F)
mat_pctDiff_mo <- diff(mat_ts_mo) / mat_ts_mo[-nrow(mat_ts_mo), ] * 100
mat_pctDiff_mo <- as.matrix(mat_pctDiff_mo[-1, ])
ts_avg_mo <- rowMeans(mat_pctDiff_mo)
mu_ret_mo <- colMeans(mat_pctDiff_mo)
sd_ret_mo <- apply(mat_pctDiff_mo, 2, sd)
#----------------------------------------------
# mu_pctRet <- apply(10^-2 * mat_pctDiff_dy, 2, function(x) prod(1 + x)) - 1
# mu_pctRet_check <- (mat_ts_dy[nrow(mat_ts_dy), ] - mat_ts_dy[1, ]) / mat_ts_dy[1, ]
# mu_pctRet / mu_pctRet_check
#----------------------------------------------
# Compare histograms of daily, weekly, and monthly returns
# df_plot <- data.frame(ts_id = names(mu_ret_dy), mu_ret_dy, mu_ret_wk, mu_ret_mo)
# colnames(df_plot)[-1] <- c("Mean Daily Return", "Mean Weekly Return", "Mean Monthly Return")
# gathercols <- colnames(df_plot)[-1]
# df_plot <- df_plot %>% gather_("Return Type", "Value", gathercols)
# #gg <- ggplot(df_plot, aes(Value, fill = `Return Type`))
# gg <- ggplot(df_plot, aes(Value))
# gg <- gg + geom_density(alpha = 0.4)
# gg <- gg + facet_wrap(~`Return Type`, nrow = 1, scales = "free")
# #gg <- gg + coord_cartesian(xlim = c(-0.02, 0.02))
# gg
#----------------------------------------------
mat_ts <- mat_ts_dy
colnames(mat_ts) <- as.character(df_symb_detail$detail[order(df_symb_detail$symbol)])
#----------------------------------------------
mat_pctDiff <- mat_pctDiff_dy
#mat_pctDiff <- mat_pctDiff_wk
date_vec <- row.names(mat_pctDiff)
#length(date_vec)
row.names(mat_pctDiff) <- as.character(date_vec)
colnames(mat_pctDiff) <- as.character(df_symb_detail$detail[order(df_symb_detail$symbol)])
#----------------------------------------------
check_on_data <- F
if(check_on_data){
  df_plot <- as.data.frame(mat_pctDiff)
  gathercols <- colnames(df_plot)
  df_plot$Date <- date_vec
  df_plot <- df_plot %>% gather_("Security", "Weekly change (%)", gathercols)
  df_plot$Type <- NA
  df_plot$Type[which(df_plot$Security %in% spy_sector_detail)] <- "US Sectors"
  df_plot$Type[which(df_plot$Security %in% minerals_detail)] <- "Minerals"
  df_plot$Type[which(df_plot$Security %in% agriculture_detail)] <- "Agriculture"
  df_plot$Type[which(df_plot$Security %in% energy_detail)] <- "Energy"
  df_plot$Type[which(df_plot$Security %in% currency_detail)] <- "Major Currency Pairs"
  df_plot$Type[which(df_plot$Security %in% emerg_mkt_detail)] <- "Emerging Markets"
  df_plot$Type[which(df_plot$Security %in% crypto_detail)] <- "Cryptocurrencies/\nBlockchain"
  df_plot$Type[which(df_plot$Security %in% Tbond_detail)] <- "T-bonds"
  #------------------------------------------------------------
  # gg <- ggplot(df_plot, aes(x = Year, y = Value, group = Security, color = Security))
  # gg <- gg + geom_line()
  # gg <- gg + facet_wrap(~ Type, nrow = 3, scales = "free")
  # gg <- gg + theme(legend.position = "none")
  # gg
  #------------------------------------------------------------
  this_type <- "US Sectors"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Minerals"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Agriculture"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Energy"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Major Currency Pairs"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Emerging Markets"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Cryptocurrencies/Blockchain"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "T-bonds"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
}

```


```{r, fig.show = 'hold', fig.cap="\\label{fig:hReturns}Historical (top) daily returns and (bottom) period return, financial data.", fig.width=6, fig.height=6, fig.align='left', echo=FALSE}
#=======================================================================
# Define function to plot period returns as barchart with colors
plot_returns_barchart <- function(df_pctRet, group_colors, list_graph_options, graph_on = T){
  if(is.null(list_graph_options)){
    fig_title <- NULL
    legend_position <- NULL
    ylab <- NULL
  }else{
    fig_title <- list_graph_options[["fig_title"]]
    legend_position <- list_graph_options[["legend_position"]]
    ylab <- list_graph_options[["ylab"]]
  }

  if(is.null(group_colors)){
    n_groups <- length(unique(df_pctRet$Group))
    bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_groups)
    group_colors <- sample(bag_of_colors, n_groups)
  }
  
  colnames(df_pctRet)[1] <- "Period Return"
  df_plot <- df_pctRet
  gg <- ggplot(df_plot, aes(x = Item, y = `Period Return`, fill = Group))
  #gg <- gg + scale_color_brewer(palette = "Dark2")
  gg <- gg + scale_fill_manual(values = group_colors)
  gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
  if("Dataset" %in% colnames(df_plot)){
      gg <- gg + facet_wrap(~Dataset, ncol = 1)
  }
  if(!is.null(fig_title)){
      gg <- gg + labs(title = fig_title)
gg <- gg + theme(plot.title = element_text(face = "bold", size = 9))
  }
  if(!is.null(ylab)){
    gg <- gg + labs(y = ylab)
  }else{
    gg <- gg + theme(axis.title.y = element_blank())
  }
  if(!is.null(legend_position)){
    gg <- gg + theme(legend.position = legend_position)
  }else{
    gg <- gg + theme(legend.position = "bottom")
  }
  gg <- gg + theme(axis.text.x = element_text(size = 9, angle = 60, hjust = 1),
                   #axis.text.x = element_blank(),
                   axis.title.x = element_blank(),
                   axis.text.y = element_text(size = 9),
                   #axis.title.y = element_text(size = 9),
                   legend.title = element_blank(),
                   legend.text = element_text(size = 9))
  #  gg <- gg + coord_equal()
  #  gg <- gg + coord_flip()
  if(graph_on){print(gg)}
  return(gg)

}

#=======================================================================
# Define function to plot daily returns box and whiskers chart 
# plot_boxchart <- function(df_pctDiff, group_colors, list_graph_options, graph_on = T){
#   if(is.null(list_graph_options)){
#     legend_position <- "right" #"none"
#     axis_titles <- "on" #"off"
#     axis_text <- "on" #"y only"
#     fig_title <- NULL
#   }else{
#     legend_position <- list_graph_options[["legend_position"]]
#     axis_titles <- list_graph_options[["axis_titles"]]
#     axis_text <- list_graph_options[["axis_text"]]
#     fig_title <- list_graph_options[["fig_title"]]
#   }
#   
#   df_plot <- df_pctDiff
#     gg <- ggplot(df_plot, aes(x = Item, y = `Daily Returns`, fill = Group))
#   #gg <- gg + scale_color_brewer(palette = "Dark2")
#   gg <- gg + scale_fill_manual(values = group_colors)
#   #gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
#   gg <- gg + geom_boxplot()
#   if("Dataset" %in% colnames(df_plot)){
#     gg <- gg + facet_wrap(~Dataset, ncol = 1)
#   }
#   if(!is.null(fig_title)){
#     gg <- gg + labs(title = fig_title)
#     gg <- gg + theme(plot.title = element_text(face = "bold", size = 9))
#   }
#   if(axis_titles == "off"){
#     gg <- gg + theme(axis.title = element_blank())
#   }
#   if(axis_text == "y only"){
#     gg <- gg + theme(axis.text.x = element_blank(),
#                      axis.text.y = element_text(size = 9))
#   }
#   if(axis_text == "on"){
#     gg <- gg + theme(axis.text.y = element_text(size = 9),
#                      axis.text.x = element_text(size = 9, angle = 60, hjust = 1))
#   }
#     gg <- gg + theme(legend.position = legend_position)
#     if(graph_on){print(gg)}
#     return(gg)
# 
# }
#=======================================================================
# Define function to order data frames by group for period returns barchart and daily returns box and whiskers plots
get_hist_dfs_ready <- function(mat_ts_in, group_info){
  #------------------------------------------------------------
  # Group the vars according to provided group info
  #varNames_ordered <- do.call(c, group_info[[1]])
  outlist <- group_fn(group_info)
  cols_ordered_by_group <- outlist[[1]]
  group_color_vec <- outlist[[2]]
  group_vec_ordered <- outlist[[3]]
  ind_ordered_cols <- outlist[[4]]
  df_match_group <- data.frame(Item = cols_ordered_by_group, Group = group_vec_ordered)
  mat_ts_ordered <- mat_ts_in[, cols_ordered_by_group]
  #------------------------------------------------------------
  # Get date info
  from_date <- row.names(mat_ts_ordered)[1]
  to_date <- row.names(mat_ts_ordered)[nrow(mat_ts_ordered)]
  from_date <- gsub("-", "/", from_date)
  to_date <- gsub("-", "/", to_date)
  date_interval <- paste(from_date, to_date, sep = " - ")
  #------------------------------------------------------------
  # Get returns barchart df organized by group
  nab_pctRet <- 100 * (mat_ts_ordered[nrow(mat_ts_ordered), ] - mat_ts_ordered[1, ]) / mat_ts_ordered[1, ]
  #---
  # check
  mat_pctDiff <- 100 * diff(mat_ts) / mat_ts[-nrow(mat_ts), ]
  nab_pctRet_check <- 100 * apply(10^-2 * mat_pctDiff, 2, function(x) prod(1 + x)) - 1
  nab_pctRet / nab_pctRet_check
  #---
  df_pctRet <- data.frame(Returns = nab_pctRet)
  df_pctRet$Item <- row.names(df_pctRet)
  df_pctRet$Group <- factor(group_vec_ordered)
  df_pctRet$Item <- factor(df_pctRet$Item, levels = cols_ordered_by_group)
  #------------------------------------------------------------
  # Get boxplot df organized by group
    # df_pctDiff <- as.data.frame(mat_pctDiff_ordered)
    # df_pctDiff <- df_pctDiff %>% gather_("Item", "Daily Returns", colnames(df_pctDiff))
    # df_match_group <- data.frame(Item = cols_ordered_by_group, Group = group_vec_ordered)
    # df_pctDiff <- merge(df_pctDiff, df_match_group, by = "Item")
    # df_pctDiff$Item <- factor(df_pctDiff$Item, levels = cols_ordered_by_group)
    # df_pctDiff$Group <- factor(df_pctDiff$Group)
  #------------------------------------------------------------
  out_list <- list(df_pctRet, date_interval)
  return(out_list)
}


#=======================================================================
# Define function to plot covariance/correlation matrices
plot_covmat <- function(covmat, fig_title = "Covariance Matrix", graph_on = T){
  covmat[upper.tri(covmat)] <- NA
  df_plot <- covmat %>% tbl_df()
  these_levels <- colnames(df_plot)
  df_plot$ItemX <- colnames(df_plot)
  gathercols <- colnames(df_plot)[-ncol(df_plot)]
  df_plot <- df_plot %>% gather_("ItemY", "Value", gathercols)
  df_plot$ItemX <- factor(df_plot$ItemX, levels = these_levels)
  df_plot$ItemY <- factor(df_plot$ItemY, levels = these_levels)

  #midpoint <- min(covmat) + (max(covmat) - min(covmat)) / 2
  midpoint <- 0
  gg <- ggplot(df_plot, aes((ItemX), (ItemY)))
  gg <- gg + geom_tile(aes(fill = Value))#, width = 4, height = 4)
  gg <- gg + geom_text(aes(label = round(Value, 2)), size = 3)
  if(!is.null(fig_title)){
    gg <- gg + labs(title = fig_title)
  }
  gg <- gg + theme_bw()
  gg <- gg + theme(axis.text.x = element_text(angle = 60, hjust = 1),
                 axis.title = element_blank(),
                 legend.title = element_blank(),
                 plot.title = element_text(face = "bold", size = 9))
  gg <- gg + scale_fill_gradient2(low = "khaki", mid = "cyan", high = "magenta", midpoint, na.value = "white")
  if(graph_on){print(gg)}
  return(gg)

}
#=======================================================================
# End function definition
#=======================================================================
# Separate out a reduced matrix for the basic MV Analysis demonstration
# group_names
demonstration_group <- c("US Sectors")
ind_group <- which(group_names %in% demonstration_group)
these_items <- do.call(c, list_groups[ind_group])
ind_keep <- which(colnames(mat_ts) %in% these_items)
mat_ts_eg <- mat_ts[, ind_keep]
#-----------------------------------------------------------------------
# For a good illustrative example of portfolio optimization, select a financial period over which the returns in the portfolio are as varied as possible yet with the minimum return being >= 2%. This can be achieved by adjusting "buffer". Also'd be nice to have all signal period returns > 0, but this does not seem possible in this data set.
#-----------------------------------------------------------------------
# ind_train <- 1:round(nrow(mat_pctDiff) * 1 / 2)
# ind_test <- setdiff(1:nrow(mat_pctDiff), ind_train)
length_Q <- 67 # Length of a financial quarter (3 months)
buffer <- 5#36 # Select to give high cv of returns but with min return >=1-2%
#------------------------------------------------------------
# min_pctRet <- c()
# cv_pctRet <- c()
# buffr <- c()
# n_negRet_sigs_train <- c()
# n_negRet_sigs_test <- c()
# for(i in 1:110){
# buffer <- 2+i
#------------------------------------------------------------
ind_train <- (nrow(mat_ts_eg) - 2 * length_Q - buffer + 1):(nrow(mat_ts_eg) -  length_Q - buffer)
ind_test <- (nrow(mat_ts_eg) - length_Q - buffer + 1):(nrow(mat_ts_eg) - buffer)
mat_ts_eg_train <- mat_ts_eg[ind_train, ]
mat_ts_eg_test <- mat_ts_eg[ind_test, ]
  #------------------------------------------------------------
  # Get returns barchart df organized by group
  nab_decRet_eg_train <- (mat_ts_eg_train[nrow(mat_ts_eg_train), ] - mat_ts_eg_train[1, ]) / mat_ts_eg_train[1, ]
  nab_decRet_eg_test <- (mat_ts_eg_test[nrow(mat_ts_eg_test), ] - mat_ts_eg_test[1, ]) / mat_ts_eg_test[1, ]
  nab_pctRet_eg_train <- 100 * nab_decRet_eg_train
  nab_pctRet_eg_test <- 100 * nab_decRet_eg_test
  #---
  # check
  # mat_pctDiff <- 100 * diff(mat_ts_eg_train) / mat_ts_eg_train[-nrow(mat_ts_eg_train), ]
  # nab_pctRet_check <- 100 * (apply(10^-2 * mat_pctDiff, 2, function(x) prod(1 + x)) - 1)
  # nab_pctRet / nab_pctRet_check
  #------------------------------------------------------------
mat_decDiff_eg_train <- diff(mat_ts_eg_train) / mat_ts_eg_train[-nrow(mat_ts_eg_train), ]
mat_decDiff_eg_test <- diff(mat_ts_eg_test) / mat_ts_eg_test[-nrow(mat_ts_eg_test), ]
mat_pctDiff_eg_train <- 100 * mat_decDiff_eg_train
mat_pctDiff_eg_test <- 100 * mat_decDiff_eg_test
covmat_decDiff_train <- cov(mat_decDiff_eg_train)
covmat_decDiff_test <- cov(mat_decDiff_eg_train)
covmat_pctDiff_train <- cov(mat_pctDiff_eg_train)
covmat_pctDiff_test <- cov(mat_pctDiff_eg_train)
#------------------------------------------------------------
# mat_P <- eigen(covmat_decDiff_train)$vectors
# mat_S <- mat_decDiff_eg_train %*% mat_P
# mat_P_test <- eigen(covmat_decDiff_test)$vectors
# mat_S_test <- mat_decDiff_eg_test %*% mat_P_test
# nab_decRet_sigs_train <- (apply(mat_S, 2, function(x) prod(1 + x)) - 1)
# nab_decRet_sigs_test <- (apply(mat_S_test, 2, function(x) prod(1 + x)) - 1)
# n_negRet_sigs_train[i] <- length(which(nab_decRet_sigs_train[1:4] < 0))
# n_negRet_sigs_test[i] <- length(which(nab_decRet_sigs_test[1:4] < 0))
# min_pctRet[i] <- min(nab_pctRet)
# cv_pctRet[i] <- sd(nab_pctRet) / mean(nab_pctRet)
# buffr[i] <- buffer
# }
# View(data.frame(buffr, min_pctRet, cv_pctRet, n_negRet_sigs_train, n_negRet_sigs_test))
#----------------------------------------------------------
train_start_date <- row.names(mat_pctDiff_eg_train)[1]
train_stop_date <- row.names(mat_pctDiff_eg_train)[nrow(mat_pctDiff_eg_train)]
test_start_date <- row.names(mat_pctDiff_eg_test)[1]
test_stop_date <- row.names(mat_pctDiff_eg_test)[nrow(mat_pctDiff_eg_test)]
#----------------------------------------------------------

```


An example application of MV Analysis is presented below using daily financial data for `r ncol(mat_pctDiff_eg_train)` securities over the period `r train_start_date` to `r train_stop_date`, downloaded from yahoo finance using the R tidyquant package. These securities were chosen so as to be broadly representative of the U.S. economy. Details can be found in the appendix.

Period returns for each asset in the dataset are presented in the bottom of Figure \ref{fig:hReturns}. The period return of a given portfolio asset $\bar{r}$ with given price time series $\mathbf{p}$ is calculated as 

$$
\bar{r} = \frac{p_T - p_1}{p_1}
$$
where $T$ denotes the length of the time series. Daily returns are calculated
$$
r_{t} = \frac{p_t - p_{t-1}}{p_{t-1}}
$$
for any given day $t$ in the series. As a practical matter, note that the period returns can also be calculated from the daily returns as follows.

$$
\bar{r} = \prod_{t = 2}^T (1 + r_{t}) - 1
$$
<!-- They are mostly exchange traded funds tracking broad categories of stocks, bonds, markets, and commodities, plus major currency pairs traded on the foreign exchange market. -->

<!-- [Table here detailing the ETF names, what they track, and my category name] -->


```{r, fig.show = 'hold', fig.cap="\\label{fig:hReturns}Historical (top) daily returns and (bottom) period return, financial data.", fig.width=6, fig.height=6, fig.align='left', echo=FALSE}
# Plot historical returns for period and items defined above
fig_title <- "Period Returns (%)"
list_graph_options <- list()
list_graph_options[["fig_title"]] <- fig_title
list_graph_options[["legend_position"]] <- "none"
list_graph_options[["ylab"]] <- NULL
df_pctRet <- data.frame(Value = nab_pctRet_eg_train, Item = names(nab_pctRet_eg_train), Group = demonstration_group)
gg_barchart <- plot_returns_barchart(df_pctRet, group_colors, list_graph_options, graph_on = F)
#----------------------------------------------------------
# Covariance matrix plot
gg_covmat <- plot_covmat(covmat_pctDiff_train, graph_on = F)
#----------------------------------------------------------
#gg_boxplot + gg_barchart + plot_layout(ncol = 1, heights = c(2, 1))
gg_barchart + gg_covmat + plot_layout(ncol = 1, heights = c(1, 2))
#----------------------------------------------------------


```

\pagebreak

As explained above, MV Analysis requires the expected returns of each portfolio item, and their corresponding covariance matrix, in order to calculate the efficient frontier and budget weights. The expected returns are just the period returns. The covariance matrix $\Sigma$ is calculated from the centered daily returns as follows.

$$
\Sigma=\frac{1}{1 - n} X'X \:\:; \:\:\:X_{it} = r_{it}-\frac{1}{T}\sum_{t=1}^Tr_{it}
$$
where the $i$ subscript indexes the portfolio item.

The optimal frontier and budget shares based on this data are given in Figure \ref{fig:mvConv}.

```{r, fig.show = "hold", fig.width = 5, fig.height=5, fig.align='left', fig.cap="\\label{fig:mvConv}Optimal frontier and budget shares, financial data.", echo = FALSE}
#=======================================================================
# Define portfolio optimization function
optimize_portfolio <- function(M_inv, covmat_inv, covmat, mat_nab, targ_vec,
                               backtest_info = NULL){
  #print(M)
  lambdas <- -2 * M_inv %*% targ_vec
  # Return shadow price
  l_R <- lambdas[1]
  # Budget shadow price
  l_C <- lambdas[2]
  # Optimal budget shares
  wStar <- -1 / 2 * covmat_inv %*% mat_nab %*% lambdas
  #print(sum(wStar))
  # Portfolio variance
  Vtarg <- t(wStar) %*% covmat %*% wStar
  # Rtarg <- targ_vec[1]
  Rtarg <- t(wStar) %*% mat_nab[, 1]
  #----------------------------------------------------
  # check <- t(lambdas) %*% targ_vec + 2*l_V * V
  # print(check)
  #----------------------------------------------------
  sd_targ <- sqrt(Vtarg)
  #if(l_V > 0){wStar <- rep(NA, length(wStar))}
  #----------------------------------------------------
  if(!is.null(backtest_info)){
    nab_decRet_test <- backtest_info[["nab_decRet_test"]]
    covmat_test <- backtest_info[["covmat_test"]]
    Rtest <- t(wStar) %*% nab_decRet_test
    Vtest <- t(wStar) %*% covmat_test %*% wStar
    sd_test <- sqrt(Vtest)
  }else{
    Rtest <- NA
    Vtest <- NA
    sd_test <- NA
  }
  #----------------------------------------------------
  frontier_vec <- c(Rtarg, sd_targ, l_R, l_C, Rtest, sd_test)
  list_out <- list(wStar, frontier_vec)
  return(list_out)
}
#=======================================================================
# Define function for budget shares plot
plot_budgetShares <- function(df_wStar, group_small = NULL, color_vec = NULL, graph_on = T, list_graph_options = NULL){
    # Budget shares plot
  # Note df_w_in should have just the target risk column (i.e. don't also include the backtest risk column). At any rate, the budget risk column should be the same as the one used in the frontier plot.
  #df_plot <- df_wStar_prop
  #------------------------------------
  if(!is.null(list_graph_options)){
    legend_position <- list_graph_options[["legend_position"]]
    fig_title <- list_graph_options[["fig_title"]]
    axis_titles <- list_graph_options[["axis_titles"]]
    Xaxis_numbers_off <- list_graph_options[["Xaxis_numbers_off"]]
  }else{
      legend_position = "bottom"
      fig_title = NULL
      axis_titles = "on"
      Xaxis_numbers_off = F
  }
  
  #------------------------------------
  df_plot <- df_wStar
  gathercols <- colnames(df_plot)[-1]
  #------------------------------------
  if(!is.null(group_small)){
    mat_plot <- as.matrix(df_plot[, -1])
    mu_vec <- apply(mat_plot, 2, mean)
    ind_group_small <- which(mu_vec < 10^-2)
    other_col <- rowSums(mat_plot[, ind_group_small])
    
    mat_plot <- mat_plot[, -ind_group_small]
    df_plot <- as.data.frame(mat_plot)
    df_plot$Other <- other_col
    df_plot$`Risk (standard deviation)` <- df_wStar$`Risk (standard deviation)`
    gathercols <- colnames(df_plot)[-ncol(df_plot)]
  }
  #------------------------------------
  df_plot$portfolio_id <- 1:nrow(df_wStar)
  df_match_V <- df_plot[, c("portfolio_id", "Risk (standard deviation)")]
  df_plot <- df_plot %>% gather_("Item", "Budget shares", gathercols)
  df_plot <- df_plot %>% group_by(Item) %>% 
    mutate(mu = median(`Budget shares`)) %>% 
    as.data.frame()
  df_plot <- df_plot[order(df_plot$mu, decreasing = T), ]
  #ind_order_mu <- order(df_plot$mu, df_plot$Item, decreasing = T)
  df_plot$Item <- factor(df_plot$Item,
                         levels = unique(df_plot$Item),
                         ordered = T)
    #------------------------------------
if(is.null(color_vec)){
  # Randomly assign a color to each portfolio item if none assigned
  n_items <- ncol(df_wStar) - 1
    bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_items)
    color_vec <- sample(bag_of_colors, n_items)
}
    #------------------------------------
#  df_plot$`Risk (standard deviation)` <- as.factor(df_plot$`Risk (standard deviation)`)
  gg <- ggplot(df_plot, aes(x = `Risk (standard deviation)`, y = `Budget shares`, fill = Item))
  #legend_position <- "right"
  gg <- gg + geom_area(position = "stack")
 # gg <- gg + geom_bar(stat = "identity")
  gg <- gg + scale_fill_manual(values = color_vec)

  gg <- gg + theme(legend.title = element_blank(),
                   legend.position = legend_position,
                   legend.text = element_text(size = 8),
                   axis.title = element_text(size = 8),
                   axis.text = element_text(size = 8)
                   )
  if(axis_titles == "off"){
    gg <- gg + theme(axis.title = element_blank())
  }
  if(axis_titles == "x only"){
    gg <- gg + theme(axis.title.y = element_blank())
  }
  if(Xaxis_numbers_off){
    gg <- gg + theme(axis.text.x = element_blank())
  }
  if(!is.null(fig_title)){
    gg <- gg + labs(title = fig_title)
    gg <- gg + theme(plot.title = element_text(size = 8))
  }
  if(length(unique(df_plot$Item)) > 15){
    gg <- gg + theme(legend.position = "none")
  }
  gg_weights <- gg
  
  if(graph_on){print(gg)}
  
  return(gg_weights)
  
}
#=========================================================
plot_frontier <- function(df_frontier, ROI_basis = F, list_graph_options = NULL, graph_on = T){
  if(!is.null(list_graph_options)){
    fig_title <- list_graph_options[["fig_title"]]
  }else{
    fig_title <- NULL
  }
  
    df_plot <- df_frontier
  # if(ncol(df_plot) == 3){
  #   gathercols <- colnames(df_plot)[2:3]
  #   df_plot <- df_plot %>% gather_("Type", "Value", gathercols)
  #   gg <- ggplot(df_plot, aes(x = `Risk (standard deviation)`, y = Value, group = Type, color = Type))
  # }else{
  #   gg <- ggplot(df_plot, aes(x = `Risk (standard deviation)`, y = `Return target`))
  # }
  if(ROI_basis){
    gg <- ggplot(df_plot, aes(x = `Risk (standard deviation)`, y = `ROI target`))
  }else{
    gg <- ggplot(df_plot, aes(x = `Risk (standard deviation)`, y = `Return target`))
  }
  if(!is.null(fig_title)){
    gg <- gg + labs(title = fig_title)
    gg <- gg + theme(plot.title = element_text(face = "bold", size = 10))
  }
  gg <- gg + theme(axis.title.x = element_blank(),
                   axis.text.x = element_blank(),
                   axis.title.y = element_text(size = 8),
                   axis.text.y = element_text(size = 8))
  gg <- gg + geom_point()
  if(graph_on){print(gg)}
  return(gg)
}
#=======================================================================
get_optimal_frontier <- function(covmat, mat_nab,
                                 fun_env = NULL){
  #-------------------------------------------
  if(is.null(fun_env)){
    n_points_on_frontier = 50
    R_range = 0.3
    backtest_info = NULL
    C_targ = 1
    dimRet = F
  }else{
    n_points_on_frontier = fun_env[["n_points_on_frontier"]]
    R_range <- fun_env[["R_range"]]
    backtest_info = fun_env[["backtest_info"]]
    C_targ = fun_env[["C_targ"]]
    dimRet = fun_env[["dimRet"]]
  }
  #-------------------------------------------
      covmat_inv <- solve(covmat)
  M <- t(mat_nab) %*% covmat_inv %*% mat_nab # Merton matrix
  M_inv <- solve(M)
  R_at_minRisk <- M[1, 2] / M[2, 2] * C_targ
  #minRisk <- 
  Rtarg_vec <- seq(R_at_minRisk, R_at_minRisk + R_range, length.out = n_points_on_frontier)
  #-------------------------------------------
  list_wStar <- list()
  list_frontier <- list()
  for(i in 1:length(Rtarg_vec)){
    this_Rtarg <- Rtarg_vec[i]
    targ_vec <- c(this_Rtarg, C_targ)
        #-------------------------------------------
          list_out <- optimize_portfolio(M_inv, covmat_inv, covmat, mat_nab, targ_vec,
                                   backtest_info)
      #-------------------------------------------
    list_wStar[[i]] <- list_out[[1]]
    list_frontier[[i]] <- list_out[[2]]
  }
  #-------------------------------------------
  df_frontier <- data.frame(do.call(rbind, list_frontier))
  colnames(df_frontier) <- c("Return target",
                             "Risk (standard deviation)",
                             "Risk shadow price",
                             "Budget shadow price",
                             "Return backtest",
                             "Risk backtest")
  df_wStar <- data.frame(df_frontier$`Risk (standard deviation)`, t(do.call(cbind, list_wStar)))
  varNames_ordered <- row.names(mat_nab)
  colnames(df_wStar) <- c("Risk (standard deviation)", varNames_ordered)
  
  df_frontier$`Tot. investment` <- rowSums(abs(df_wStar[, -1]))
  df_frontier$`ROI target` <- df_frontier$`Return target` / df_frontier$`Tot. investment`
  df_frontier$`ROI backtest` <- df_frontier$`Return backtest` / df_frontier$`Tot. investment`
  #--------------------------------------
  list_out <- list(df_wStar, df_frontier, R_at_minRisk)
  return(list_out)
}
#=======================================================================
#=======================================================================
# End function definitions
#=======================================================================
#=======================================================================
# Conventional risk-reward frontier
n_items <- ncol(mat_ts_eg_train)
C_targ <- 1
nab_C <- rep(1, n_items)
utility_interpretation <- F
#--------------------------------------------------------------
# Covariance matrix
covmat_train <- covmat_decDiff_train #diag(eigen(covmat_decDiff_train)$values)
covmat_test <- covmat_decDiff_test #diag(eigen(covmat_decDiff_test)$values)
#--------------------------------------------------------------
# Expected returns vector
nab_decRet_train <- nab_decRet_eg_train
nab_decRet_test <- nab_decRet_eg_test
  #---
  # check
# nab_decRet_train <- (mat_ts_eg_train[nrow(mat_ts_eg_train), ] - mat_ts_eg_train[1, ]) / mat_ts_eg_train[1, ]
# nab_decRet_test <- (mat_ts_eg_test[nrow(mat_ts_eg_test), ] - mat_ts_eg_test[1, ]) / mat_ts_eg_test[1, ]
  # mat_pctDiff <- 100 * diff(mat_ts_eg_train) / mat_ts_eg_train[-nrow(mat_ts_eg_train), ]
  # nab_pctRet_check <- 100 * (apply(10^-2 * mat_pctDiff, 2, function(x) prod(1 + x)) - 1)
  # nab_pctRet / nab_pctRet_check
#--------------------------------------------------------------
mat_nab <- cbind(nab_decRet_train, nab_C)
n_points_on_frontier <- 50
R_range <- 0.25
backtest_info <- list()
backtest_info[["nab_decRet_test"]] <- nab_decRet_test
backtest_info[["covmat_test"]] <- covmat_test
#--------------------------------------------------------------
fun_env_getOptFront <- list()
fun_env_getOptFront[["n_points_on_frontier"]] <- n_points_on_frontier
fun_env_getOptFront[["R_range"]] <- R_range
fun_env_getOptFront[["backtest_info"]] <- backtest_info
fun_env_getOptFront[["C_targ"]] <- C_targ
fun_env_getOptFront[["dimRet"]] <- F
#--------------------------------------------------------------
list_out <- get_optimal_frontier(covmat_train, mat_nab,
                                 fun_env = fun_env_getOptFront)
df_wStar <- list_out[[1]]
df_frontier <- list_out[[2]]
R_at_minRisk <- list_out[[3]]
#--------------------------------------------------------------
gg_frontier <- plot_frontier(df_frontier, ROI_basis = F, list_graph_options = NULL, graph_on = F)
#--------------------------------------------------------------
# df_wStar_agg <- df_wStar %>% gather_("Item", "Budget Share", colnames(df_wStar)[-1])
# df_wStar_agg <- merge(df_wStar_agg, df_match_group, by = "Item")
# df_wStar_agg <- df_wStar_agg %>% group_by(`Risk (standard deviation)`, Group) %>% summarise(`Budget Share` = sum(`Budget Share`)) %>% spread(Group, `Budget Share`)
#gg_budget <- plot_budgetShares(df_wStar_agg, group_small = NULL, color_vec = group_colors, graph_on = F, list_graph_options = NULL)
n_items <- ncol(df_wStar) - 1
bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_items)
color_vec_mv_eg <- sample(bag_of_colors, n_items)
gg_budget <- plot_budgetShares(df_wStar, color_vec = color_vec_mv_eg, graph_on = F, list_graph_options = NULL)
#--------------------------------------------------------------
gg_frontier + gg_budget + plot_layout(ncol = 1)
#https://gotellilab.github.io/GotelliLabMeetingHacks/NickGotelli/ggplotPatchwork.html
#--------------------------------------------------------------
df_frontier_conv <- df_frontier
df_wStar_conv <- df_wStar


df_plot <- df_frontier
df_plot_mod <- df_plot[, c("Risk (standard deviation)", "ROI target")]
df_plot_btest <- df_plot[, c("Risk backtest", "ROI backtest")]
df_plot_mod$Type <- "Optimal\nsolution"
df_plot_btest$Type <- "Backtest"
colnames(df_plot_mod) <- c("Risk", "ROI", "Type")
colnames(df_plot_btest) <- c("Risk", "ROI", "Type")
df_plot <- rbind(df_plot_mod, df_plot_btest)

gg <- ggplot(df_plot, aes(x = Risk, y = ROI, group = Type, color = Type))
gg <- gg + geom_point()
gg


```


# Adapting MV analysis to the AR4D context

## Sub-linear scaling and positive budget shares

As explained in the introduction, in financial contexts investments scale linearly. A doubling of the investment doubles expected return. In the AR4D context, on the other hand, returns to investment are marginally diminishing. In other words, the additional return resulting from any small increase in investment in the $i^{th}$ portfolio item will be inversely proportionate to the quantity already invested in the $i^{th}$ portfolio item. This statement can be formalized as follows.

\begin{equation}
\frac{\partial R}{\partial w_i} \sim \frac{d \ln (w_i)}{dw_i}
\label{eq:uDefine}
\end{equation}

<!-- Note that multiplying through by $R / w_i$ gives an interesting alternative expression, -->

<!-- \begin{equation} -->
<!-- \frac{\partial \ln R}{\partial \ln w_i} \sim \frac{d \ln R}{dR} -->
<!-- \end{equation} -->

<!-- The _percentage increase in_ return resulting from _a one percent_ increase in investment in the $i^{th}$ portfolio item will be inversely proportionate to the _return_ already achieved. -->

Equation \ref{eq:uDefine} can be used to arrive at an expression for $R$ as follows:

\begin{eqnarray}
R(\mathbf{w}; \boldsymbol{\alpha}, k) &=& \int \nabla_{\mathbf{w}} R \cdot d\mathbf{w} \\
&=& \int \nabla_{\mathbf{w}} \ln(\mathbf{w}) \cdot d\mathbf{w} \\
&=& \boldsymbol{\alpha} \cdot \ln(\mathbf{w}) + k
\end{eqnarray}

where the proportionality constants $\boldsymbol{\alpha}$ are determined by context. In the present setting, the constants $\boldsymbol{\alpha}$ correspond to the vector $\boldsymbol{\mu}$ in the $R$ function. This leaves just $k$, the constant of integration, unfixed. $R$ is thus said to be defined up to translation.

\begin{equation}
R(\mathbf{w}; \boldsymbol{\mu}, k) = \boldsymbol{\mu} \cdot \ln(\mathbf{w}) + k
\end{equation}

By deducing a return function from the law of diminishing marginal returns, the budget shares $\mathbf{w}$ in the portfolio optimization set up above are effectively replaced with $\ln(\mathbf{w})$, which can be interpreted as a measure of their usefulness or utlity. The problem then becomes one of allocating optimal "utility shares" rather than budget shares. For optimization purposes, then, the budget constraint must likewise be adapted from its monetary form

\begin{equation}
C = \mathbf{1} \cdot \mathbf{w}
\end{equation}

to a utility budget form

\begin{equation}
C = \mathbf{1} \cdot \ln(\mathbf{w})
\end{equation}

Making these replacements to the portfolio optimization set up above, and differentiating with respect to $\ln(\mathbf{w})$ instead of $\mathbf{w}$, MV Analysis outputs the optimal utility budget shares $\ln(\mathbf{w}^*)+k$, rather than the optimal monetary budget shares. The optimal monetary budget shares, up to a factor $e^k$, are then easily obtained by applying the exponential function. As a result, utility shares may be positive or negative, but the monetary budget shares $\mathbf{w}^*$ must always be positive. By enforcing the law of diminishing marginal returns, then, one also forces monetary budget shares to be positive.

Of course, the investor or donor is constrained by a real monetary budget, and so will only be interested in solutions $e^k\mathbf{w}^*$ such that the elements in $e^k\mathbf{w}^*$ sum to that monetary budget. To enforce this requirement, then, simply choose $k$ such that the elements $e^k \mathbf{w}^*$ sum to the budget constraint.

\pagebreak

The optimal frontier and budget shares for a portfolio with diminishing marginal returns is presented in Figure \ref{fig:mvUtility}. At first glance, this frontier appears inferior to the efficient frontier resulting from conventional MV Analysis (Figure \ref{fig:mvConv}). However, the high expected returns observed in the conventional approach are mostly attributable to unrealistic leveraging of the portfolio, resulting from negative budget shares. That is to say, a sum of the absolute values of the budget shares displayed in Figure \ref{fig:mvConv} would indicate that the investor must be prepared to borrow sums in excess of 2 to 6 times their budget in order to take up the corresponding positions on the frontier. To be fair, then, a comparison between the conventional approach and the diminishing returns approach must be made on the basis of expected return on investment (equation \ref{eq:roi_def}), rather than expected return alone.

\begin{equation}
ROI = \frac{R-C}{C}
\label{eq:roi_def}
\end{equation}

This comparison is presented in Figure \ref{fig:mv_roiCompare}. On an ROI basis, then, the efficient frontier generated by the diminishing returns approach is superior to that of the conventional approach.

<!-- [in practice, real portfolio returns are often far lower, and far riskier, than the optimal frontier would suggest. This is evident in the portfolio backtests in Figure \ref{fig:...}.   This is especially true when the covariance or correlation matrix used in the analysis is ill conditioned.  frontiers often diverge from reality.   the frontier is only as accurate as the data it is based on. The backtests are shown in Figure \ref{fig:mvBacktest} ... Also to be expected that the conventional frontier backtest is higher than the utility frontier backtest simply because negative weights require that the total investment is greater than that of the portfolio with only positive weights. If we look not just at the return, but at the return on investment, then the utility weights approach performs better than the conventional approach. Keep in mind this is just one scenario. you'd have to look at many before concluding definitively one way or the other.] -->


<!-- # ```{r, fig.show='hold', fig.width=4, fig.height=2, fig.align='center', fig.cap="\\label{fig:mv_roiCompare}Comparison of the conventional and utility efficient frontiers on an ROI basis.", echo = FALSE} -->

<!-- mv_conv_U_compare_colors <- c("black", "blue") -->
<!-- df_plot_conv <- df_frontier_conv -->
<!-- df_plot_conv$Tot_investment <- rowSums(abs(df_wStar_conv[, -1])) -->
<!-- df_plot_conv$`ROI target` <- df_plot_conv$`Return target` / df_plot_conv$Tot_investment -->
<!-- df_plot_conv$`Budget shares` <- "Conventional\n(can be negative)" -->
<!-- df_plot_U <- df_frontier_U -->
<!-- df_plot_U$Tot_investment <- rowSums(abs(df_wStar_U[, -1])) -->
<!-- df_plot_U$`ROI target` <- df_plot_U$`Return target` / df_plot_U$Tot_investment -->
<!-- df_plot_U$`Budget shares` <- "Utility\n(positive only)" -->
<!-- df_plot <- rbind(df_plot_conv, df_plot_U) -->
<!-- gg <- ggplot(df_plot, aes(x = `Risk (standard deviation)`, y = `ROI target`, group = `Budget shares`, color = `Budget shares`)) -->
<!-- gg <- gg + geom_point() -->
<!-- gg <- gg + scale_color_manual(values = mv_conv_U_compare_colors) -->
<!-- gg -->
<!-- #-------------------------------------------------------- -->
<!-- # Prepare for the backtest plots farther ahead -->
<!-- df_plot_conv$`ROI backtest` <- df_plot_conv$`Return backtest` / df_plot_conv$Tot_investment -->
<!-- df_plot_U$`ROI backtest` <- df_plot_U$`Return backtest` / df_plot_U$Tot_investment -->
<!-- #======================================================================= -->

<!-- ``` -->


## "Reverse engineering" the covariance matrix

In principal components analysis, a dataset $X$ containing $t$ observations of $n$ variables is distilled into a dataset $S$ of just $k<n$ variables that capture the main tendencies and structure in the data.^[The data is always centered. If the variables in $X$ follow diverse scalings and/or units of measurement (i.e. if apples are being compared to oranges), then $X$ should also be scaled to unit variance. In this exposition, the variables are all of the same type, and so $X$ is centered but not scaled. See [@AbdiPCA] for an introduction to principal components analysis.] The distilled matrix $S$ is defined

\begin{equation}
S = X \tilde{P}
\label{eq:sigs_def}
\end{equation}

where $\tilde{P}$ is a matrix containing the $k$ leading eigenvectors of the full set of eigenvectors $P$, which is taken from the eigendecomposition of the data covariance matrix $\Sigma_{XX}$ (equation \ref{eq:eigDecomp}).

\begin{equation}
\Sigma_{XX} = PUP'
\label{eq:eigDecomp}
\end{equation}

where $U$ is the diagonal matrix of eigenvalues of $\Sigma_{XX}$. 

From the definition (\ref{eq:sigs_def}), it follows that the signals are uncorrelated with each other, and that their variance is given by the eigenvalues of the data covariance matrix.

\begin{eqnarray}
\Sigma_{SS} &=& \frac{1}{n-1}S'S \\
&=& \frac{1}{n-1} \tilde{P}'X'X\tilde{P} \\
&=& \tilde{P}'\Sigma_{XX}\tilde{P} \\
&=& \tilde{P}'PUP'\tilde{P}=\tilde{U}
\label{eq:covmat_SS}
\end{eqnarray}

The columns of the distilled matrix $S$ are referred to as the principal components (PC), or the PC scores, or the factor scores. When dealing with noisy time series, as in this paper, they might just as well be reffered to as the "signals", in the sense that they are signals extracted from noise.

The meaning of the signals can be interpreted based on how correlated they are with the variables in the original dataset. These correlations ($K_{XS}$) are found by first finding their corresponding covariances ($\Sigma_{XS}$).

\begin{eqnarray}
\Sigma_{XS} &=& \frac{1}{n-1}X'S \\
&=& \frac{1}{n-1}X'XP \\
&=& KP = PUP'P \\
&=& PU
\end{eqnarray}

The correlation matrix $K_{XS}$ then follows as

\begin{eqnarray}
K_{XS} &=& D(\boldsymbol{\sigma}_X)^{-1} \Sigma_{XS} D(\boldsymbol{\sigma}_S)^{-1} \\
&=&D(\boldsymbol{\sigma}_X)^{-1} PU D(\boldsymbol{\sigma}_S)^{-1}
\end{eqnarray}

But the standard deviations of the signals are just the square roots of the eigenvalues (recall equation \ref{eq:covmat_SS}), so this reduces to

\begin{eqnarray}
K_{XS} &=& D(\boldsymbol{\sigma}_X)^{-1}\Sigma_{XS} U^{-{1 \over 2}} \\
&=& D(\boldsymbol{\sigma}_X)^{-1}PU U^{-{1 \over 2}} \\
&=& D(\boldsymbol{\sigma}_X)^{-1}PU^{1 \over 2}
\end{eqnarray}

Note that if $X$ were scaled to unit variance, then this would reduce further to

\begin{equation}
K_{XS} = PU^{1 \over 2}
\end{equation}

The correlations matrix $K_{XS}$ is sometimes referred to as the "loadings" matrix, in the sense that it indicates how much each variable in $X$ loads onto a given signal (or, vice versa, how much each signal loads onto a given variable).^[Although many prefer to call $P$ the loadings.] In keeping with this convention, and in order to reduce notational clutter, $K_{XS}$ is henceforth relabeled $L$.

The correlations between the financial assets from the example above and their four leading signals are presented in Figure \ref{fig:corrXS_barchart}. Concrete meaning can be attributed to each of these otherwise abstract signals by examining how correlated they are with the portfolio items.


```{r, fig.show='hold', fig.width=6, fig.height=3, fig.align='right', fig.cap="\\label{fig:corrXS_barchart}Correlation of portfolio items with leading signals."}


# Define function to calculate variable-signal correlations, financial data
get_S_and_corrXS <- function(mat_X_in){
  # mat_P = eigenvectors of the data correlation matrix
  # mat_U = corresponding eigenvalues
  mat_X_centered <- scale(mat_X_in, scale = F)
  # out_svd <- svd(mat_X_centered)
  # sing_values <- out_svd$d
  # n_obs <- nrow(mat_X_centered)
  # eig_values <- sing_values^2 / (n_obs - 1)
  # mat_P <- out_svd$v

mat_P <- eigen(cov(mat_X_in))$vectors
if(mean(mat_P[, 1]) < 0){mat_P <- -mat_P}
eig_values <- eigen(cov(mat_X_in))$values
mat_U <- diag(eig_values)
  
#mat_P_sigs <- mat_P[, 1:n_signals]
# eig_values[1:n_signals] / eigen(cov(mat_X_centered))$values[1:n_signals] #check
# mat_P / eigen(cov(mat_X_in))$vectors #check

#mat_U <- diag(eig_values)

#mat_U_sigs <- matU[, 1:n_signals]
#---------------------------------------------
sd_X <- apply(mat_X_in, 2, sd)
D_sdX_inv <- diag(1 / sd_X)
cormat_XS <- D_sdX_inv %*% mat_P %*% sqrt(mat_U)
row.names(cormat_XS) <- colnames(mat_X_in)
mat_L <- cormat_XS
#mat_L <- diag(1 / apply(mat_X_in, 2, sd)) %*% mat_P %*% sqrt(mat_U)
  #---------------------------------------------------------
  # Set sign of eigenvectors such that signals best conform to their most highly correlated items
# First have to get average of highest correlated items for each signal
corrThresh <- 0.55
n_items <- ncol(mat_L)
list_X_hiCorr_avg <- list()
  for(i in 1:n_items){
    this_loadvec <- mat_L[, i]
    ind_tracks <- which(abs(this_loadvec) >= corrThresh)
    if(length(ind_tracks) == 0){
        ind_tracks <- which(abs(this_loadvec) == max(abs(this_loadvec)))
    }
    if(length(ind_tracks) == 1){
              list_X_hiCorr_avg[[i]] <- mat_X_centered[, ind_tracks]
    }else{
          loadvec_kept <- this_loadvec[ind_tracks]
    list_X_hiCorr_avg[[i]] <- rowMeans(mat_X_centered[, ind_tracks])

    }
}
mat_X_hiCorr_avg <- do.call(cbind, list_X_hiCorr_avg)
mat_S_all <- mat_X_centered %*% mat_P
#mat_S_all <- mat_X_in %*% mat_P
for(i in 1:n_items){
  this_S <- mat_S_all[, i]
  this_X_hiCorr_avg <- mat_X_hiCorr_avg[, i]
  mse <- mean((this_S - this_X_hiCorr_avg)^2)
  mse_neg <- mean((-this_S - this_X_hiCorr_avg)^2)
  if(mse_neg < mse){
        mat_P[, i] <- -mat_P[, i]
      }
    }
cormat_XS <- D_sdX_inv %*% mat_P %*% sqrt(mat_U)
row.names(cormat_XS) <- colnames(mat_X_in)
mat_L <- cormat_XS
mat_S_all <- mat_X_centered %*% mat_P
#---------------------------------------------
# res <- FactoMineR::PCA(mat_pctDiff_in, scale.unit = F, ncp = ncol(mat_pctDiff_in), graph = F)
# mat_L_FactoMiner <- res$var$coord
# mat_L / mat_L_FactoMiner

list_out <- list(mat_S_all, cormat_XS, eig_values, mat_P)
return(list_out)
}
#====================================================
# Barchart of variable-signal correlations
plot_corrXS_barchart <- function(mat_L, group_info = NULL, xAxis_title = NULL, sigNames = NULL){
  
  n_signals <- ncol(mat_L)
    df_plot <- data.frame(Item = row.names(mat_L), mat_L)
    df_plot$Item <- as.character(df_plot$Item)
    #-------------------------------------------------------
    if(is.null(sigNames)){
  signal_id <- paste("Signal", 1:n_signals)
}else{
  signal_id <- paste("Signal", 1:n_signals, "-", sigNames)
}
  colnames(df_plot)[2:(n_signals + 1)] <- signal_id
  #-------------------------------------------------------
  gathercols <- as.character(signal_id) 
  df_plot <- gather_(df_plot, "Signal", "Correlation", gathercols)
    df_plot <- transform(df_plot,
                       Signal = factor(Signal, levels = gathercols))

  if(!is.null(group_info)){
  outlist <- group_fn(group_info)
  cols_ordered_by_group <- outlist[[1]]
  group_color_vec <- outlist[[2]]
  group_vec_ordered <- outlist[[3]]
  df_match_group <- data.frame(Item = cols_ordered_by_group, Group = group_vec_ordered)
  df_plot <- merge(df_plot, df_match_group, by = "Item")
  df_plot <- df_plot[order(df_plot$Group), ]
  df_plot$Item <- factor(df_plot$Item, levels = unique(df_plot$Item))
     gg <- ggplot(df_plot, aes(x = Item, y = Correlation, fill = Group))
     gg <- gg + scale_fill_manual(values = unique(group_color_vec))
    }else{
      gg <- ggplot(df_plot, aes(x = Item, y = Correlation))
  }
  gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
  gg <- gg + ylim(limits = c(-1, 1))
  gg <- gg + facet_wrap(~ Signal, nrow = 1)
  if(!is.null(xAxis_title)){
    gg <- gg + labs(y = xAxis_title)
  }
  gg <- gg + theme(axis.text = element_text(size = 8),
                   axis.title.x = element_text(size = 8),
                   axis.title.y = element_blank(),
                   legend.title = element_blank(),
                   legend.text = element_text(size = 8),
                   strip.text = element_text(size = 8))
  gg <- gg + coord_equal()
  gg <- gg + coord_flip()
  gg

}

#====================================================
n_signals <- 4
mat_X_in <- mat_decDiff_eg_train
#mat_X_in <- mat_ts_eg_train
list_out <- get_S_and_corrXS(mat_X_in)
mat_S_all <- list_out[[1]]
cormat_XS <- list_out[[2]]
eig_values <- list_out[[3]]
mat_P <- list_out[[4]]
mat_S <- mat_S_all[, 1:n_signals]
mat_L <- cormat_XS[, 1:n_signals]

#eigen(cov(mat_S))$values / eig_values[1:n_signals]

plot_corrXS_barchart(mat_L)

```

Clearly, Signal 4 represents movements in communications. Signal 3 is concerned with Utilities and Real Estate, and so might be called the "Housing & Urban Development Signal". The interpretation of Signals 1 and 2 is not so straightforward, since they are correlated with many portfolio items. In such cases, it is often useful to apply an orthogonal rotation to $L$ in order to clarify the picture. That is to say, instead of examining $L$, one examines $L_{\circlearrowleft}$.

\begin{equation}
L_{\circlearrowleft} = LB
\end{equation}

where $B$ is the orthogonal rotation matrix, such that $B'B = I$.
<!-- Note that a rotation of $L$ implies a rotation of the signals, since -->

<!-- \begin{equation} -->
<!-- SR = XP -->
<!-- \end{equation} -->

In Figure \ref{fig:corrXS_barchart_rot} a special kind of orthogonal rotation, called a varimax rotation, is applied to $L$. Varimax rotations flesh out structure by maximizing sparseness in the rotated matrix. After applying this rotation, it becomes clear that Signal 1 is representative of Biotechnology and Healthcare; and so Signal 1 might be called the "Pharmaceutical Signal". Signal 2 loadings are now more pronounced, especially Financials, Industrial, and Transportation. Signal 2 might thus be called the "Financial and Physical Infrastructure Signal." In Figure \ref{fig:signals_with_hiCorr_items}, it becomes particularly evident how each signal hews closely to its most highly correlated items, offering visual confirmation of these interpretations. 

```{r, fig.show='hold', fig.width=6, fig.height=3, fig.align='left', fig.cap="\\label{fig:corrXS_barchart_rot}Varimax rotated correlation of portfolio items with leading signals.", echo=F}

mat_Lrot <- varimax(mat_L)[[1]]
mat_Lrot <- matrix(as.numeric(mat_Lrot),
                          attributes(mat_Lrot)$dim,
                          dimnames = attributes(mat_Lrot)$dimnames)
mat_R <- varimax(mat_L)[[2]]
mat_R <- matrix(as.numeric(mat_R),
                          attributes(mat_R)$dim,
                          dimnames = attributes(mat_R)$dimnames)

xAxis_title <- "Varimax Rotated Correlation"
plot_corrXS_barchart(mat_Lrot, group_info = NULL, xAxis_title, sigNames = NULL)


```


```{r, fig.show='hold', fig.width=6, fig.height=5, fig.align='left', fig.cap="\\label{fig:signals_with_hiCorr_items}Signals plotted together with their most highly correlated assets. The signals are plotted as thick grey lines."}
# Define function to plot signals with their highest loading items.
plot_sigs_wHiLoad_items <- function(mat_X_in, mat_S, mat_L, color_vec, sigNames = NULL, load_threshold = 0.55, n_display_max = 4){
n_signals <- ncol(mat_L)
if(is.null(sigNames)){
  fig_title_vec <- paste("Signal", 1:n_signals)
}else{
  fig_title_vec <- paste("Signal", 1:n_signals, "-", sigNames)
}
# Plot signals against highest correlated variables
  date_vec <- row.names(mat_S)
  xAxis_labels <- date_vec[seq(1, nrow(mat_S), length.out = 5)]
  list_gg <- list()
  for(i in 1:n_signals){
    this_loadvec <- mat_L[, i]
    ind_tracks <- which(abs(this_loadvec) >= load_threshold)
    if(length(ind_tracks) == 0){
        ind_tracks <- which(abs(this_loadvec) == max(abs(this_loadvec)))
    }

    n_display <- length(ind_tracks)
    loadvec_kept <- this_loadvec[ind_tracks]
    if(n_display > n_display_max){
      n_to_omit <- n_display - n_display_max
      #random_omission <- sample(1:n_display, n_to_omit)
      #mat_pctDiff_tracks <- mat_pctDiff_tracks[, -random_omission]
      ind_omit <- order(loadvec_kept)[c(1:n_to_omit)]
      ind_tracks <- ind_tracks[-ind_omit]
      loadvec_kept <- this_loadvec[ind_tracks]
    }
    mat_X_tracks <- mat_X_in[, ind_tracks]
    color_vec_tracks <- color_vec[ind_tracks]
    #------------
    df_plot_sig <- data.frame(Date = date_vec, Value = mat_S[, i])
    df_plot_tracks <- data.frame(Date = date_vec, mat_X_tracks)
    if(ncol(df_plot_tracks) > 2){
      gathercols <- colnames(mat_X_tracks)
      colnames(df_plot_tracks)[-1] <- gathercols
      df_plot_tracks <- df_plot_tracks %>% gather_("ts", "Value", gathercols)
    }else{
      colnames(df_plot_tracks)[-1] <- "Value"
      df_plot_tracks$ts <- colnames(mat_X_in)[ind_tracks]
    }
    #------------
    gg <- ggplot()
    gg <- gg + geom_line(data = df_plot_sig, aes(x = Date, y = Value, group = 1), color = "grey", lwd = 1.3)
    gg <- gg + geom_line(data = df_plot_tracks, aes(x = Date, y = Value, group = ts, color = ts))
    gg <- gg + scale_color_manual(values = color_vec_tracks)
    gg <- gg + scale_x_discrete(breaks = xAxis_labels)
    gg <- gg + labs(title = fig_title_vec[i])

    if(i == n_signals){
      #gg <- gg + labs(title = paste("Signal", i), caption = fig_title)
      gg <- gg + theme(#axis.text.x = element_text(angle = 60, hjust = 1),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.title = element_blank(),
        plot.title = element_text(size = 9)
        #plot.caption = element_text(hjust = 0, size = 10)
      )
    }else{
      gg <- gg + theme(axis.text.x = element_blank(),
                       axis.title.x = element_blank(),
                       axis.title.y = element_blank(),
                       legend.title = element_blank(),
                       plot.title = element_text(size = 9))
    }
    
    list_gg[[i]] <- gg
    
  }
  
  gg_all <- wrap_plots(list_gg) + plot_layout(ncol = 1)
  gg_all
}

# Name signals based on correlations
# sigNames <- c("Portfolio average\n(Especially Pharmaceutical, Technology, and Luxury Goods)",
#               "Transportation and Industrial",
#               "Housing & Urban Development (Inverse)",
#               "Communications (Inverse)")
# 
# mat_X_in <- mat_pctDiff_in
# plot_sigs_wHiLoad_items(mat_X_in, mat_S, mat_L, sigNames, load_threshold = 0.55, n_display_max = 4)
sigNames <- c("Pharmaceutical",
              "Financial and Physical Infrastructure",
              "Housing & Urban Development",
              "Communications")
  

color_vec <- color_vec_mv_eg
plot_sigs_wHiLoad_items(mat_X_in, mat_S, mat_Lrot, color_vec, sigNames, load_threshold = 0.65, n_display_max = 4)
  
```  

In practice, the number of signals that should be distilled from the original data set $X$ depends upon how much of the variance in $X$ the researcher wishes to capture or reflect in the signals, and how many signals are required to reach this subjectively determined threshold. The portion of the system's evolution reflected in any given signal ($c_i$) is defined as the signal's variance divided by the sum of all signal variances. Recalling from equation \ref{eq:covmat_SS} that a signal's variance is just the corresponding eigenvalue $u_i$ extracted from $\Sigma_{XX}$, this is expressed

\begin{equation}
c_i = \frac{u_i}{\sum_{i = 1}^n u_i}
\end{equation}

The cumulative variance captured by a group of $k$ signals is then

\begin{equation}
c_k <- \frac{\sum_{i=1}^k u_i}{\sum_{i = 1}^n u_i}
\end{equation}

The individual and cumulative portions explained by each signal are plotted in Figure \ref{fig:varExplained_barchart}. Customarily, researchers like to retain signals such that at least 90% of the variance in the original data is explained. The horizontal dashed line in the plot marks this subjective threshold.

```{r, fig.show='hold', fig.width=5, fig.height=2, fig.align='center', fig.cap="\\label{fig:varExplained_barchart}Plot of the individual and cumulative portions of variance explained by each signal.", echo=F}
c_vec <- eig_values / sum(eig_values)
ck_vec <- cumsum(c_vec)
  df_plot <- data.frame(Signal = paste("Signal", 1:length(eig_values)), Portion_explained = c_vec, Portion_explained_cumul = ck_vec)
  colnames(df_plot)[2:3] <- c("Individually", "Cumulatively")
  gathercols <- colnames(df_plot)[2:3]
  df_plot <- gather_(df_plot, "Portion explained", "Value", gathercols)
  df_plot$Signal <- factor(df_plot$Signal,
                           levels = unique(df_plot$Signal),
                           ordered = T)
    gg <- ggplot(df_plot, aes(x = Signal, y = Value, fill = `Portion explained`))
  gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
  gg <- gg + scale_fill_manual(values = c("wheat3", "paleturquoise"))
  gg <- gg + geom_hline(yintercept = 0.9, color = "coral", size = 1, linetype = "dashed")
  gg <- gg + theme(axis.text.y = element_text(size = 9),
                   axis.text.x = element_text(size = 9, angle = 60, hjust = 1),
                   axis.title = element_blank(),
                   legend.title = element_text(size = 9),
                   legend.text = element_text(size = 9))

  gg
  
  #====================================================
n_signals <- which(ck_vec > 0.9)[1]
#n_signals <- 3
#====================================================

  
```
<!-- ... perils of such rules of thumb have been documented [@...]. A more rigorous alternative has been proposed by ...[@...], building off of earlier work...theorem in random matrix theory...etc. An advantage of this method is that it tells the researcher how much of the data can be meaningfully distinguished from noise, rather than the researcher arbitrarily imposing a fidelity upon the data which it may not possess. -->

<!-- The perils of using these arbitrary cutoff rules has been well documented [@russell2002search]. Here I follow Laloux, Cizeau, Bouchaud, and Potters [-@laloux1999noise] who apply a more rigorous technique developed by physicists in the 1960s [@dehesa1983mathematical; @mehta2004random]. The technique is based on a key theorem of Random Matrix Theory that says that the distribution of the eigenvalues of any random correlation matrix is a function of the dimensions of the underlying matrix. Physicists have used this theorem to identify the important components ("collective modes") in complexly interacting systems by comparing the eigenvalue density plot of their data correlation matrix against that of a random matrix. The eigenvalues of the data correlation matrix that extend beyond the random matrix eigenvalue density can be identified as corresponding to components of the system that can be meaningfully distinguished from noise. -->


<!-- \begin{equation} -->
<!-- \sigma = 1 - {u_{max} \over n} -->
<!-- \end{equation} -->

<!-- \begin{equation} -->
<!-- \hat{u}_{max} = \sigma (1 + {n \over \tau} + 2 \sqrt{n \over \tau}) -->
<!-- \label{eq:eigvalMax_randMat} -->
<!-- \end{equation} -->

<!-- # ```{r} -->
<!--   n_items <- ncol(mat_pctDiff_in) -->
<!--   Q <- n_obs / n_items -->

<!--       eigvalDens_fn <- function(sigma2_rand, Q){ -->
<!--       eigvalRand_max <- sigma2_rand * (1 + 1 / Q + 2 / sqrt(Q)) -->
<!--   eigvalRand_min <- sigma2_rand * (1 + 1 / Q - 2 / sqrt(Q)) -->
<!--     eig_values <- seq(eigvalRand_min, eigvalRand_max, length.out = 50) -->
<!--   eigvalDensity_rand <- Q / (2 * pi * sigma2_rand) * sqrt((eigvalRand_max - eig_values) * (eig_values - eigvalRand_min)) / eig_values -->
<!-- return(data.frame(x = eig_values, y = eigvalDensity_rand)) -->
<!--   } -->


<!--   #list_dens <- density(eig_values) -->
<!--   #sigma2_rand <- 1 - sum(eig_values[1]) / n_items -->
<!--   sigma2_rand <- 0.4 -->
<!--   df_eigvalDens <- eigvalDens_fn(sigma2_rand, Q) -->
<!--   eigvalRand_max <- max(df_eigvalDens$x) -->
<!-- which(eig_values > eigvalRand_max) -->

<!--   #df_plot <- data.frame(x = list_dens$x, y = list_dens$y) -->
<!--   df_plot <- data.frame(x = eig_values) -->
<!--   gg <- ggplot() -->
<!--   #gg <- gg + geom_histogram(data = df_plot, aes(x)) -->
<!--   gg <- gg + geom_histogram(data = df_plot, aes(x = x, y = ..count../sum(..count..))) -->
<!--   gg <- gg + geom_line(data = df_eigvalDens, aes(x, y), color = "blue") -->
<!-- gg -->

<!-- x <- df_eigvalDens$x -->
<!-- y <- df_eigvalDens$y -->
<!-- pracma::trapz(x, y) -->

<!-- sum(diff(x)*rollmean(y,2)) -->



<!--   s_sq <- 1 - eigval_max / n_items -->
<!--   #s_sq <- 1 -->
<!--   ind_keep <- which(eig_values > eigvalRand_max) -->
<!--   eig_values[ind_keep] -->
  
<!-- #``` -->

<!-- Evaluating this formula at the parameters $u_{max}$, $n$, and $\tau$ from the financial data gives $\hat{u}_{max}=$ `eigvalRand_max`. Only the 3 leading eigenvalues of $K_{XX}$ are greater than this value, suggesting that only the first 3 signals should be retained. The remainder of signals, by implication, are not meaningfully distinguishable from noise. ...suggests that 75% of the variance can be meaningfully distinguished from noise. -->

<!-- ...approximating the assets covariance matrix [which leads directly to] -->
<!-- ...signal portfolio optimization (inc. backtests?) -->
The plot thus shows that the leading `r n_signals` signals are sufficient to meet this criterion. The data correlation matrix can be approximated from these retained signals as follows.

\begin{equation}
\hat{\Sigma}_{XX} = \tilde{L}\tilde{L}'
\end{equation}

The difference between the financial data correlation matrix and the signals derived correlation matrix is shown in Figure \ref{fig:compareCorMats}. The signals derived correlation matrix is approximate in the sense that it approximates the data correlation matrix, but it should not necessarily be considered inferior in terms of accuracy. To the extent that the original data are contaminated by noise, the signals derived correlation matrix may even be more accurate with respect to the "true process" that generates the data.

```{r, fig.show='hold', fig.width=6, fig.height=4, fig.align='center', fig.cap="\\label{fig:compareCovMats}The data covariance matrix minus the covariance matrix derived from the retained signals."}
mat_S <- mat_S_all[, 1:n_signals]
mat_L <- cormat_XS[, 1:n_signals]
eig_values_sigs <- eig_values[1:n_signals]

covmat_XX <- cov(mat_pctDiff_eg_train)
covmat_XX_derived <- mat_L %*% t(mat_L)
mat_diff_covmats <- covmat_XX - covmat_XX_derived

plot_covmat(mat_diff_covmats, fig_title = NULL, graph_on = F)

#plot_covmat(covmat_XX_derived, fig_title = NULL, graph_on = F)


# cormat_XX <- cor(mat_pctDiff_train_mv)
# D_sdX <- diag(apply(mat_X_in, 2, sd))
# mat_L_cor <- D_sdX %*% mat_L
# cormat_XX_derived <- mat_L_cor %*% t(mat_L_cor)
# cormat_XX <- cor(mat_pctDiff_train_mv)
# mat_diff_cormats <- cormat_XX - covmat_XX_derived
# plot_covmat(mat_diff_cormats, fig_title = NULL, graph_on = F)



```

<!-- MV Analysis is still a work in progress [@Michaud...?]. One of the main issues limiting its usefulness in everyday applications is its sensitivity to noisy data, which often results in efficient frontiers that overstate returns and understate risk (as seen in the backtest in Figure \ref{fig:...}).-->

## The signals portfolio

```{r, fig.show='hold', fig.width=6, fig.height=3, fig.align='left', fig.cap="\\label{fig:corrXS_barchart_rot}Varimax rotated correlation of portfolio items with leading signals, backtest data.", echo=F}
mat_X_in_test <- mat_decDiff_eg_test
list_out <- get_S_and_corrXS(mat_X_in_test)
mat_S_test_all <- list_out[[1]]
cormat_XS_test <- list_out[[2]]
eig_values_test <- list_out[[3]]
mat_P_test <- list_out[[4]]
#----

mat_L_test <- cormat_XS_test[, 1:n_signals]
mat_Lrot_test <- varimax(mat_L_test)[[1]]
mat_Lrot_test <- matrix(as.numeric(mat_Lrot_test),
                          attributes(mat_Lrot_test)$dim,
                          dimnames = attributes(mat_Lrot_test)$dimnames)
mat_R_test <- varimax(mat_L_test)[[2]]
mat_R_test <- matrix(as.numeric(mat_R_test),
                          attributes(mat_R_test)$dim,
                          dimnames = attributes(mat_R_test)$dimnames)

xAxis_title <- "Varimax Rotated Correlation (test data)"
plot_corrXS_barchart(mat_Lrot_test[, 1:4], group_info = NULL, xAxis_title, sigNames = NULL)

```


```{r, fig.show = "hold", fig.width = 5, fig.height=5, fig.align='left', fig.cap="\\label{fig:mvSigs}Optimal frontier and budget shares, signals portfolio.", echo = FALSE}
ind_rearrange <- c(2, 1, 3:n_signals)
#=============================================================================
# To convince yourself that sign of eigenvectors can be changed with impunity
# M <- matrix(runif(4*4), 4, 4)
# M <- round(t(M) %*% M, 5)
# P <- eigen(M)$vectors
# U <- diag(eigen(M)$values)
# these_cols <- c(2, 4)
# P_new <- P
# P_new[, these_cols] <- -P[, these_cols]
# round(M - P %*% U %*% t(P), 4)
# round(M - P_new %*% U %*% t(P_new), 4)
#=============================================================================
#=============================================================================
# wFun <- function(l_R, q, eig_values_sigs, nab_decRet){
#   u <- eig_values_sigs
#   term0 <- q * (27 * q * u^2 + l_R^3 * nab_decRet^3)
#   term <- 18 * u * sqrt(term0) + 2 * 3^(7 / 2) * q * u^2 + sqrt(3) * l_R^3 * nab_decRet^3
#   print(term0)
# wStar_inv <- 1 / (6 * u) * (3^(-1 / 6) * term^(1 / 3) + 3^(-1 / 6) * l_R * nab_decRet + 3^(1 / 6) * l_R^2 * nab_decRet^2 * term^(-1 / 3))
# wStar_out <- 1 / wStar_inv
# return(wStar_out)
# }

wFun <- function(l_R, l_C, eig_values_sigs, nab_decRet){
  u <- eig_values_sigs
  if(l_C > 0){
      lR3_lobound <- max(-27 * u^2 * l_C / nab_decRet^3)
  lR_lobound <- sign(lR3_lobound) * abs(lR3_lobound)^(1 / 3)
  if(l_R < lR_lobound){
    l_R <- lR_lobound
  }

  }
  term0 <- round(27 * u^2 + l_R^3 * nab_decRet^3 / l_C, 15)
  term <- sqrt(term0) + sqrt(27) * u
  #print(term0)
wStar_out <- 1 / sqrt(3) * (l_C^(-1 / 3) * term^(1 / 3) - l_C^(-2 / 3) * term^(-1 / 3) * l_R * nab_decRet)
return(wStar_out)
}
#=============================================================================
# covmat_inv <- solve(10^3 * covmat)
# nab_decRet <- nab_decRet_sigs_train
# eig_values_sigs <- 1 / diag(covmat_inv)
# n <- 20
# lR_vec <- seq(-0.5, -0.01, length.out = n)
# lC_vec <- seq(0.01, 0.5, length.out = n)
# mat_plot <- matrix(NA, n, n)
# for(i in 1:n){
#   l_R <- ls_vec[i]
#   for(j in 1:n){
#     l_C <- lC_vec[j]
#     Ctarg <- 1
#     l_R <- -0.2
#     l_C <- 0.02
#     q <- l_C - l_R / Ctarg
#     wStar_out <- wFun(l_R, q, eig_values_sigs, nab_decRet)
#     C <- sum(wStar_out)
#     l_R <- l_R * C
#     Ctarg <- C
#         q <- C^3 * q#(l_C - l_R / Ctarg)
#     wStar_out <- wFun(l_R, q, eig_values_sigs, nab_decRet_in)
#     #sum(wStar_out)
#   wStar <- wStar_out
# nab_C <- wStar^2
# mat_nab <- cbind(nab_decRet, nab_C)
# 
# -Rtarg * diag(nab_decRet) %*% wStar + gamma * wStar^3 - 2 * l_s * eig_values_sigs
# 
# M <- t(mat_nab) %*% covmat_inv %*% mat_nab
# M_inv <- round(solve(M), 8)
# #targ_vec_derived <- 1 / 2 * M %*% diag(1 / c(C^3, C)) %*% lambdas
# lambdas <- c(Rtarg / l_s, gamma / l_s)
# targ_vec_derived <- 1 / 2 * M %*% lambdas
# # print(lambdas - 2 * M_inv %*% targ_vec_derived)
# # print(targ_vec_derived)
# # targ_vec_derived <- targ_vec_derived * c(C, 1 / C)
# print(exp(-targ_vec_derived[1]))
# # print(targ_vec_derived)
# # print(exp(-(targ_vec_derived[1] / C)))
# mat_plot[i, j] <- exp(-targ_vec_derived[1])
#   }
# }
# df_plot <- as.data.frame(mat_plot)
# colnames(df_plot) <- as.character(round(lC_vec, 4))
#   these_levels <- colnames(df_plot)
# gathercols <- colnames(df_plot)
# df_plot$l_R <- as.character(round(lR_vec, 4))
#   df_plot <- df_plot %>% gather_("l_C", "Value", gathercols)
#     df_plot$l_C <- factor(df_plot$l_C, levels = these_levels)
#   df_plot$l_R <- factor(df_plot$l_R, levels = these_levels)
#     midpoint <- 0
#   gg <- ggplot(df_plot, aes(l_C, l_R))
#   gg <- gg + geom_tile(aes(fill = Value))#, width = 4, height = 4)
#   gg <- gg + geom_text(aes(label = round(Value, 3)), size = 3)
#   gg <- gg + theme_bw()
#   gg <- gg + theme(axis.text.x = element_text(angle = 60, hjust = 1),
#                  axis.title = element_blank(),
#                  legend.title = element_blank(),
#                  plot.title = element_text(face = "bold", size = 9))
#   gg <- gg + scale_fill_gradient2(low = "khaki", mid = "cyan", high = "magenta", midpoint, na.value = "white")
# gg
# 
# gg <- ggplot(df_plot, aes())
#   df_plot <- covmat %>% tbl_df()
#   these_levels <- colnames(df_plot)
#   df_plot$ItemX <- colnames(df_plot)
#   gathercols <- colnames(df_plot)[-ncol(df_plot)]
#   df_plot <- df_plot %>% gather_("ItemY", "Value", gathercols)
#   df_plot$ItemX <- factor(df_plot$ItemX, levels = these_levels)
#   df_plot$ItemY <- factor(df_plot$ItemY, levels = these_levels)
# 
#   #midpoint <- min(covmat) + (max(covmat) - min(covmat)) / 2
#   midpoint <- 0
#   gg <- ggplot(df_plot, aes((ItemX), (ItemY)))
#   gg <- gg + geom_tile(aes(fill = Value))#, width = 4, height = 4)
#   gg <- gg + geom_text(aes(label = round(Value, 2)), size = 3)
#   if(!is.null(fig_title)){
#     gg <- gg + labs(title = fig_title)
#   }
#   gg <- gg + theme_bw()
#   gg <- gg + theme(axis.text.x = element_text(angle = 60, hjust = 1),
#                  axis.title = element_blank(),
#                  legend.title = element_blank(),
#                  plot.title = element_text(face = "bold", size = 9))
#   gg <- gg + scale_fill_gradient2(low = "khaki", mid = "cyan", high = "magenta", midpoint, na.value = "white")



#=========================================================================
rootfn_dimRet1 <- function(l_R, l_C, nab_decRet, covmat_inv, Rtarg){
  l_R <- lambdas_in[1]
  l_C <- lambdas_in[2]
  print(lambdas_in)
eig_values_sigs <- 1 / diag(covmat_inv)
wStar_out <- wFun(l_R, l_C, eig_values_sigs, nab_decRet)
C <- sum(wStar_out)
#-----
# targ_vec <- c(-log(Rtarg), C)
# wStar <- wStar_out
# #print(wStar)
# nab_C <- wStar^2
# mat_nab <- cbind(nab_decRet, nab_C)
# M <- t(mat_nab) %*% covmat_inv %*% mat_nab
# M_inv <- solve(M)
# lambdas_out <- 2 * M_inv %*% targ_vec
#-----
targ_vec_1 <- c(-log(Rtarg), 1)
print(wStar_out)
if(C > 1){
  wStar_1 <- wStar_out / C
}else{
    wStar_1 <- wStar_out
  }
nab_C_1 <- wStar_1^2
mat_nab <- cbind(nab_decRet, nab_C_1)
M_1 <- t(mat_nab) %*% covmat_inv %*% mat_nab
# #M / (diag(c(1, C^2)) %*% M_1 %*% diag(c(1, C^2)))
# #M_inv / solve(diag(c(1, C^2)) %*% M_1 %*% diag(c(1, C^2)))
M1_inv <- solve(M_1)
# # #M_inv / (diag(c(1, C^-2)) %*% M1_inv %*% diag(c(1, C^-2)))
lambdas_out <- 2 * M1_inv %*% targ_vec_1
#-----
slack_lambdas <- lambdas_in - lambdas_out
# print(slack_lambdas)
#print(lambdas_out)
return(slack_lambdas)
}
#=========================================================================
optimize_portfolio_dimRet1 <- function(nab_decRet, covmat, Rtarg, backtest_info = NULL){
#Rtarg <- 0.029 #.0292
covmat_inv <- solve(covmat)
cond <- 0
n_tries <- 0
while(cond == 0){
#  l_R <-  runif(1) * 10^-3 #-0.01465934
l_C <-  runif(1) * 10^-2 #0.09718086
eig_values_sigs <- 1 / diag(covmat_inv)
u <- eig_values_sigs
  lR3_lobound <- -27 * u^2 * l_C / nab_decRet^3
  lR_lobound <- max(sign(lR3_lobound) * abs(lR3_lobound)^(1 / 3))
l_R <- runif(1) * lR_lobound
lambdas_in <- c(l_R, l_C)

try_it <- try(nleqslv::nleqslv(lambdas_in, rootfn_dimRet1, jac = NULL, nab_decRet, covmat_inv, Rtarg, control = list(maxit = 2500, ftol = 10^-9, btol = 10^-4, xtol = 10^-10)))
  if(inherits(try_it, "try-error")){next()}
  nleqslv_out <- try_it
  if((mean(abs(nleqslv_out$fvec)) < 1) | (n_tries < 10)){
    if(nleqslv_out$message == "Function criterion near zero" |
   mean(abs(nleqslv_out$fvec)) < 10^-7 |
   n_tries > 5){
  cond <- 1
}
  }
  n_tries <- n_tries + 1
}

lambdas_out <- nleqslv_out$x
print(nleqslv_out$message)
print(nleqslv_out$fvec)

l_R <- lambdas_out[1]
l_C <- lambdas_out[2]

eig_values_sigs <- diag(1 / covmat_inv)
wStar_out <- wFun(l_R, l_C, eig_values_sigs, nab_decRet)
C <- sum(wStar_out)
C
sum(wFun(l_R * C, l_C * C^3, eig_values_sigs, nab_decRet))
#----
# check
targ_vec <- c(-log(Rtarg), C)
wStar <- wStar_out
#print(wStar)
nab_C <- wStar^2
mat_nab <- cbind(nab_decRet, nab_C)
M <- t(mat_nab) %*% covmat_inv %*% mat_nab
M_inv <- round(solve(M), 8)
lambdas_out - 2 * M_inv %*% targ_vec
#----
wStar_1 <- wStar_out / C
#print(wStar)
nab_C_1 <- wStar_1^2
mat_nab <- cbind(nab_decRet, nab_C_1)
M_1 <- t(mat_nab) %*% covmat_inv %*% mat_nab
M / (diag(c(1, C^2)) %*% M_1 %*% diag(c(1, C^2)))
M_inv / solve(diag(c(1, C^2)) %*% M_1 %*% diag(c(1, C^2)))
M1_inv <- round(solve(M_1), 8)
M_inv / (diag(c(1, C^-2)) %*% M1_inv %*% diag(c(1, C^-2)))
lambdas_out - 2 * diag(c(1, C^-2)) %*% M1_inv %*% diag(c(1, C^-2)) %*% targ_vec
#===============
#print(exp(-(M[1, 2]) / M[2, 2]))
#===============
#----
targ_vec_1 <- c(-C * log(Rtarg), 1)
lambdas_out - 2 * diag(c(C^-1, C^-3)) %*% M1_inv %*% targ_vec_1
lambdas_out_1 <- diag(c(C, C^3)) %*% lambdas_out
lambdas_out_1 - 2 * M1_inv %*% targ_vec_1
sum(wFun(lambdas_out_1[1], lambdas_out_1[2], eig_values_sigs, nab_decRet))
l_R_1 <- lambdas_out_1[1]
l_C_1 <- lambdas_out_1[2]
#--------------------------------------------------------------------
wStar_inv <- 1 / wStar
Rtarg_1 <- exp(-t(nab_decRet) %*% wStar_inv) # = Rtarg^C
Vtarg_1 <- t(wStar_inv) %*% covmat %*% wStar_inv
sd_targ_1 <- sqrt(Vtarg_1)
wStar_inv <- 1 / wStar_out
Rtarg <- exp(-t(nab_decRet) %*% wStar_inv)
Vtarg <- t(wStar_inv) %*% covmat %*% wStar_inv
sd_targ <- sqrt(Vtarg)
#--------------------------------------------------------------------
if(!is.null(backtest_info)){
  nab_decRet_test <- backtest_info[["nab_decRet_test"]]
  covmat_test <- backtest_info[["covmat_test"]]
  Rtest <- exp(-(t(nab_decRet_test) %*% wStar_inv))
  Vtest <- t(wStar_inv) %*% covmat_test %*% wStar_inv
  sd_test <- sqrt(Vtest)
}else{
  Rtest <- NA
  Vtest <- NA
  sd_test <- NA
}
#--------------------------------------------------------------------
  frontier_vec <- c(Rtarg, sd_targ, l_R, l_C, Rtest, sd_test)
  list_out <- list(wStar, frontier_vec)
  return(list_out)
}
#=========================================================================
  #Rtarg_limits <- c(0.01, 0.7)
get_optimal_frontier_dimRet1 <- function(nab_decRet, covmat, Rtarg_limits, n_points_on_frontier, backtest_info = NULL){
  Rtarg_vec <- seq(Rtarg_limits[1], Rtarg_limits[2], length.out = n_points_on_frontier)
  list_wStar <- list()
  list_frontier <- list()
  for(i in 1:n_points_on_frontier){
    Rtarg <- Rtarg_vec[i]
    list_out <- optimize_portfolio_dimRet1(nab_decRet, covmat, Rtarg, backtest_info)
    
    list_wStar[[i]] <- list_out[[1]]
    list_frontier[[i]] <- list_out[[2]]
    print(i)
  }
  #-------------------------------------------
  df_frontier <- data.frame(do.call(rbind, list_frontier))
  colnames(df_frontier) <- c("Return target",
                             "Risk (standard deviation)",
                             "Risk shadow price",
                             "Budget shadow price",
                             "Return backtest",
                             "Risk backtest")
  df_wStar <- data.frame(df_frontier$`Risk (standard deviation)`, t(do.call(cbind, list_wStar)))
  varNames_ordered <- names(nab_decRet)
  colnames(df_wStar) <- c("Risk (standard deviation)", varNames_ordered)
  
  df_frontier$`Tot. investment` <- rowSums(abs(df_wStar[, -1]))
  df_frontier$`ROI target` <- df_frontier$`Return target` / df_frontier$`Tot. investment`
  df_frontier$`ROI backtest` <- df_frontier$`Return backtest` / df_frontier$`Tot. investment`
  #--------------------------------------
  list_out <- list(df_wStar, df_frontier)#, R_at_minRisk)
  return(list_out)
  }

#=========================================================================
#=========================================================================
#=========================================================================
#=========================================================================
mat_P_sigs <- mat_P[, 1:n_signals]
mat_Sx_train <- mat_decDiff_eg_train %*% mat_P_sigs
#----
mat_P_sigs_test <- mat_P_test[, 1:n_signals]
mat_Sx_test <- mat_decDiff_eg_test %*% mat_P_sigs_test
eig_values_sigs_test <- eig_values_test[1:n_signals]
#----
nab_decRet_sigs_train <- (apply(mat_Sx_train, 2, function(x) prod(1 + x)) - 1)
names(nab_decRet_sigs_train) <- paste("Signal", 1:n_signals)
nab_decRet_sigs_test <- (apply(mat_Sx_test, 2, function(x) prod(1 + x)) - 1)
#----
nab_decRet_sigs_test <- nab_decRet_sigs_test[ind_rearrange]
#----
mat_U <- diag(eig_values_sigs)
mat_U_test <- diag(eig_values_sigs_test[ind_rearrange])
#=========================================================================
# dimRet:
Ctarg <- 1
covmat <- mat_U
covmat_test <- mat_U_test
nab_decRet <- nab_decRet_sigs_train
nab_decRet_test <- nab_decRet_sigs_test
backtest_info <- list()
backtest_info[["nab_decRet_test"]] <- nab_decRet_test
backtest_info[["covmat_test"]] <- covmat_test
Rtarg_limits <- c(0.0225, 0.035)
#-------------------------------------------------------------------------
list_out <- get_optimal_frontier_dimRet1(nab_decRet, covmat, Rtarg_limits, n_points_on_frontier, backtest_info)
df_wStar <- list_out[[1]]
df_frontier <- list_out[[2]]

#df_frontier <- subset(df_frontier, `Risk shadow price` < 0)
gg <- plot_frontier(df_frontier, ROI_basis = F, list_graph_options = NULL, graph_on = F)
gg_frontier <- gg



n_items <- ncol(df_wStar) - 1
bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_items)
color_vec_mv_eg <- sample(bag_of_colors, n_items)
gg_budget <- plot_budgetShares(df_wStar, color_vec = color_vec_mv_eg, graph_on = F, list_graph_options = NULL)
#--------------------------------------------------------------
gg_frontier + gg_budget + plot_layout(ncol = 1)
#--------------------------------------------------------------





#=========================================================================
#=========================================================================
# conventional:
# nab_C <- rep(1, n_signals)
# Ctarg <- 1
# covmat_train <- mat_U
# covmat_test <- mat_U_test
# #--------------------------------------------------------------
# mat_nab <- cbind(nab_decRet_sigs_train, nab_C)
# n_points_on_frontier <- 50
# R_range <- 0.25
# backtest_info <- list()
# backtest_info[["nab_decRet_test"]] <- nab_decRet_sigs_test
# backtest_info[["covmat_test"]] <- covmat_test
# #--------------------------------------------------------------
# fun_env_getOptFront <- list()
# fun_env_getOptFront[["n_points_on_frontier"]] <- n_points_on_frontier
# fun_env_getOptFront[["R_range"]] <- R_range
# fun_env_getOptFront[["backtest_info"]] <- backtest_info
# fun_env_getOptFront[["C_targ"]] <- C_targ
# fun_env_getOptFront[["dimRet"]] <- F
# #--------------------------------------------------------------
# list_out <- get_optimal_frontier(covmat_train, mat_nab,
#                                  fun_env = fun_env_getOptFront)
# df_wStar <- list_out[[1]]
# df_frontier <- list_out[[2]]
# #--------------------------------------------------------------
# gg_frontier <- plot_frontier(df_frontier, ROI_basis = F, list_graph_options = NULL, graph_on = F)
# #--------------------------------------------------------------
# # df_wStar_agg <- df_wStar %>% gather_("Item", "Budget Share", colnames(df_wStar)[-1])
# # df_wStar_agg <- merge(df_wStar_agg, df_match_group, by = "Item")
# # df_wStar_agg <- df_wStar_agg %>% group_by(`Risk (standard deviation)`, Group) %>% summarise(`Budget Share` = sum(`Budget Share`)) %>% spread(Group, `Budget Share`)
# #gg_budget <- plot_budgetShares(df_wStar_agg, group_small = NULL, color_vec = group_colors, graph_on = F, list_graph_options = NULL)
# n_items <- ncol(df_wStar) - 1
# bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_items)
# color_vec_mv_eg <- sample(bag_of_colors, n_items)
# gg_budget <- plot_budgetShares(df_wStar, color_vec = color_vec_mv_eg, graph_on = F, list_graph_options = NULL)
# #--------------------------------------------------------------
# gg_frontier + gg_budget + plot_layout(ncol = 1)
# #https://gotellilab.github.io/GotelliLabMeetingHacks/NickGotelli/ggplotPatchwork.html
# #--------------------------------------------------------------
# df_plot <- df_frontier
# df_plot_mod <- df_plot[, c("Risk (standard deviation)", "ROI target")]
# df_plot_btest <- df_plot[, c("Risk backtest", "ROI backtest")]
# df_plot_mod$Type <- "Optimal\nsolution"
# df_plot_btest$Type <- "Backtest"
# colnames(df_plot_mod) <- c("Risk", "ROI", "Type")
# colnames(df_plot_btest) <- c("Risk", "ROI", "Type")
# df_plot <- rbind(df_plot_mod, df_plot_btest)
# 
# gg <- ggplot(df_plot, aes(x = Risk, y = ROI, group = Type, color = Type))
# gg <- gg + geom_point()
# gg

```


```{r, fig.show='hold', fig.width=5, fig.height=5, fig.align='center', fig.cap="\\label{fig:wDisagg}The asset budget shares derived from the signals."}

# Map the signal weights back to the assets
mat_wStar <- t(as.matrix(df_wStar[, -1]))
#colSums(mat_L %*% diag(1 / colSums(mat_L)))
mat_wStar_assets <- mat_P_sigs %*% mat_wStar
mat_wStar_assets <- exp(mat_wStar_assets)
mat_wStar_assets <- apply(mat_wStar_assets, 2, function(x) x / sum(x))
df_wStar_assets <- data.frame(df_wStar$`Risk (standard deviation)`, t(mat_wStar_assets))
colnames(df_wStar_assets) <- c("Risk (standard deviation)", row.names(mat_Lrot))
gg_budget <- plot_budgetShares(df_wStar_assets, color_vec = color_vec_mv_eg, graph_on = F, list_graph_options = NULL)
gg_budget
#==========================================================



# 
covmat_XX_test <- cov(mat_pctDiff_eg_test)
Vtarg <- c()
ROItarg <- c()
Vtarg_test <- c()
ROItarg_test <- c()
#
for(i in 1:ncol(mat_wStar_assets)){
  wStar <- mat_wStar_assets[, i]
  Vtarg[i] <- t(wStar) %*% covmat_XX %*% wStar
  ROItarg[i] <- wStar %*% nab_pctRet_train
  Vtarg_test[i] <- t(wStar) %*% covmat_XX_test %*% wStar
  ROItarg_test[i] <- wStar %*% nab_pctRet_test
}
#

   df_frontier <- data.frame(sqrt(Vtarg), ROItarg, sqrt(Vtarg_test), ROItarg_test)
   colnames(df_frontier) <- c("Risk (standard deviation)", "ROI target", "Risk backtest", "ROI backtest")
  #plot_frontier(df_frontier, ROI_basis = T, graph_on = F)

df_plot_train <- df_frontier[, c("Risk (standard deviation)", "ROI target")]
df_plot_test <- df_frontier[, c("Risk backtest", "ROI backtest")]
df_plot_train$Type = "Efficient frontier"
df_plot_test$Type = "Backtest"
colnames(df_plot_train)[1:2] <- c("Risk (standard deviation)", "ROI")
colnames(df_plot_test)[1:2] <- c("Risk (standard deviation)", "ROI")
df_plot <- rbind(df_plot_train, df_plot_test)

gg <- ggplot(df_plot, aes(x = `Risk (standard deviation)`, y = ROI, group = Type, color = Type))
gg <- gg + geom_point()
gg <- gg + scale_color_manual(values = c("blue", "black"))
gg <- gg + labs(title = "Backtest")
gg <- gg + theme(axis.title.y = element_blank(),
                 legend.title = element_blank(),
                 legend.position = "bottom",
                 plot.title = element_text(face = "bold", size = 10))
gg_backtest <- gg
gg_backtest

  



```


Inaccuracy in the efficient frontier due to noisy covariance matrices severely limits the usefulness of MV Analysis in everyday practice [@Michaud...]. A backtest of the efficient frontiers from the example pursued in this paper reaffirms this (Figure \ref{fig:backtest}). It stands to reason that replacement of the data covariance matrix with a signals derived covariance matrix could improve the accuracy of the efficient frontier. But the signals derived covariance matrix contains eigenvalues equal to zero, and is thus is not invertible; and hence cannot be used in MV Analysis. Nonetheless, there is an alternative route to the same goal of reducing noise in MV Analysis: the portfolio of assets can be replaced with a portfolio of signals.

```{r, fig.show='hold', fig.width=7, fig.height=3, fig.align='center', fig.cap="\\label{fig:backtest}Backtest of the efficient frontier using both conventional and utility shares. In both cases, real return on investment over the subsequent period turns out to be much lower, and risk much higher, than the efficient frontier would suggest."}
# Back tests
mv_conv_U_compare_colors <- rev(mv_conv_U_compare_colors)
#--------------------------------------------------------
df_plot_conv_mod <- df_plot_conv[, c("Risk (standard deviation)", "ROI target")]
df_plot_conv_btest <- df_plot_conv[, c("Risk backtest", "ROI backtest")]
df_plot_conv_mod$Type <- "Optimal\nsolution"
df_plot_conv_btest$Type <- "Backtest"
df_plot_conv_btest$Returns <- "Linear"
colnames(df_plot_conv_mod) <- c("Risk", "ROI", "Type")
colnames(df_plot_conv_btest) <- c("Risk", "ROI", "Type", "Returns")
# #--------------------------------------------------------
# df_plot <- rbind(df_plot_conv_mod, df_plot_conv_btest)
# gg <- ggplot(df_plot, aes(x = Risk, y = ROI, group = Type, color = Type))
# gg <- gg + geom_point()
# gg <- gg + labs(title = "Conventional budget shares")
# gg <- gg + theme(legend.position = "none",
#                  plot.title = element_text(face = "bold", size = 10))
# gg_conv_backtest <- gg + scale_color_manual(values = mv_conv_U_compare_colors)
# #--------------------------------------------------------
df_plot_U_mod <- df_plot_U[, c("Risk (standard deviation)", "ROI target")]
df_plot_U_btest <- df_plot_U[, c("Risk backtest", "ROI backtest")]
df_plot_U_mod$Type <- "Optimal\nsolution"
df_plot_U_btest$Type <- "Backtest"
df_plot_U_btest$Returns <- "Sub-linear"
colnames(df_plot_U_mod) <- c("Risk", "ROI", "Type")
colnames(df_plot_U_btest) <- c("Risk", "ROI", "Type", "Returns")
#--------------------------------------------------------
# df_plot <- rbind(df_plot_U_mod, df_plot_U_btest)
# gg <- ggplot(df_plot, aes(x = Risk, y = ROI, group = Type, color = Type))
# gg <- gg + geom_point()
# gg <- gg + labs(title = "Utility budget shares")
# gg <- gg + theme(legend.title = element_blank(),
#                  axis.title.y = element_blank(),
#                  plot.title = element_text(face = "bold", size = 10))
# gg_U_backtest <- gg + scale_color_manual(values = mv_conv_U_compare_colors)
# #--------------------------------------------------------
# gg_conv_backtest + gg_U_backtest + plot_layout(ncol = 2)
#--------------------------------------------------------
df_plot <- rbind(df_plot_conv_btest, df_plot_U_btest)
gg <- ggplot(df_plot, aes(x = Risk, y = ROI, group = Returns, color = Returns))
gg <- gg + geom_point()
gg <- gg + theme(plot.title = element_text(face = "bold", size = 10))
gg <- gg + scale_color_manual(values = mv_conv_U_compare_colors)
gg

# gg <- ggplot()
# gg <- gg + geom_point(data = df_plot_conv, aes(x = `Risk backtest`, y = `Return on investment`))
# gg <- gg + geom_point(data = df_plot_U, aes(x = `Risk backtest`, y = `Return backtest`), color = "blue")
# gg
# 
# 
# 
# 
# gg_backTest_conv <- plot_frontier_wBacktest(df_frontier_conv, graph_on = T)
# gg_backTest_U <- plot_frontier_wBacktest(df_frontier_U, graph_on = T)
# gg_backTest_conv + gg_backTest_U + plot_layout(ncol = 2)


```

That is to say, instead of optimizing over a portfolio of $n$ assets, the problem is reduced to one of optimizing over just the $k$ retained signals. Instead of using the data covariance matrix in MV Analysis, the signals covariance matrix is used, which is just the first $k$ rows and columns of the diagonal eigenvalue matrix $\tilde{U}$ (recall equation \ref{eq:covmat_SS}). The vector of expected returns to signals $\boldsymbol{\mu}_S$ can be aggregated up from the asset expected returns based on the .
<!-- The expected return to each signal can be calculated as the sum of the expected returns to each asset weighted by their loadings onto the given signal.  -->

\begin{equation}
\tilde{\mu} = \frac{\mathbf{p}_T' \mathbf{e} - \mathbf{p}_1' \mathbf{e}}{\mathbf{p}_1' \mathbf{e}}
\end{equation}

To test this idea, the signals portfolio efficient frontier and budget shares are calculated using the financial data, and displayed in Figure \ref{fig:sigPortfolio}. The backtest on the right side of the Figure indicates that the signals portfolio frontier is far more accurate than that of the conventional approach in Figure \ref{fig:backtest}. The real ROI even exceeds the expected ROI over a substantial part of the frontier.

```{r, fig.show='hold', fig.width=6, fig.height=3, fig.align='center', fig.cap="\\label{fig:sigPortfolio}\\textit{(Left) }Optimal frontier and budget shares for the signals portfolio. \\textit{(Right) } Backtest of the frontier."}


mat_Lrot <- varimax(mat_L)[[1]]
mat_Lrot <- matrix(as.numeric(mat_Lrot),
                          attributes(mat_Lrot)$dim,
                          dimnames = attributes(mat_Lrot)$dimnames)
# mat_R <- varimax(mat_L)[[2]]
# mat_R <- matrix(as.numeric(mat_R),
#                           attributes(mat_R)$dim,
#                           dimnames = attributes(mat_R)$dimnames)


mat_X_in_test <- mat_pctDiff_eg_test * 10^-2
list_out <- get_S_and_corrXS(mat_X_in_test)
mat_S_test_all <- list_out[[1]]
cormat_XS_test <- list_out[[2]]
eig_values_test <- list_out[[3]]
mat_P_test <- list_out[[4]]
mat_S_test <- mat_S_test_all[, 1:n_signals]
mat_L_test <- cormat_XS_test[, 1:n_signals]
eig_values_sigs_test <- eig_values_sigs[1:n_signals]
# mat_L_test / L_test
mat_Lrot_test <- varimax(mat_L_test)[[1]]
mat_Lrot_test <- matrix(as.numeric(mat_Lrot_test),
                          attributes(mat_Lrot_test)$dim,
                          dimnames = attributes(mat_Lrot_test)$dimnames)

mat_P_sigs <- mat_P[, 1:n_signals]
mat_P_sigs_test <- mat_P_test[, 1:n_signals]

# eig_values_test / eigen(cov(mat_X_in))$values
#cov(mat_S_test)
#--------------------------------------------------------------
# mat_Lrot <- varimax(cormat_XS)[[1]]
# mat_Lrot <- matrix(as.numeric(mat_Lrot),
#                           attributes(mat_Lrot)$dim,
#                           dimnames = attributes(mat_Lrot)$dimnames)
# mat_Lrot <- mat_Lrot[, 1:n_signals]
# mat_R <- varimax(mat_L)[[2]]
# mat_R <- matrix(as.numeric(mat_R),
#                           attributes(mat_R)$dim,
#                           dimnames = attributes(mat_R)$dimnames)
# mat_R <- mat_R[1:n_signals, 1:n_signals]
# mat_Lrot_test <- varimax(cormat_XS_test)[[1]]
# mat_Lrot_test <- matrix(as.numeric(mat_Lrot_test),
#                           attributes(mat_Lrot_test)$dim,
#                           dimnames = attributes(mat_Lrot_test)$dimnames)
# mat_Lrot_test <- mat_Lrot_test[, 1:n_signals]
# mat_R_test <- varimax(mat_L_test)[[2]]
# mat_R_test <- matrix(as.numeric(mat_R_test),
#                           attributes(mat_R_test)$dim,
#                           dimnames = attributes(mat_R_test)$dimnames)
# mat_R_test <- mat_R_test[1:n_signals, 1:n_signals]
# xAxis_title <- "Varimax Rotated Correlation Test"
# plot_corrXS_barchart(mat_Lrot, xAxis_title)
#--------------------------------------------------------------
nab_C <- rep(-1, n_signals)
utility_interpretation <- F
#--------------------------------------------------------------
# Covariance matrix
# covmat_train <- U_train[1:n_signals, 1:n_signals]
# covmat_test <- U_test[1:n_signals, 1:n_signals]
#round(cov(mat_S), 3) / U_train[1:n_signals, 1:n_signals]

covmat_train <- cov(mat_S)
covmat_test <- cov(mat_S_test)

# covmat_train <- t(mat_R) %*% diag(eig_values[1:n_signals]) %*% mat_R
# covmat_test <- t(mat_R_test) %*% diag(eig_values_test[1:n_signals]) %*% mat_R_test

# diag(covmat_train) / eig_values[1:n_signals]
# diag(covmat_test) / eig_values_test[1:n_signals]
#--------------------------------------------------------------
# Expected returns vector
# nab_pctRet_sigs_train <- (mat_S[nrow(mat_S),] - mat_S[1, ]) / mat_S[1, ]
# nab_pctRet_sigs_test <- (mat_S[nrow(mat_S_test),] - mat_S_test[1, ]) / mat_S_test[1, ]
nab_pctRet_sigs_train <- as.numeric((t(mat_ts_eg_train[nrow(mat_ts_eg_train), ]) %*% mat_P_sigs - t(mat_ts_eg_train[1, ]) %*% mat_P_sigs) / t(mat_ts_eg_train[1, ]) %*% mat_P_sigs)
nab_pctRet_sigs_test <- as.numeric((t(mat_ts_eg_test[nrow(mat_ts_eg_test), ]) %*% mat_P_sigs_test - t(mat_ts_eg_test[1, ]) %*% mat_P_sigs_test) / t(mat_ts_eg_test[1, ]) %*% mat_P_sigs_test)

# nab_pctRet_sigs_train <- apply(10^-2 * mat_S, 2, function(x) prod(1 + x)) - 1
# nab_pctRet_sigs_test <- apply(10^-2 * mat_S_test, 2, function(x) prod(1 + x)) - 1
#---
# nab_pctRet_sigs_train <- as.numeric(t(nab_pctRet_train) %*% diag(1 / rowSums(L_train)) %*% L_train)
# nab_pctRet_sigs_test <- as.numeric(t(nab_pctRet_test) %*% diag(1 / rowSums(L_test)) %*% L_test)
# nab_pctRet_sigs_train <- as.numeric(t(nab_pctRet_train) %*% diag(1 / rowSums(mat_Lrot)) %*% mat_Lrot)
# nab_pctRet_sigs_test <- as.numeric(t(nab_pctRet_test) %*% diag(1 / rowSums(mat_Lrot_test)) %*% mat_Lrot_test)
#nab_pctRet_sig_train <- runif(n_signals) * 10^-1
names(nab_pctRet_sigs_train) <- paste("Signal", 1:length(nab_pctRet_sigs_train))
names(nab_pctRet_sigs_test) <- paste("Signal", 1:length(nab_pctRet_sigs_test))
#--------------------------------------------------------------
mat_nab <- cbind(nab_pctRet_sigs_train, nab_C)
n_points_on_frontier <- 50
Rtarg_limits <- c(0.1, 0.15)
backtest_info <- list()
backtest_info[["nab_pctRet_test"]] <- nab_pctRet_sigs_test
backtest_info[["covmat_test"]] <- covmat_test
#--------------------------------------------------------------
fun_env_getOptFront <- list()
fun_env_getOptFront[["n_points_on_frontier"]] <- n_points_on_frontier
fun_env_getOptFront[["Rtarg_limits"]] <- Rtarg_limits
fun_env_getOptFront[["utility_interpretation"]] <- utility_interpretation
fun_env_getOptFront[["backtest_info"]] <- backtest_info
fun_env_getOptFront[["C_targ"]] <- C_targ
#--------------------------------------------------------------
list_out <- get_optimal_frontier(covmat_train, mat_nab,
                                 fun_env = fun_env_getOptFront)
df_wStar <- list_out[[1]]
df_frontier <- list_out[[2]]
#--------------------------------------------------------------
list_graph_options <- list()
list_graph_options[["fig_title"]] <- "Efficient frontier and budget shares"
gg_frontier <- plot_frontier(df_frontier, ROI_basis = T, list_graph_options, graph_on = F)
#--------------------------------------------------------------
n_items <- ncol(df_wStar) - 1
bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_items)
color_vec_mv_sigs_eg <- sample(bag_of_colors, n_items)
gg_budget <- plot_budgetShares(df_wStar, color_vec = color_vec_mv_sigs_eg, graph_on = F, list_graph_options = NULL)
#--------------------------------------------------------------
df_plot_train <- df_frontier[, c("Risk (standard deviation)", "ROI target")]
df_plot_test <- df_frontier[, c("Risk backtest", "ROI backtest")]
df_plot_train$Type = "Efficient frontier"
df_plot_test$Type = "Backtest"
colnames(df_plot_train)[1:2] <- c("Risk (standard deviation)", "ROI")
colnames(df_plot_test)[1:2] <- c("Risk (standard deviation)", "ROI")
df_plot <- rbind(df_plot_train, df_plot_test)

gg <- ggplot(df_plot, aes(x = `Risk (standard deviation)`, y = ROI, group = Type, color = Type))
gg <- gg + geom_point()
gg <- gg + scale_color_manual(values = c("blue", "black"))
gg <- gg + labs(title = "Backtest")
gg <- gg + theme(axis.title.y = element_blank(),
                 legend.title = element_blank(),
                 legend.position = "bottom",
                 plot.title = element_text(face = "bold", size = 10))
gg_backtest <- gg
#--------------------------------------------------------------
(gg_frontier + gg_budget + plot_layout(ncol = 1)) | gg_backtest
#--------------------------------------------------------------


# mat_U_inv <- diag(1 / eig_values_sigs)
# M <- t(mat_nab) %*% mat_U_inv %*% mat_nab
# M
# 
# (mat_nab[, 1] %*% mat_U_inv) %*% mat_nab[,1]
# (mat_nab[, 2] %*% mat_U_inv) %*% mat_nab[,2]
# (mat_nab[, 1] %*% mat_U_inv) %*% mat_nab[,2]
# 
# kappa(M)



```

MV Analysis then outputs the optimal budget shares to be allocated to each signal ($\mathbf{\tilde{w}}^*$), rather than to each asset. The disaggregation of these signal budget shares down to allocations to each individual portfolio item ($\mathbf{w}^*$) follows from the definition of signals in equation \ref{eq:sigs_def}.

\begin{equation}
\mathbf{w}^* = P\mathbf{\tilde{w}}^*
\label{eq:w_disagg}
\end{equation}




# An illustrative example of AR4D risk adjusted portfolio optimization

In practice, AR4D risk adjusted portfolio optimization would come after Step 3 in the resource allocation workflow, either complementing or replacing the ranking of research proposals in order of priority that is slated to occur in Step 4. By this point in the resource allocation workflow, a list of research proposals would have been drawn up, and the expected impact of each proposal assessed and quantified. A hypothetical list of such proposals is presented in Figure \ref{fig:ExpPctRet_Examp}.^[The AR4D proposals are loosely grouped into four categories to facilitate interpretation of the gaphics, but there is no strict rule followed, and clearly some overlap, in the grouping.]
<!-- This quantification of net impacts is calculated over the dimensions of interest to the stakeholders. For example, if the ... then the net impact is an aggregation of the net benefits in each of these categories. -->

```{r, fig.show = "hold", fig.width = 5, fig.height = 3, fig.align = "left", fig.cap = "\\label{fig:ExpPctRet_Examp}A set of hypothetical AR4D proposals and their expected returns."}
#---------------------------------------------
prop_CSA <- c("Heat Tolerant\nBeans", "Coffee Agroforestry", "Digital Agriculture", "Low Emission\nSilvo-Pastoril")
econGrowth_CSA <- c(0.17, 0.28, 0.4, -0.35)
econEquality_CSA <- c(0.85, 0.32, 0.81, 0.27)
envSust_CSA <- c(0.65, 0.42, 0.5, 0.8)
#---------------------------------------------
# prop_socialCap <- c("Micro Finance\nand Insurance", "Multi-stakeholder\nPlatforms", "Market Access")
# econGrowth_socialCap <- c(0.2, 0.45, 0.34)
# econEquality_socialCap <- c(0.87, 0.62, 0.79)
# envSust_socialCap <- c(0.1, 0.1, 0)
#---------------------------------------------
prop_smallHolder <- c("Cassava for\nBio-ethanol", "Triple Purpose\nSweet Potato", "Dairy Cooperative", "Multi-stakeholder\nPlatforms")
econGrowth_smallHolder <- c(0.67, -0.31, 0.15, 0.4)
econEquality_smallHolder <- c(0.6, 0.75, 0.83, 0.81)
envSust_smallHolder <- c(0.32, 0.4, -0.52, 0.1)
#---------------------------------------------
prop_highYcommod <- c("Mega Maize", "Hyper Rice", "Ultra Cow")#, "Uber Palm")
econGrowth_highYcommod <- c(0.82, 0.86, 0.76)#, 0.88)
econEquality_highYcommod <- c(-0.34, -0.42, -0.63)#, -0.58)
envSust_highYcommod <- c(-0.6, -0.52, -0.67)#, -0.83)
#---------------------------------------------
df_CSA <- data.frame(Proposal = prop_CSA, `Economic Growth` = econGrowth_CSA, `Economic Equality` = econEquality_CSA, `Environmental Sustainability` = envSust_CSA, Group = "Climate Smart\nAgriculture")

# df_socialCap <- data.frame(Proposal = prop_socialCap, `Economic Growth` = econGrowth_socialCap, `Economic Equality` = econEquality_socialCap, `Environmental Sustainability` = envSust_socialCap, Group = "Social Capital")

df_smallHolder <- data.frame(Proposal = prop_smallHolder, `Economic Growth` = econGrowth_smallHolder, `Economic Equality` = econEquality_smallHolder, `Environmental Sustainability` = envSust_smallHolder, Group = "Smallholder\nResilience")

df_highYcommod <- data.frame(Proposal = prop_highYcommod, `Economic Growth` = econGrowth_highYcommod, `Economic Equality` = econEquality_highYcommod, `Environmental Sustainability` = envSust_highYcommod, Group = "High Value Yield\nEnhancement")

list_df <- list(df_highYcommod, df_smallHolder, df_CSA)#, df_socialCap)

df_Lrot <- as.data.frame(do.call(rbind, list_df))
colnames(df_Lrot)[2:4] <- gsub("\\.", " ", colnames(df_Lrot)[2:4])
group_names <- as.character(unique(df_Lrot$Group))
n_groups <- length(group_names)
bag_of_colors <- randomcoloR::distinctColorPalette(k = 2 * n_groups)
group_colors_arb <- sample(bag_of_colors, n_groups)

# Randomly assign an expected pct. return to each AR4D proposal
n_prop <- nrow(df_Lrot)
n_signals <- ncol(df_Lrot) - 2
# nab_pctRet_ar4d <- exp(rnorm(n_prop))
# nab_pctRet_ar4d <- round(nab_pctRet_ar4d / sum(nab_pctRet_ar4d), 3) * 100
nab_pctRet_ar4d <- runif(n_prop)
nab_pctRet_ar4d <- round(nab_pctRet_ar4d / sum(nab_pctRet_ar4d), 3) * 100
df_pctRet <- data.frame(nab_pctRet_ar4d, Item = df_Lrot$Proposal, Group = df_Lrot$Group)
names(nab_pctRet_ar4d) <- df_Lrot$Proposal
# Plot expected returns for each AR4D proposal
group_colors <- group_colors_arb
#plot_returns_barchart(df_pctRet, group_colors)
list_graph_options <- list()
list_graph_options[["fig_title"]] <- NULL
list_graph_options[["ylab"]] <- "Percent"
list_graph_options[["legend_position"]] <- NULL
plot_returns_barchart(df_pctRet, group_colors, list_graph_options, graph_on = F)

```

The data required to calculate an AR4D proposal covariance matrix does not generally exist. However, the diagonal elements of such a matrix, i.e. the variance of each proposal, can be elicited from stakeholders by asking, for example, what they think the maximum, minimum, and most probable impact of each given proposal will be. The proposal impact variance can then be computed on the basis of a triangular distribution as


\begin{equation}
\sigma^2 = \frac{y_{low}^2 + y_{high}^2 + y_{likely}^2 - y_{low}y_{high} - y_{low}y_{likely}-y_{high}y_{likely}}{18}
\end{equation}

[triangular density graphic?]
If, for whatever reason, it is not possible to estimate these variances, then they can be scaled to unity. In the hypothetical resource allocation exercise at hand, the standard deviations were elicited from stakeholders, and appear along the diagonal of the AR4D proposal covariance matrix in Figure .... No estimate is yet available for the covariances, and so these are left blank for now.

```{r, fig.show='hold', fig.width=6, fig.height=4, fig.align='center', fig.cap="\\label{fig:covMat_ar4d}The AR4D proposal covariance matrix, with proposal risk assessments (along the diagonal) elicited from stakeholders. The off-diagonal elements, i.e. the covariances between proposals, remain unknown."}

fig_title <- "Covariances Unknown"
D_sd2X <- matrix(NA, n_prop, n_prop)
row.names(D_sd2X) <- as.character(df_pctRet$Item)
colnames(D_sd2X) <- as.character(df_pctRet$Item)
sd2X_vec <- 5 * runif(n_prop)
diag(D_sd2X) <- sd2X_vec
covmat_XX <- D_sd2X
plot_covmat(covmat_XX, fig_title, graph_on = F)


```

Usually, signals are deduced from data, or from the data covariance matrix. If no data exists, then the analysis is abandoned. However, equation ... effectively means that, in the absence of data, the data covariance matrix can be "reverse engineered" from a (rotated) loadings matrix deduced from domain knowledge. The "reverse engineered" matrix is an approximation of the data covariance matrix, but is not necessarily less accurate than the data covariance matrix. 

In the present hypothetical example, a "signal" can be thought of as a development process that the stakeholders have at the forefront of their minds. Signals can also be thought of as the key dimensions that best describe the evolution of the proposed AR4D activities and outcomes, within a particular problem space that is of interest to the stakeholders. These key dimensions can be elicited by asking stakeholders questions such as "What are our key AR4D objectives? And what are the key metrics by which to measure progress towards those objectives?" In the AR4D context, there is often pre-existing consensus on such questions, which can be found laid out in strategy documents. The Sustainable Development Goals may be considered a set of dimensions defining a very broad problem space. The key dimensions of a particular AR4D portfolio space could be selected as a subset of these. The number of portfolio dimensions must be less than the number of portfolio items.

In the hypothetical example presented here, the stakeholders have decided that Economic Growth, Income Equality, and Environmental Sustainability are the dimensions that best register the evolution of the proposals under consideration. This loosely corresponds to the "triple bottom line" discussed in business contexts. The next step is to invite the stakeholders to rate, from $-100$ to $100$, each research proposal in terms of its contribution towards each of these objectives. A positive rating means the proposal contributes toward the objective; a negative rating means the proposal works against the objective; and a rating of zero means that the proposal has no influence over the given objective. The hypothetical results of such an exercise are presented in Figure \ref{fig:loadsRotExamp}. These ratings are then divided by $100$, and the matrix as a whole is interpreted as an orthogonal rotation of the matrix of correlations between AR4D proposals and signals ($L_{\circlearrowleft}$).

```{r, fig.show = "hold", fig.width = 6, fig.height = 3, fig.align = "center", fig.cap = "\\label{fig:loadsRotExamp}A hypothetical example of rotated signal loadings elicited from a stakeholder survey."}
df_x <- df_Lrot
df_x$Group <- NULL
prop_col <- df_x$Proposal
df_x$Proposal <- NULL
mat_Lrot <- as.matrix(df_x)
rownames(mat_Lrot) <- prop_col
list_groups <- list(prop_highYcommod, prop_smallHolder, prop_CSA)
# n_groups <- length(list_groups)
# bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_groups)
# group_colors <- sample(bag_of_colors, n_groups)
group_info <- list(list_groups, group_names, group_colors)

sigNames <- c("GDP Growth", "Economic\nEquality", "Environmental\nSustainability")
xAxis_title <- "Rotated Correlations"
plot_corrXS_barchart(mat_Lrot, group_info, xAxis_title, sigNames)

```



<!-- This would be the case, for example, of a yield enhancing variety that requires increased use of chemical inputs that degrade soils, pollute water sources, and pose health risks. Such a technology might contribute toward the economic growth objective, but works against the environmental sustainability objective. Likewise, it is customary in AR4D communities to assume that a tradeoff exists between the objectives of economic growth and economic equality [@Alston].^{Recent empirical studies have cast doubt on this idea [@;@;@].} Such tradeoffs are inherent in any research proposal. It is critical that stakeholders acknowledge them.-->


<!-- #```{r} -->
<!-- #D_sdX <- diag(apply(mat_pctDiff_train_mv, 2, sd)) -->
<!-- # n_proj <- 11 -->
<!-- # n_sig <- 4 -->
<!-- # D_sdX <- diag(runif(n_proj)) #stand devs of projects, elicited -->
<!-- # mat_Lrot_eg <- matrix(rnorm(66), n_proj, n_sig) -->
<!-- # mat_Lrot_eg[which(mat_Lrot_eg > 1)] <- 0.98 -->
<!-- # mat_Lrot_eg[which(mat_Lrot_eg < -1)] <- -0.98 #rotated loadings (X<->S correlations), elicited -->
<!-- # #Now derive U, R, P -->
<!-- # Q <- D_sdX %*% mat_Lrot_eg -->
<!-- # R <- t(eigen(t(Q) %*% Q)$vectors) #orthog rotation matrix -->
<!-- # eigvals <- eigen(t(Q) %*% Q)$values #signal variances -->
<!-- # P_dervd <- D_sdX %*% mat_Lrot_eg %*% t(R) %*% diag(1 / eigvals^(1 / 2)) #eigenvectors of covmat of X -->
<!-- # round(t(P_dervd) %*% P_dervd, 3) #check orthogonality -->
<!-- #``` -->

As shown in section 3.2, the implicit covariance matrix ($\hat{\Sigma}$) can then be found by multiplying $L_{\circlearrowleft}L_{\circlearrowleft}'$. The result is presented in Figure \ref{fig:covmatProps}. This crowdsourced approximation to a data covariance matrix offers insight into the implicit covariances---i.e. tradeoffs and synergies---between AR4D proposals in the portfolio. This is remarkable in and of itself and may be useful for orienting stakeholder discussion. For risk adjusted portfolio optimization, we must turn to the signals portfolio.

```{r, fig.show = "hold", fig.width = 6, fig.height = 4, fig.align = "center", fig.cap = "\\label{fig:covmatProps}AR4D proposal covariance matrix derived from the elicited loadings."}

covmat_XX_derived <- mat_Lrot %*% t(mat_Lrot)
fig_title <- "Crowdsourced AR4D Proposal Covariance Matrix"
plot_covmat(covmat_XX_derived, fig_title, graph_on = F)



```

Nonetheless, as discussed in section 3.3, optimal frontier and budget shares can be obtained via another route. The signals portfolio is optimized. Individual AR4D proposal budget shares can then be disaggregated from these    To determine the optimal resource allocation, the signals portfolio is optimized.

In order to optimize the signals portfolio, the signal expected returns and covariance matrix are required. Signal expected returns can be aggregated up from the expected returns to each portfolio item via equation \ref{eq:}. The signals covariance matrix is just the diagonal matrix of the `r n_signals` leading eigenvalues ($U$) corresponding to the `r n_signals` retained signals (recall equation \ref{eq:}). The matrix $U$ can be derived from the elicited loadings $L_{\circlearrowleft}$ and project risk $\boldsymbol{\sigma}$ as follows. First, define a new matrix $Q$.

\begin{equation}
Q = D(\boldsymbol{\sigma}_X) L_{\circlearrowleft}
\end{equation}

By equation \ref{eq:} it follows that 

\begin{equation}
Q = P U^{1 \over 2} R
\end{equation}

and hence

\begin{eqnarray}
Q'Q &=& L_{\circlearrowleft}' D(\boldsymbol{\sigma}^2_X) L_{\circlearrowleft} \\
&=& R'U R
\end{eqnarray}

where $R$ is the orthogonal rotation matrix. It follows, then, that $U$ (and $R$) can be obtained through the eigendecomposition of $Q'Q$.

Having thus deduced the signals covariance matrix $U$ and vector of expected returns $\boldsymbol{\mu}$ from the elicited information $L_{\circlearrowleft}$ and $\boldsymbol{\sigma}$, the optimal frontier and budget shares for the signals portfolio are calculated in Figure \ref{fig:mvAR4D}.

```{r, fig.show='hold', fig.width=5, fig.height=5, fig.align='center', fig.cap="\\label{fig:mvAR4D}Optimal frontier and budget shares for the AR4D signals portfolio.", echo = FALSE}
#--------------------------------------------------------------
# Covariance matrix
D_sdX_inv <- diag(10^2 / sqrt(sd2X_vec))
mat_Q <- D_sdX_inv %*% mat_Lrot
eig_decomp_QQ <- eigen(t(mat_Q) %*% mat_Q)
eig_values_QQ <- eig_decomp_QQ$values
eig_values_QQ <- 10^-6 * eig_values_QQ
mat_U <- diag(eig_values_QQ)
covmat_SS <- mat_U
#--------------------------------------------------------
# Get mat_P
mat_U_sqrt_inv <- diag(1 / sqrt(eig_values_QQ))
mat_R <- t(eig_decomp_QQ$vectors) * 10^3
mat_P <- mat_Q %*% t(mat_R) %*% mat_U_sqrt_inv
if(mean(mat_P[, 1]) < 0){mat_P <- -mat_P}
mat_P_sigs <- mat_P[, 1:n_signals]
#--------------------------------------------------------------
nab_C <- rep(1, n_signals)
#--------------------------------------------------------------
# Expected returns vector
#nab_pctRet_ar4d_sigs <- as.numeric(t(10^-2 * nab_pctRet_ar4d) %*% mat_P_sigs)
nab_pctRet_ar4d_sigs <- as.numeric(t(10^-2 * nab_pctRet_ar4d) %*% mat_Lrot)
names(nab_pctRet_ar4d_sigs) <- sigNames
#--------------------------------------------------------------
mat_nab <- cbind(nab_pctRet_ar4d_sigs, nab_C)
n_points_on_frontier <- 20
Rtarg_limits <- c(0.11, 0.19)
backtest_info <- NULL
#--------------------------------------------------------------
covmat <- covmat_SS
#covmat_test <- mat_U_test
nab_decRet <- nab_pctRet_ar4d_sigs
#nab_decRet_test <- nab_decRet_sigs_test
backtest_info <- NULL
Rtarg_limits <- c(0.182, 0.2)
#-------------------------------------------------------------------------
list_out <- get_optimal_frontier_dimRet1(nab_decRet, covmat, Rtarg_limits, n_points_on_frontier, backtest_info)
df_wStar <- list_out[[1]]
df_frontier <- list_out[[2]]

#df_frontier <- subset(df_frontier, `Risk shadow price` < 0)
gg <- plot_frontier(df_frontier, ROI_basis = F, list_graph_options = NULL, graph_on = F)
gg_frontier <- gg



n_items <- ncol(df_wStar) - 1
bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_items)
color_vec_mv_eg <- sample(bag_of_colors, n_items)
gg_budget <- plot_budgetShares(df_wStar, color_vec = color_vec_mv_eg, graph_on = F, list_graph_options = NULL)
#--------------------------------------------------------------
gg_frontier + gg_budget + plot_layout(ncol = 1)
#--------------------------------------------------------------



















fun_env_getOptFront <- list()
fun_env_getOptFront[["n_points_on_frontier"]] <- n_points_on_frontier
fun_env_getOptFront[["Rtarg_limits"]] <- Rtarg_limits
fun_env_getOptFront[["utility_interpretation"]] <- utility_interpretation
fun_env_getOptFront[["backtest_info"]] <- backtest_info
fun_env_getOptFront[["C_targ"]] <- C_targ
#--------------------------------------------------------------
list_out <- get_optimal_frontier(covmat_SS, mat_nab,
                                 fun_env = fun_env_getOptFront)
df_wStar <- list_out[[1]]
df_frontier <- list_out[[2]]
#--------------------------------------------------------------
list_graph_options <- list()
list_graph_options[["fig_title"]] <- "Efficient frontier and budget shares"
gg_frontier <- plot_frontier(df_frontier, ROI_basis = T, list_graph_options, graph_on = F)
#--------------------------------------------------------------
n_items <- ncol(df_wStar) - 1
bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_items)
color_vec_mv_sigs_eg <- sample(bag_of_colors, n_items)
gg_budget <- plot_budgetShares(df_wStar, color_vec = color_vec_mv_sigs_eg, graph_on = F, list_graph_options = NULL)
#--------------------------------------------------------------
df_plot_train <- df_frontier[, c("Risk (standard deviation)", "ROI target")]
df_plot_test <- df_frontier[, c("Risk backtest", "ROI backtest")]
df_plot_train$Type = "Efficient frontier"
df_plot_test$Type = "Backtest"
colnames(df_plot_train)[1:2] <- c("Risk (standard deviation)", "ROI")
colnames(df_plot_test)[1:2] <- c("Risk (standard deviation)", "ROI")
df_plot <- rbind(df_plot_train, df_plot_test)

gg <- ggplot(df_plot, aes(x = `Risk (standard deviation)`, y = ROI, group = Type, color = Type))
gg <- gg + geom_point()
gg <- gg + scale_color_manual(values = c("blue", "black"))
gg <- gg + labs(title = "Backtest")
gg <- gg + theme(axis.title.y = element_blank(),
                 legend.title = element_blank(),
                 legend.position = "bottom",
                 plot.title = element_text(face = "bold", size = 10))
gg_backtest <- gg
#--------------------------------------------------------------
gg_frontier + gg_budget + plot_layout(ncol = 1)
#--------------------------------------------------------------
```

The budget allocation to each AR4D proposal is then disaggregated from the signal budget shares by equation \ref{eq:}. The matrix of `r n_signals` leading eigenvectors $P$ of the proposal covariance matrix is required in order to perform this last step. Having deduced $R$ and $U$ above, it is possible to deduce $P$ as follows.

\begin{eqnarray}
Q R' U^{-{1 \over 2}} &=& PU^{1 \over 2}RR'U^{-{1 \over 2}} \\ &=& P
\end{eqnarray}

Having deduced $P$, the optimal AR4D proposal budget allocation is calculated in Figure \ref{fig:wDisagg_ar4d}.

```{r, fig.show='hold', fig.width=5, fig.height=5, fig.align='center', fig.cap="\\label{fig:wDisagg_ar4d}AR4D proposal budget allocation derived from the signal budget shares."}
#--------------------------------------------------------
# Disaggregate budget shares to portfolio items
mat_wStar <- t(as.matrix(df_wStar[, -1]))
#colSums(mat_L %*% diag(1 / colSums(mat_L)))
mat_wStar_assets <- mat_P_sigs %*% mat_wStar
mat_wStar_assets <- exp(mat_wStar_assets)
mat_wStar_assets <- apply(mat_wStar_assets, 2, function(x) x / sum(x))
df_wStar_assets <- data.frame(df_wStar$`Risk (standard deviation)`, t(mat_wStar_assets))
colnames(df_wStar_assets) <- c("Risk (standard deviation)", row.names(mat_Lrot))
gg_budget <- plot_budgetShares(df_wStar_assets, graph_on = F, list_graph_options = NULL)
gg_budget

```























































The dimension covariance and correlation matrices are given in Figure \ref{fig:covmatDim}.

<!-- #```{r, fig.show = "hold", fig.width = 6, fig.height = 4, fig.align = "center", fig.cap = "\\label{fig:covmatDim}Implicit dimensional covariance and correlation matrices."} -->

<!-- covmat <- t(mat_Lrot) %*% mat_Lrot -->
<!-- sd_vec <- diag(sqrt(covmat)) -->
<!-- cormat <- diag(1 / sd_vec) %*% covmat %*% diag(1 / sd_vec) -->
<!-- rownames(cormat) <- rownames(covmat) -->
<!-- colnames(cormat) <- colnames(covmat) -->


<!-- n_col <- ncol(covmat) -->
<!-- df_plot <- covmat %>% tbl_df() -->
<!-- these_levels <- colnames(df_plot) -->
<!-- df_plot$V1 <- colnames(df_plot) -->
<!-- gathercols <- colnames(df_plot)[-ncol(df_plot)] -->
<!-- df_plot <- df_plot %>% gather_("V2", "Value", gathercols) -->

<!-- df_plot$V1 <- factor(df_plot$V1, levels = these_levels) -->
<!-- df_plot$V2 <- factor(df_plot$V2, levels = these_levels) -->

<!-- #midpoint <- min(covmat) + (max(covmat) - min(covmat)) / 2 -->
<!-- midpoint <- 0 -->
<!-- gg <- ggplot(df_plot, aes(V1, V2)) -->
<!-- gg <- gg + geom_tile(aes(fill = Value))#, width = 4, height = 4) -->
<!-- gg <- gg + geom_text(aes(label = round(Value, 2)), size = 3) -->
<!-- gg <- gg + theme(axis.text.x = element_text(angle = 60, hjust = 1), -->
<!--                  axis.title = element_blank(), -->
<!--                  legend.title = element_blank(), -->
<!--                  legend.position = "top", -->
<!--                  plot.title = element_text(size = 10)) -->
<!-- gg <- gg + labs(title = "Covariance Matrix") -->
<!-- gg <- gg + scale_fill_gradient2(low = "magenta", mid = "khaki", high = "cyan", midpoint) -->
<!-- gg_covmat <- gg -->


<!-- df_plot <- cormat %>% tbl_df() -->
<!-- these_levels <- colnames(df_plot) -->
<!-- df_plot$V1 <- colnames(df_plot) -->
<!-- gathercols <- colnames(df_plot)[-ncol(df_plot)] -->
<!-- df_plot <- df_plot %>% gather_("V2", "Value", gathercols) -->
<!-- df_plot$V1 <- factor(df_plot$V1, levels = these_levels) -->
<!-- df_plot$V2 <- factor(df_plot$V2, levels = these_levels) -->

<!-- #midpoint <- min(covmat) + (max(covmat) - min(covmat)) / 2 -->
<!-- midpoint <- 0 -->
<!-- gg <- ggplot(df_plot, aes(V1, V2)) -->
<!-- gg <- gg + geom_tile(aes(fill = Value))#, width = 4, height = 4) -->
<!-- gg <- gg + geom_text(aes(label = round(Value, 2)), size = 3) -->
<!-- gg <- gg + theme(axis.text.x = element_text(angle = 60, hjust = 1), -->
<!--                  axis.title = element_blank(), -->
<!--                  axis.text.y = element_blank(), -->
<!--                  legend.title = element_blank(), -->
<!--                  legend.position = "top", -->
<!--                  plot.title = element_text(size = 10)) -->
<!-- gg <- gg + labs(title = "Correlation Matrix") -->
<!-- gg <- gg + scale_fill_gradient2(low = "magenta", mid = "khaki", high = "cyan", midpoint) -->
<!-- gg_cormat <- gg -->


<!-- gg_covmat | gg_cormat -->

<!-- #kappa(covmat) -->
<!-- #eigen(covmat)$values -->
<!-- #eigen(covmat)$vectors -->

<!-- # R <- eigen(covmat)$vectors -->
<!-- # mat_L <- mat_Lrot %*% t(R) -->
<!-- #  -->
<!-- # mat_Lrot <- varimax(mat_L)[[1]] -->
<!-- # mat_Lrot <- matrix(as.numeric(mat_Lrot), attributes(mat_Lrot)$dim, dimnames=attributes(mat_Lrot)$dimnames) -->
<!-- #  -->
<!-- # interpret_loadings(mat_Lrot, fun_env) -->


<!-- #``` -->

When approached in this way, the portfolio optimization problem is dimensionally reduced from fourteen proposals to just three crosscutting signals. Each signal may itself be viewed as a _thematic_ AR4D portfolio, i.e. as a set of investments that is weighted towards a particular development theme (in this case, either Economic Growth, Economic Equality, or Environmental Sustainability).

If the return to proposal $i$ in dimension $j$ can be expressedThen the expected return to each signal can be calculatedwhere $n$ is the number of proposals in the portfolio.


<!-- #```{r, fig.show='hold', fig.width=5, fig.height=4, fig.align='center', fig.cap="\\label{fig:AR4DFrontier}Hypothetical example of the optimal frontier and budget shares in the AR4D context.", echo = FALSE} -->

<!-- # sum_Lrows <- as.numeric(mat_Lrot %*% as.matrix(rep(1, n_signals))) -->
<!-- # mat_Lrot_normdByRows <- diag(1 / sum_Lrows) %*% mat_Lrot -->
<!-- # #rowSums(mat_Lrot_normdByRows) -->
<!-- # rownames(mat_Lrot_normdByRows) <- rownames(mat_Lrot) -->
<!-- # mat_mu <- diag(mu_pctRet) %*% mat_Lrot_normdByRows -->
<!-- # #mat_mu %*% as.matrix(rep(1, n_signals)) - as.matrix(mu_pctRet) -->
<!-- # #mu_pctRet_dims <- colSums(mat_mu) -->
<!-- # mu_pctRet_dims <- colMeans(mat_mu) -->
<!-- # #------------------------------------------------------------- -->
<!-- # nab_pctRet <- mu_pctRet_dims -->
<!-- # nab_C <- rep(1, n_signals) -->
<!-- # mat_nab <- cbind(nab_pctRet, nab_C) -->
<!-- #  -->
<!-- # n_points_on_frontier <- 50 -->
<!-- # fun_env <- list(n_points_on_frontier, -->
<!-- #                 utility_interpretation = T, -->
<!-- #                 backtest_info = NULL, -->
<!-- #                 frontier_and_budget_plot = T, -->
<!-- #                 group_info = NULL, -->
<!-- #                 group_colors = NULL, -->
<!-- #                 group_small = NULL, -->
<!-- #                 C_targ = 1, -->
<!-- #                 fig_title = NULL) -->
<!-- # AR4D_eg_rTarg_limits <- c(0.08, .6) -->
<!-- # list_out <- get_optimal_frontier(covmat, mat_nab, -->
<!-- #                                  Rtarg_limits = AR4D_eg_rTarg_limits, -->
<!-- #                                  fun_env) -->
<!-- #  -->
<!-- # df_wStar <- list_out[[1]] -->
<!-- # df_frontier <- list_out[[2]] -->
<!-- # outlist_gg <- list_out[[4]] -->
<!-- # gg_frontier <- outlist_gg[1] -->
<!-- # gg_weights <- outlist_gg[2] -->
<!-- #  -->
<!-- # gg_frontier[[1]] + gg_weights[[1]] + plot_layout(ncol = 1) -->

<!-- #(eigen(covmat)$vectors)^2 %*% as.matrix(mu_pctRet_dims) -->

<!-- #``` -->


The resource allocation to the individual AR4D proposals within each of these dimensions is calculated as follows.,,,The optimal budget allocation to the $i^{th}$ proposal then follows as



<!-- # ```{r, fig.show='hold', fig.width=7, fig.height=5, fig.align='center', fig.cap="\\label{fig:propWgts}dfgg.", echo = FALSE} -->

<!-- # fun_env <- list(n_points_on_frontier, -->
<!-- #                 utility_interpretation = F, -->
<!-- #                 backtest_info = NULL, -->
<!-- #                 frontier_and_budget_plot = T, -->
<!-- #                 group_info = NULL, -->
<!-- #                 group_colors = NULL, -->
<!-- #                 group_small = NULL, -->
<!-- #                 C_targ = 1, -->
<!-- #                 fig_title = NULL) -->
<!-- #  -->
<!-- # list_out <- get_optimal_frontier(covmat, mat_nab, -->
<!-- #                                  Rtarg_limits = AR4D_eg_rTarg_limits, -->
<!-- #                                  fun_env) -->
<!-- #  -->
<!-- # df_wStar <- list_out[[1]] -->
<!-- # #rowSums(df_wStar[, -1]) -->
<!-- #  -->
<!-- # mat_wStar <- as.matrix(df_wStar) -->
<!-- # #mat_Lrot_normdByCols <- mat_Lrot %*% diag(1 / colSums(mat_Lrot)) -->
<!-- # #colSums(mat_Lrot_normdByCols) -->
<!-- # mat_wStar_prop <- mat_Lrot %*% t(mat_wStar[, -1]) -->
<!-- # #colSums(mat_wStar_prop) -->
<!-- # mat_wStar_prop <- mat_wStar_prop %*% diag(1 / colSums(mat_wStar_prop)) -->
<!-- # mat_wStar_prop <- exp(mat_wStar_prop) -->
<!-- # mat_wStar_prop <- mat_wStar_prop %*% diag(1 / colSums(mat_wStar_prop)) -->
<!-- # # mat_wStar_prop <- mat_wStar_prop %*% diag(1 / colSums(mat_wStar_prop)) -->
<!-- # #colSums(mat_wStar_prop) -->
<!-- # mat_wStar_prop <- cbind(df_wStar$`Risk (standard deviation)`, t(mat_wStar_prop)) -->
<!-- # colnames(mat_wStar_prop) <- c(colnames(df_wStar)[1], rownames(mat_Lrot_normd)) -->
<!-- # df_wStar_prop <- as.data.frame(mat_wStar_prop) -->
<!-- #  -->
<!-- # gg_wgts <- plot_budgetShares(df_wStar_prop, n_points_on_frontier, graph_on = T, group_small = NULL, group_info = group_info, group_colors = group_colors_arb, legend_position = "bottom") -->
<!-- #  -->
<!-- # list_df <- list(df_highYcommod, df_CSA, df_socialCap, df_smallHolder) -->
<!-- # list_gg <- list() -->
<!-- # last_i <- length(list_df) -->
<!-- # axis_titles <- "off" -->
<!-- # Xaxis_numbers_off <- T -->
<!-- # for(i in 1:last_i){ -->
<!-- #   df_this <- list_df[[i]] -->
<!-- #   this_group <- unique(df_this$Group) -->
<!-- #   ind_these <- c(1, which(colnames(df_wStar_prop) %in% df_this$Proposal)) -->
<!-- #   df_wStar_prop_thisGroup <- df_wStar_prop[, ind_these] -->
<!-- #   if(i == last_i){ -->
<!-- #     axis_titles <- "x only" -->
<!-- #     Xaxis_numbers_off <- F -->
<!-- #     } -->
<!-- #   gg_wgts_thisGroup <- plot_budgetShares(df_wStar_prop_thisGroup, -->
<!-- #                                      n_points_on_frontier, graph_on = F, -->
<!-- #                                      group_small = NULL, group_info = NULL, -->
<!-- #                                      group_colors = NULL, -->
<!-- #                                      legend_position = "right", -->
<!-- #                                      fig_title = this_group, -->
<!-- #                                      axis_titles, -->
<!-- #                                      Xaxis_numbers_off) -->
<!-- #   list_gg[[i]] <- gg_wgts_thisGroup -->
<!-- # } -->
<!-- #  -->
<!-- # # ind_soCap <- c(1, which(colnames(df_wStar_prop) %in% df_socialCap$Proposal)) -->
<!-- # # df_wStar_prop_soCap <- df_wStar_prop[, ind_soCap] -->
<!-- # # gg_wgts_soCap <- plot_budgetShares(df_wStar_prop_soCap, n_points_on_frontier, graph_on = T, group_small = NULL, group_info = NULL, group_colors = NULL, legend_position = "right") -->
<!-- #  -->
<!-- # gg_wgts | (list_gg[[1]] / list_gg[[2]] / list_gg[[3]] / list_gg[[4]]) | plot_layout(widths = c(2, 1)) -->

<!-- ``` -->


# Discussion
Interpretation of the example. Note that proposals with low expected return do not necessarily correspond to a low budget share...vice versa...Note how the optimal solution changes with risk tolerance. At lower risk tolerances, the optimal investment focuses on economic growth, followed by environmental sustainability and economic equality. However, as risk tolerance increases, the optimal investment dictates that investments in environmental sustainability be displaced by investments in economic growth and, to a lesser extent, economic equality. / Equation \ref{eq:...} effectively means that, in the absence of data, domain knowledge of the key dimensions of the problem space, and of how the various moving parts of the problem (i.e. the portfolio items) map onto these dimensions, is sufficient to estimate the data covariance matrix. The crowdsourced covariance matrix, moreover, need not be considered inferior to the data covariance matrix in terms of accuracy. In noisy contexts, it may very well be more accurate than the data covariance matrix. Regardless of accuracy, the crowdsourced covariance matrix has value in that it provides a picture...consistent with what we think we know about the system. The appropriateness of the approach is determined by the ratio $\nu$ defined in equation \ref{eq:ratio_conf}.

\begin{equation}
\nu = \frac{\text{confidence in domain knowledge}}{\text{confidence in data}}
\label{eq:ratio_conf}
\end{equation}

As $\nu$ is higher, the crowdsource approach is more suitable. As $\nu$ is lower, it becomes more appropriate to estimate the covariance matrix on the basis of data. For values of $\nu$ close to 1, one might consider estimation from both directions.

-elicitation can be accomplished in a variety of ways (eg.: online surveys, in-person events, consultation of strategy documents, some combination thereof, etc.).
-dimensional reduction
-

* Benefit of reverse engineering: the empirical covariance matrix has a lot of noise in it, and very ill-conditioned for large datasets. Backtest. In the example in Figure ..., the frontier looks good, offering portfolio returns of 20% or more for relatively small risk. However, a backtest of the optimal weights (right side of Figure \ref{fig:mvConv}) indicates that the real return on investment in these optimal portfolios would have been much less, and the risk much higher, than the efficient fontier suggests. [ The dataset is large enough to allow for backtesting of the optimal portfolio. The portfolio is optimized using the first two thirds of the data (the "train" data), and then backtested against the remaining third. ]  [Note also that the frontier here looks much less appealing than the unmodified frontier above. However, a backtest (right side of Figure \ref{fig:mvUtility}) suggests that it is more accurate than the unmodified approach.]

* May not be exactly "accurate", but generates an optimal solution that is consistent with what we think we know about the domain. Moreover, it should kept in mind that data are not infallible sources of information. Data of the sort considered here, in particular, are inherently noisy. Under such circumstances, arguably, it may be preferible to build up the covariance matrix from crowdsourced domain knowledge, in the manner shown above, rather than calculate it from data.

* Can get into interpretation of $\Lambda$ eigenvalues deduced from $L_{\circlearrowleft}$

* Dimensionally reduced from potentially several dozen to just a handful of signals.



























































































<!-- ## Separating signals from noise, identifying main assets contributing to market movements -->

<!-- MV analysis is, in many ways, still a work in progress, with unresolved methodological issues severely limiting its usefulness even in its native financial context. In particular, covariance matrices estimated from price data tend to contain a lot of noise; and the optimal solutions then tend to diverge considerably from reality as a result of this [@michaud1989markowitz]. In my adaptation below, I propose a new approach to this problem by first dimensionally reducing the portfolio from several dozen assets to just a handful of "signals" constructed from the eigenvectors of the correlation matrix that can be meaningfully distinguished from noise. -->

<!-- The meaningful eigenvectors are separated out from the noisy eigenvectors using an old, but little-utilized technique developed in the study of physical systems. Once signals have been isolated, their meaning is interpreted based on how much the different portfolio items load onto them. A varimax rotation of the loadings matrix turns out to be of great assistance in this task. The interpretation of the signals is then further confirmed and illustrated by constructing the signal time series and then plotting them against their respective highest loading price series. The highest loading price series are seen to track their respective signals closely. -->





<!-- <!-- [Scraps: -->
<!-- <!-- The Consultative Group on International Agricultural Research (CGIAR) is said to have a "long history of good intentions but limited success in developing appropriate approaches for priority setting" [@Birner2016]. An historic effort begun in 2011 to restructure CGIAR financing around Core Research Programs (CRPs) was ad hoc, thereby reinforcing the very institutional inertia it was designed to disrupt [@Birner2016]. ReDonor attempts to establish criteria and methods for a merit-based allocation of ever-scarcer research funds are seen as invasive characterized as "development at the expense of research" [@Birner2016] or even the "Balkanization" of research [@Petsko2011]. "One of the geniuses" of CG centers is said to be their ability to consistently attract "aid funding of long-term research" while keeping "aid professionals from setting research agendas" [@mccalla2014cgiar]. Scientists respond with ultimatums of their own, demanding a return to the days of "more stable funding" and "budget autonomy" [@leeuwis2018reforming]. -->
<!-- <!-- The failure of reforms is attributed to the unwillingness of donors, and the World Bank leadership of the CGIAR, to take on entrenched center interests [@mccalla2014cgiar]. Interpretation of "price" is different from the financial context. This could reflect 1) higher kcals per unit weight, 2) higher quality kcals, 3) higher demand, or 4) some combination of all of the above. -->


<!-- # Discussion and conclusion -->

<!-- Discussion -->

<!-- -AR4D resource allocation has been called for for a long time . -->
<!-- -There is a cultural impasses between donors and research institutes .  -->
<!-- -Much of the cultural impasse is premised upon a highly qualitative resource allocation procedure that is easily influenced by stakeholder pressure. basic lack of precise impartial tools at the portfolio level.  -->
<!-- -It stands to reason, then, that the elaboration of such tools could go a long way towards ameliorating the toxic environment.  -->
<!-- -[BUT] Adaptation of MV analysis to the AR4D context requires clearing some non-trivial methodological hurdles, which have also been of concern in the financial context: addressing the negative budget allocations and noise reduction. Here I have explored the possibility of redressing these issues in a novel ways (...) . -->
<!-- -Here I am mainly concerned with introduction and adaptation of these methods to the AR4D context. and pointing towards their potential. In the financial example, there is enough data to conduct a backtest. The results of the backtest are encouraging, but do not necessarily prove that the methods presented here will always generate portfolio frontiers that are more accurate than the conventional frontier. More empirical tests are necessary for that. -->


<!-- [end] -->

<!-- MV Analysis is still a work in progress even in the financial context where it originated. In my adaptation to the AR4D context, I have attempted to redress the issue of negative budget shares by replacing the revenue and cost functions with utility functions. To redress the issue of noise-induced inaccuracy, I have attempted to purge the noise from the portfolio by dimensionally reducing the portfolio to a handful of signals constructed from those eigenvectors of the data correlation matrix that can be meaningfully distinguished from noise. To isolate these signals, I have applied a criterion developed in the study of phsyical systems, more rigorous than the usual rules of thumb. A backtest of these modifications compares favorably to conventional MV Analysis. -->



<!-- #```{r, fig.show='hold', fig.width=6, fig.height=3, fig.align='center', fig.cap="\\label{fig:sigPortfolio_U}\\textit{(Left) }Optimal frontier and budget shares for the signals portfolio, using the utility approach to account for diminishing returns and to force positive budget shares. \\textit{(Right) } Backtest of the frontier."} -->
<!-- #-------------------------------------------------------------- -->
<!-- utility_interpretation <- T -->
<!-- #-------------------------------------------------------------- -->
<!-- Rtarg_limits <- c(10^(-9), 1.3) -->
<!-- #-------------------------------------------------------------- -->
<!-- fun_env_getOptFront[["Rtarg_limits"]] <- Rtarg_limits -->
<!-- fun_env_getOptFront[["utility_interpretation"]] <- utility_interpretation -->
<!-- #-------------------------------------------------------------- -->
<!-- list_out <- get_optimal_frontier(covmat_train, mat_nab, -->
<!--                                  fun_env = fun_env_getOptFront) -->
<!-- df_wStar <- list_out[[1]] -->
<!-- df_frontier <- list_out[[2]] -->
<!-- #-------------------------------------------------------------- -->
<!-- list_graph_options[["fig_title"]] <- "Efficient frontier and budget shares" -->
<!-- gg_frontier <- plot_frontier(df_frontier, ROI_basis = T, list_graph_options, graph_on = F) -->
<!-- #-------------------------------------------------------------- -->
<!-- # df_wStar_agg <- df_wStar %>% gather_("Item", "Budget Share", colnames(df_wStar)[-1]) -->
<!-- # df_wStar_agg <- merge(df_wStar_agg, df_match_group, by = "Item") -->
<!-- # df_wStar_agg <- df_wStar_agg %>% group_by(`Risk (standard deviation)`, Group) %>% summarise(`Budget Share` = sum(`Budget Share`)) %>% spread(Group, `Budget Share`) -->
<!-- #gg_budget <- plot_budgetShares(df_wStar_agg, group_small = NULL, color_vec = group_colors, graph_on = F, list_graph_options = NULL) -->
<!-- n_items <- ncol(df_wStar) - 1 -->
<!-- gg_budget <- plot_budgetShares(df_wStar, color_vec = color_vec_mv_sigs_eg, graph_on = F, list_graph_options = NULL) -->
<!-- #-------------------------------------------------------------- -->
<!-- df_plot_train <- df_frontier[, c("Risk (standard deviation)", "Return target")] -->
<!-- df_plot_test <- df_frontier[, c("Risk backtest", "Return backtest")] -->
<!-- df_plot_train$Type = "Optimal solution" -->
<!-- df_plot_test$Type = "Backtest" -->
<!-- colnames(df_plot_train)[1:2] <- c("Risk (standard deviation)", "Return") -->
<!-- colnames(df_plot_test)[1:2] <- c("Risk (standard deviation)", "Return") -->
<!-- df_plot <- rbind(df_plot_train, df_plot_test) -->

<!-- gg <- ggplot(df_plot, aes(x = `Risk (standard deviation)`, y = Return, group = Type, color = Type)) -->
<!-- gg <- gg + geom_point() -->
<!-- gg <- gg + scale_color_manual(values = c("blue", "black")) -->
<!-- gg <- gg + labs(title = "Backtest") -->
<!-- gg <- gg + theme(axis.title.y = element_blank(), -->
<!--                  legend.title = element_blank(), -->
<!--                  legend.position = "bottom", -->
<!--                  plot.title = element_text(face = "bold", size = 10)) -->
<!-- gg_backtest <- gg -->
<!-- #-------------------------------------------------------------- -->
<!-- (gg_frontier + gg_budget + plot_layout(ncol = 1)) | gg_backtest -->


<!-- #``` -->




\pagebreak

# References
