---
title: Risk-adjusted, multi-objective optimization of agricultural research for development portfolios
author:
  - name: Benjamin Schiek
    email: b.schiek@cgiar.org
    affiliation: Alliance Bioversity-CIAT
    corresponding: b.schiek@cgiar.org
address:
  - code: Alliance Bioversity-CIAT
    address: Alliance of Bioversity International and CIAT, Km 17 Recta Cali-Palmira, Valle del Cauca, Colombia, CP 763537
abstract: |
  For several decades, efforts to improve quantitative foresight and decision support in agricultural research for development have focused on the development of models to assess the potential impacts of individual research projects (or programs) under a range of future climate and socioeconomic scenarios. However, if the ultimate aim of such exercises is to determine an optimal allocation of research funds across the projects under assessment, then a portfolio level tool is also required in order to translate a set of research impact assessments into a set of optimal budget allocations. Here I present a novel    This tool should take account of the potential risks each project individually, as well as the synergies and tradeoffs (covariances) between projects, and output the risk-minimizing project budget allocation for a given portfolio return target and budget. Here I develop such a tool. 
keywords: 
  - keyword1
  - keyword2
journal: "Research Policy"
date: "`r Sys.Date()`"
classoption: preprint, 3p, authoryear
bibliography: AR4D Portfolio Optimization.bib
linenumbers: false
numbersections: true
# Use a CSL with `citation_package = "default"`
# csl: https://www.zotero.org/styles/elsevier-harvard
output: 
  rticles::elsevier_article:
    keep_tex: true
    citation_package: natbib
---

<!-- Conventional agricultural research for development (AR4D) priority setting exercises have long been implemented to build insight and consensus around the strengths and weaknesses of individual agricultural research proposals in a given portfolio, but stop short of providing tools that can translate collective insights into optimal resource allocation shares. They also generally exclude from the allocation decision any rigorous accounting of the risk involved in each proposal, or of tradeoffs and synergies between proposals. These methodological lacunae have repeatedly exposed resource allocation processes to ad hoc, politically driven decisionmaking, thereby contributing to growing toxicity in donor-researcher relations. Here I explore the possibility of removing the methodological basis for this discord by introducing a rigorous allocation method, adapted from financial contexts, that allocates precise, optimal budget shares to each proposal based on its expected scalability, risk, and synergies/tradeoffs with the other proposals in the portfolio. -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE)
#devtools::install_github("thomasp85/patchwork")
library(plyr)
library(tidyverse)
# library(zoo)
# library(tidyquant)
library(patchwork)
library(kableExtra)
library(pracma)

label_size <- 2.5
smallLabel_size <- 2
title_size <- 8
subtitle_size <- 7
legendText_size <- 7
axisText_size <- 6
axisTitle_size <- 7
facetTitle_size <- 7
cellText_size <- 3
#==============================================================
# Define function to plot covariance/correlation matrices
plot_covmat <- function(covmat, fig_title = "Covariance Matrix", round_to = 2, graph_on = T, legend_position = NULL, num_size = NULL){
  covmat[upper.tri(covmat)] <- NA
  df_plot <- covmat %>% tbl_df()
  these_levels <- colnames(df_plot)
  df_plot$ItemX <- colnames(df_plot)
  gathercols <- colnames(df_plot)[-ncol(df_plot)]
  df_plot <- df_plot %>% gather_("ItemY", "Value", gathercols)
  df_plot$ItemX <- factor(df_plot$ItemX, levels = these_levels)
  df_plot$ItemY <- factor(df_plot$ItemY, levels = these_levels)
  
  #midpoint <- min(covmat) + (max(covmat) - min(covmat)) / 2
  midpoint <- 0
  gg <- ggplot(df_plot, aes((ItemX), (ItemY)))
  gg <- gg + geom_tile(aes(fill = Value))#, width = 4, height = 4)
  if(!is.null(num_size)){
    gg <- gg + geom_text(aes(label = round(Value, round_to)), size = num_size)
  }else{
    gg <- gg + geom_text(aes(label = round(Value, round_to)), size = 2.5)
  }
  if(!is.null(fig_title)){
    gg <- gg + labs(title = fig_title)
  }
  gg <- gg + theme_bw()
  gg <- gg + theme(axis.text.x = element_text(size = 7, angle = 60, hjust = 1),
                   axis.text.y = element_text(size = 7),
                   axis.title = element_blank(),
                   legend.title = element_blank(),
                   plot.title = element_text(face = "bold", size = 8))
  if(!is.null(legend_position)){
    gg <- gg + theme(legend.position = legend_position)
  }
  gg <- gg + scale_fill_gradient2(low = "magenta", mid = "khaki", high = "cyan", midpoint, na.value = "white")
  if(graph_on){print(gg)}
  return(gg)
  
}
#===============================================================
# End function definition
#===============================================================


```

# Introduction

For several decades, efforts to improve quantitative foresight and decision support in agricultural research for development (AR4D) have focused on the development of models to assess the potential impacts of individual research projects under a range of future climate and socioeconomic scenarios. However, if the ultimate aim of such exercises is to determine an optimal allocation of research funds across the projects under assessment, then a portfolio level tool is also required in order to translate research impact assessments into optimal budget allocations.

Mills clearly identified and articulated the need for such a tool long ago [-@Mills1998], [emeritus scientists at the International Center for Tropical Agriculture recall implementing  informally in risk-return space...[@JCock_perscomm; @lynam2017forever]] and yet work in this direction has not yet begun in earnest. Instead of portfolio optimization, AR4D institutions hold "priority setting" exercises, whereby research programs are ranked in order of importance based on their respective impact assessments. These rankings are then translated into research budget allocations in a subjective and ad hoc manner.

The priority setting process takes little or no account of the risk associated with individual research programs, much less of the synergies and tradeoffs (i.e. correlations) that may arise between programs. Research portfolios based on this approach are thus unbalanced, and hence quite risky relative to a properly balanced portfolio. Stated conversely, by neglecting research correlations, the priority setting method misses out on an opportunity to hedge research portfolio risk by balancing positive and negative correlations against each other.

[In addition to exposing the investor to higher-than-necessary levels of risk, the ps approach leaves a great deal up to subjective...vulnerable to political pressures...bad blood. led to frustrations...]

A key challenge hindering work in this direction is the lack of data by which to calculate research correlations. In a previous article, I redress this problem by developing a method to deduce research correlation matrices based solely on expert opinion [-@schiek2021reverse]. Here I build upon that work to devise an AR4D risk-adjusted, multi-objective portfolio optimization workflow.

The proposed method solves for the impact maximizing budget allocation subject to a risk constraint calculated using the deduced correlation matrix. A budget constraint is not explicitly included, but can be arbitrarily enforced, post solution. The method rests on two novel methodological propositions. Firstly, I define impact in terms of a quantity analogous to kinetic energy in physics, which I label the "vis viva". This effectively results in a quadratic definition of program impact, which in turn results in a correspondence between optimal budget allocations and the eigenvectors of the research covariance matrix.

Secondly, I model the project funding cycle in terms of rotations in the complex plane. While this may seem outlandish at first, I argue that it is more realistic than the conventional notion of project value as something which is always positive. It also provides a meaningful framework by which to interpret the negative values that often turn up in the optimal budget allocation, and by which to transform them into positive values.
<!-- solves for the risk-minimizing (i.e. optimally hedged) research budget allocation, subject to a portfolio impact constraint. -->

Because of the correspondence between solutions and eigenvectors, the proposed method offers not one, but rather a menu of optimal budget allocations. I show that each budget allocation on this menu corresponds to a different risk tolerance. Building on my earlier work [], I also show how each solution may be interpreted as corresponding to a particular strategic objective or policy. (In the financial context, the eigenvectors correspond to market sectors [@Gopishka, @schiek].) The proposed method thus allows the investor to choose, from a menu of solutions, the budget allocation that best matches their risk tolerance and/or strategic focus for the funding cycle.

Alternatively, I show how the investor can construct a multi-objective solution as a linear combination of the individual solutions on the menu. 

...spectral reduction...

The proposed workflow is partly inspired by the portfolio optimization method originally devised in the financial context [markowitz, merton], but differs from that convention in a few key respects, especially the two just mentioned above. In passing, it is interesting to note that many financial practitioners have found "eigenportfolios" (i.e., portfolios balanced in accordance with one of the covariance matrix eigenvectors) give the best results, even though such budget allocations are generally not solutions to the financial portfolio optimization problem []. The question of why this should be so remains an open, active area of research in the financial context []. Those involved in that conversation may thus find the method presented here---which links eigenportfolios with optimal solutions---of interest.

...Financial theorists are in the process of developing their own method of guaranteeing strictly positive budget shares in the optimal portfolio based on an extension of the Perron-Frobenius theorem [], but this is limited to the leading eigenvector of the .... which does not allow the investor to consider the other risk tolerance levels and strategic objectives (sectors, in the financial context) associated with the other eigenvectors, must less allow for multi-objective optimization.  The approach to negative budget shares proposed here may be applied to any of the solutions  ...may be interpreted in terms of short selling, but are nonetheless also problematic---


I begin with a brief explanation of the motivation behind this work.


















Here I redress this gap by developing a portfolio optimization method that solves for the risk minimizing budget allocation [subject to a vis viva constraint]. More specifically, the method outputs a menu of optimal budget allocations corresponding to a range of investor risk tolerances. I show how each of the optimal budget allocations on this menu may be interpreted as a research portfolio optimally focused on a particular policy or strategic objective.

Since most donors and/or research directors are interested in optimizing across multiple strategic objectives, I introduce a subsequent step to optimize the budget allocation across these optimally focused portfolios. The resulting multi-objective optimal budget allocation is thus a linear combination of the optimally focused portfolios determined in the first step. I also present a novel technique to ensure that budget allocations are positive.

Finally, I present a hypothetical use case to better illustrate the sequence of steps in the portfolio optimization workflow.

The development of this method rests upon a novel ... * gBm

* vis viva


While AR4D is the motivating context for the present work, the method can be used in any portfolio optimization workflow.

Finally, I present a hypothetical use case in the AR4D context.

I begin with a literature review focusing on the motivation behind the present work and on the relevant analogous work in the financial context.


* Ensuring positive budget allocations

## Mills' missing fifth step

Many years ago, Bradford Mills articulated the four steps of priority setting in agricultural research for development (AR4D) as follows [-@Mills1998]:

>Step one is the development of an information base. Step two is the establishment of research alternatives. Step three is the evaluation of the potential impact of research alternatives. Step four, often considered the final output of a priority-setting exercise, is the ranking of alternatives into research priorities.

However, he went on to observe that these steps were insufficient. An as of yet non-existent

>fifth step, the development of guidelines for translating priorities into planning and resource allocation decisions, is necessary to establish direct links with planning and resource allocation activities [@Mills1998].

<!-- Mills identified this lacuna at a time when AR4D centers were just beginning to come under pressure from public and private donors to "do a lot more with a lot less" [@Alston1995]. Methodological work in response to this pressure focused on Steps 1-4. In hindsight, Mills' concern was well founded. Budgetary pressure on AR4D centers to "prove their relevance" [@Braunschweig2000], "show value for money" [@yet2016bayesian], and otherwise demonstrate "more efficient spending of resources" [@petsakos2018comparing] has only increased. -->
<!-- The development and refinement of ex-ante impact assessment models for the evaluation of individual research alternatives under different plausible future scenarios, in particular, is remarkable (see, for example, @Alston1995; @Antle1999; @Antle2015; Mills, 1998; and @nelson2014modeling). However, work has still not begun on Mills' missing fifth step. -->

```{r, fig.show = 'hold', out.width="10cm", fig.cap="\\label{fig:mills_missing_step}The AR4D priority setting workflow, adapted from Mills (1998).", fig.align='center'}

knitr::include_graphics("Mills_missing_step5.png")

```

Between Step 4 and the final allocation of funds, stakeholder politics, institutional inertia, ad hoc procedures and criteria, and other subjective forces have repeatedly undercut any careful, objective rigor occurring in Steps 1-4 [@Birner2016; @mccalla2014cgiar]. In 2011, for example...
An historic effort begun in 2011 to restructure CGIAR financing around a "portfolio" model proved emblematic of this "limited success", when the results of an extensive priority setting exercise were rejected out of hand in favor of an ad hoc budget allocation. The end result thereby 

This inconsistency, opacity, and subjectivity in resource allocation decisions has, in turn, aggravated scientists' already longstanding distrust of budget allocation mechanisms. It is only natural, under such circumstances, to see research programs growing increasingly entrenched in their respective silos, and relations between donors and research programs deteriorating to historic levels of toxicity [@Birner2016; @leeuwis2018reforming; @mccalla2014cgiar].
Resource allocation procedures are, as a result, viewed by many scientists as something of a bureaucratic existential threat, and duly condemned as "development at the expense of research" [@Birner2016], or even the "Balkanization" of research [@Petsko2011]. "One of the geniuses" of the CG centers is said to be their ability to consistently attract "aid funding of long-term research" while keeping "aid professionals from setting research agendas" [@mccalla2014cgiar].

It's about rules

integrating risk too ...Alston and Norton acknowledged in 1995 that the treatment of risk in impact assessment models was "rudimentary and in need of further refinement" [-@Alston1995]. Unfortunately, this remains true today.

The task of deciding how to optimally allocate a limited budget across a portfolio of research proposals that are all, in one way or another, vitally important, is never an easy one. However, the problem articulated by Mills over twenty years ago at once suggests its solution: An impartial, transparent resource allocation mechanism, at the portfolio level, could substantially reduce the pain involved in such decisions. In this paper, I explore the possibility of developing such a tool based on risk-adjusted portfolio optimization techniques developed in the financial context.


Repeated efforts to reform the CGIAR have made some progress in getting researchers to think carefully about the the "theory of change" surrounding their research, but no progress on the portfolio front... tend to reinforce the very institutional inertia that the reform effort had intended to disrupt (Birner &amp; Byerlee, 2016). The aim of the present paper is to reduce the methodological basis for such failures as much as possible.

discussion/motivation:

in the absence of such tool, much of the hard work in the individual impact assessment has been repeatedly undercut by politically driven. this is about establishing rules, protocols. the example of bigwigs throwing out the impact assessment conclusions...The continued breaking of rules, failure to honor agreements, moving of goal posts, leads to what is known in game theory as a credible commitment problem, receding into silos. ... the process becomes indistinguishable from rent-seeking.
risk
scale invariant
can be thought of as multi objective


<!-- However, methodological limitations have  leave plenty of room for politics etc. to shape the decision... An historic effort begun in 2011 to restructure CGIAR financing around a "portfolio" model proved emblematic of this "limited success", when the results of an extensive priority setting exercise were rejected out of hand in favor of an ad hoc budget allocation. The end result thereby reinforced the very institutional inertia that the effort had intended to disrupt (Birner &amp; Byerlee, 2016). The aim of the present paper is to reduce the methodological basis for such failures as much as possible.   The methodological limitations of conventional priority setting are more to blame than any set of persons... Mill's missing fifth step... My aim is to improve methodological rigor and objectivity ...such that research conglomerates genuinely seeking to optimize resource allocation will have the tools required to achieve this. this is an issue faced across many fields [@physics, @medicine]   Scientists respond with ultimatums of their own, demanding a return to the days of "more stable funding" and "budget autonomy" [@leeuwis2018reforming]. -->

program level, consisting of numerous research projects and complementary activities at various stages of execution.
The reader is encouraged to replace "project" with "program", as appropriate.

The conventional approach to such problems, pioneered in the financial context,
is...

\begin{equation}
\min_{\mathbf{w}}\: \sigma^2 \:\:\:\:s.t. \:\:\: C = \bar{C} \:, \:\:\: X = \bar{X}
\end{equation}

However, this ...numerous problems even in its native context...documented, for example, by ... [@]... In motivating context of the present article, there is also the issue of negative budget weights. Many authors have rejected the conventional approach in favor of "eigen-portfolios", i.e. budget allocations that are equal to one of the eigenvectors of the covariance matrix. Efforts to demonstrate that these eigenvectors are solutions to a portfolio optimization problem are somewhat strained, ultimately requiring that the return gradient be summarily/arbitrarily replaced by eigenvectors or a linear combination thereof [@Boyle].   ...Moreover, the conventional approach neglects the dynamic dimension of the problem. Here I present a dynamically explicit optimization model. I show that, when the model is dynamic, it makes sense to replace the cost and return constraints with a single "energy" constraint reflecting the finite effort available to elevate the net present value of a benefit stream to some desired level above its status quo value. The budget constraint is trivially enforced by including it in the formulation of energy. An energy constraint as opposed to a budget constraint and return target constraint. Keeping the dynamic part explicit has several benefits -- the optimal budget allocation is guaranteed to be positive... and the solution is rigorous, as opposed to arbitrarily insisting that the return gradient be ignored, and replaced by an eigenvector--which is tantamount to insisting that the return gradient is equal to the optimal budget allocation.

In the conventional formulation of the risk-adjusted portfolio optimization problem, the return and cost constraints are static, ignoring the dynamic dimension of the problem.





The Perron-Frobenius theorem guarantees that the leading eigenvector of a symmetric matrix with only  has strictly positive elements.

Since the matrix $K$ is symmetric, then its leading eigenvector generally has, or can be modified to have, only positive entries [@noutsos2006perron].

Tricks for modifying the covariance matrix so that it is effectively coerced into being an eventually positive matrix.

Transformations of covmat [@carrasco2011optimal; ledoit2004well]


# Modeling the evolution of program NPV as a geometric Brownian movement

Ex-ante impact assessment and scenario analysis are designed to quantify research project impacts at a specific point in time, usually in terms of net present value (NPV), based on assumptions about the future trajectory of numerous random processes, both research and non-research related. (To facilitate the process, assumptions regarding non-research random processes, particularly future climate and socioeconomic processes, have been conveniently packaged in a set Representative Carbon Pathways (RCPs) and Shared Socioeconomic Pathways (SSPs) [@].)

Because it is a function of numerous random processes, project NPV is itself a stochastic variable. The first step in portfolio analysis is to select a reasonable model of the stochastic evolution of project NPV between now (the time of the funding decision) and the end of the funding cycle. In other contexts, this choice is often aided by the availability of time series data. In the AR4D context, project NPV time series generally do not exist; and so the choice must be made based on certain assumed characteristics of the unobserved data. In particular, one must assume a certain size distribution of changes in the stochastic variable. That is to say, one must assume a distribution for the change in program NPV $\Delta x$ with respect to a judiciously small increment in time $\Delta t$. If the change $\Delta x / \Delta t$ over a judiciously small time step is normally distributed, then the stochastic process is accurately modeled by a geometric Brownian movement, defined as follows.

\begin{equation}
\begin{split}
\Delta x &= x(t + \Delta t) - x(t) \\
&= x(t) m \Delta t + x(t) s \epsilon \sqrt{\Delta t}
\end{split}
\label{eq:gbmEq}
\end{equation}

Where $\epsilon$ is a normally distributed random variable with mean $0$ and variance $1$, such that

\begin{equation}
\begin{split}
m \Delta t &= E \left[\frac{\Delta x}{x} \right] \\
s^2 \Delta t &= Var \left[\frac{\Delta x}{x} \right]
\end{split}
\end{equation}
<!-- and $m_k$ is the expected growth rate; that is to say $m_k = E[\Delta x_k/x_k(\hat{t})]; \:\:\: \Delta x_k = x_k(\hat{t} + \Delta t) - x_k(\hat{t})$ -->

(For a reference, see Hull [@].)

Technically, $\Delta t$ is an instantaneous or infinitesimal time interval and hence $m$ is the expected instantaneous growth rate. In practice, it is acceptable to treat $\Delta t$ as a finitely small, but judicious, time step. As a rule of thumb, the time step should be as small as possible while still ensuring that changes in $x$ occur in every time step. In the present context, it is reasonable to assume that random research and/or non-research related changes in project NPV occur at least once per annual quarter. The time step $\Delta t$ at the project level may thus be judiciously defined as an annual quarter, and $m$ may be interpreted as the expected quarterly growth rate, or the expected arithmetic return.
<!-- This selection must be made on the basis of both realism and expedience. Periods of a few substantial changes interspersed by relatively longer periods of many small changes. Geometric Brownian motion expedient and versatile  -->

By definition, project NPV appreciates over time at the quarterly discount rate $r$. Conversely put, a program growth rate different from $r$ implies that the program NPV is inaccurately calculated. Therefore, by assuming that project NPV is accurately calculated, one effectively assumes that the expected arithmetic return $m$ equals $r$.

## Expected value, variance, and covariance of program NPVs

Given the assumption that program NPV follows a geometric Brownian movement, and assuming that program NPVs are accurately calculated, the expected value of the $k^{th}$ program NPV is
<!-- at the end of the funding cycle ($x_k(T_k)$) -->

\begin{equation}
E[x_k(T_k)]\bigr|_{t = \hat{t}} = x_k(\hat{t}) e^{r \tau_k}
\label{eq:ExTraw}
\end{equation}

Where $\hat{t}$ is the time of evaluation, and $\tau_k = T_k - \hat{t}$.
<!-- In many AR4D fund allocation decisions, the focus is on programs rather than projects. A program generally consists of several projects and complementary activities in different stages of implementation. At the program level, a monthly time step is probably justified, with $m_k$ thus representing the expected monthly growth rate. -->
<!-- ...focus here is on program level...which simplifies things...can be extended to project portfolios -->
<!-- Research-related factors concern the expected effectiveness of the researched technology at experiment stations. Non-research related changes in project value occur as a result of changes in the political, socio-economic, and institutional enabling environments where the new technology is to be released. Such changes may include, for example, abrupt changes in government policies, commodity price swings, changes in seed systems and other value chain mechanisms, changes in the security environment, and so forth. -->

The variance of $x_k(T_k)$ is

\begin{equation}
Var[x_k(T_k)]\bigr|_{t = \hat{t}} = \mu_k^2 (e^{s_k^2 \tau_k} - 1)
\label{eq:xVar}
\end{equation}

<!-- $s_k = \sqrt{Var[\Delta x_k / x_k(\hat{t})]}$ -->
Where the shorthand $\mu_k = E[x_k(T_k)]\bigr|_{t = \hat{t}}$ has been introduced. This may be interpreted as a measure of the project risk. Note that this equation can be rearranged into an expression for the coefficient of variation $c_k$, as follows.

\begin{equation}
c_k = \frac{\sigma_k}{\mu_k} = \mu_k \sqrt{e^{s_k^2 \tau_k} - 1}
\label{eq:xCV}
\end{equation}

Where the shorthand $\sigma_k = \sqrt{Var[x_k(T_k)]\bigr|_{t = \hat{t}}}$ has been introduced. The covariance between two project NPVs $x_k$ and $x_j$, meanwhile, is

\begin{equation}
\text{cov}[x_k(T_k), x_j(T_j)]\bigr|_{t = \hat{t}} = \mu_k \mu_j (e^{s_{kj} \sqrt{\tau_k \tau_j}} - 1)
\label{eq:xCov}
\end{equation}

Where the shorthand $s_{kj} = \text{cov}[\Delta x_k/x_k(\hat{t}), \Delta x_j/x_j(\hat{t})]$ has been introduced, such that $s_{kk} = s_k^2$. Note that this may be rearranged into a natural extension of the coefficient of variation $c_{kj}$, as follows.

\begin{equation}
c_{kj} = \sqrt{\frac{\sigma_{kj}}{\mu_k \mu_j}} = \sqrt{e^{s_{kj} \sqrt{\tau_k \tau_j}} - 1}
\label{eq:xCcoV}
\end{equation}
<!-- Where $\rho_{kj} = \text{cor}(\Delta x_k / x_k(\hat{t}), \Delta x_j / x_j(\hat{t}))$. \rho_{i, j} s_k s_j \tau -->

Where the shorthand $\sigma_{kj} = \text{cov}[x_k(T_k), x_j(T_j)]\bigr|_{t = \hat{t}}$ has been introduced, such that $\sigma_{kk} = \sigma_k^2$ (and $c_{kk} = c_k^2$). This is a project NPV "coefficient of covariation", which may be interpreted as an indicator of the correlation between the NPVs of projects $k$ and $j$, analogous to the correlation coefficient. (Whereas the correlation coefficient is the covariance normalized by the product of the respective standard deviations, the coefficient of covariation is the covariance normalized by the product of the respective means.)
<!-- Expected portfolio NPV $\mu(\hat{t})$  is then just the sum of the individual expected project NPVs. -->
<!-- \begin{equation} -->
<!-- \mu(\hat{t}) = \Sigma_k \mu_k(\hat{t}) -->
<!-- \end{equation} -->

The portfolio variance, it follows, can be expressed

<!-- = Var[X(T)]\bigr|_{t = \hat{t}} -->
\begin{equation}
\begin{split}
\sigma(\hat{t})^2 &= \mathbf{1} ' \Sigma \mathbf{1} \\
&= \boldsymbol{\mu}(\hat{t})' G \boldsymbol{\mu}(\hat{t})
\end{split}
\end{equation}

Where $G$ is the project NPV "coefficient of covariation matrix", i.e. the matrix whose elements are $c_{kj}$. Note that the matrix $G$ is just the project NPV covariance matrix left and right multiplied by a diagonal matrix of the inverse project NPV means.

\begin{equation}
G = D(\boldsymbol{\mu})^{-1} \Sigma D(\boldsymbol{\mu})^{-1}
\end{equation}

Where the notation $D(\boldsymbol{\mu})$ indicates a diagonal matrix whose diagonal elements are the elements of the vector $\boldsymbol{\mu}$. Alternatively, $G$ can be expressed in terms of the project NPV correlation matrix (call this $K$).

\begin{equation}
G = D(\mathbf{c}) K D(\mathbf{c})
\end{equation}

Where, to be clear, the elements of $K$ are defined $[K]_{kj} = \text{cor}[x_k(T_k), x_j(T_j)]\bigr|_{t = \hat{t}}$, and the elements of the vector $\mathbf{c}$ are the project coefficients of variation $c_k$.

## Deducing program NPV means, variances, and covariances in minimum data contexts \label{sec:volEst}
<!-- ## Deducing the program correlation matrix in minimum data contexts -->

It is all very well to theorize and deduce formal expressions, but how are these things to be estimated in practice? In the financial context, means, variances, and covariances can be estimated from time series data in a straightforward manner. As mentioned earlier, however, no analogous data exists in the AR4D context.

Project NPV at the start of the funding cycle ($x_k(0)$) can be estimated based on ex-ante impact assessment. Given values for $r$ and $T_k$, it is then possible to calculate $\mu_k(0)$ by Eq \ref{eq:ExTraw}.
<!-- $\alpha_k$ -->
<!-- {eq:muFnw} -->

Estimation of the project NPV coefficients of variation $c_k$ and covariation $c_{kj}$ depends upon estimation of project NPV variances $\sigma_k^2$ and covariances $\sigma_{kj}$, which in turn hinges upon estimation of the project return variances $s_k^2 \tau_k$ and covariances $s_{kj} \sqrt{\tau_k \tau_j}$.

While it may be difficult to develop an intuitive grasp of the value of the project volatility parameter $s_k$, research managers (and/or stakeholders and/or the foresight economists conducting the program appraisal) generally have some idea of the uncertainty surrounding estimates of project NPV, such that they can be asked to quantify minimum or maximum bounds on the growth rate or expected arithmetic return $r$. For example, they can usually be relied upon to make statements to the effect of: "In the very best of cases, the maximum arithmetic return to project $k$ by the end of the project is probably about 1.5". (Or they may give the same information in percentage terms (150$\%$), or in terms of magnitude, which is then easily converted to arithmetic return.)

This return $\bar{\mathcal{R}}_k$ over the life of the project $T_k$ then implies a maximum growth rate $\bar{r}_k$ over the judiciously chosen time step $\Delta t$ that satisfies

\begin{equation}
\bar{\mathcal{R}}_k = (1 + \bar{r}_k)^{T_k} - 1
\end{equation}

And hence the implicit maximum arithmetic return over the judicious time step may be expressed

\begin{equation}
\bar{r}_k = (\bar{\mathcal{R}}_k + 1)^{\frac{1}{T_k}} - 1
\end{equation}

If the project time horizon is 12 years and the judicious time step is defined as 1 annual quarter, then $T_k = 48$, and the implicit maximum quarterly arithmetic return works out to $0.019$ given the ex-ante impact assessment $\bar{\mathcal{R}}_k = 1.5$.
<!-- # ```{r} -->
<!-- RupLoP <- 1.5 -->
<!-- Tk <- 12 * 4 -->
<!-- #RupLoP = (1 + rUp)^Tk - 1 -->
<!-- rUp <- (RupLoP + 1)^(1 / Tk) - 1 -->
<!-- ``` -->

This maximum quarterly arithmetic return may then be interpreted as the upper bound of the $95\%$ confidence interval about $r$. Let the upper and lower bounds of this interval be denoted $\bar{r}_k$ and $\underline{r}_k$, respectively. Since project NPV is assumed to follow a gBm, the arithmetic return is thereby assumed to be normally distributed, and hence symmetric about $r$, such that the upper and lower bounds of the confidence interval are related as follows.

\begin{equation}
\frac{\underline{r}_k + \bar{r}_k}{2} = r
\end{equation}

The lower bound of the confidence interval is thus easily deduced from the elicited upper bound $\bar{r}_k$ as $\underline{r}_k = 2 r - \bar{r}_k$. Moreover, by definition, 

\begin{equation}
\begin{split}
\bar{r}_k &= r \Delta t + z s_k \sqrt{\Delta t} \\
\underline{r}_k &= r \Delta t - z s_k \sqrt{\Delta t}
\end{split}
\end{equation}

Where $z = 1.96$ is the standard score corresponding to the $95\%$ confidence interval of a normally distributed random variable. (The standard score can of course be adjusted to match the practitioner's idea of a suitable confidence interval.) Subtracting the second equation from the first and rearranging then gives an expression for the volatility parameter $s_k$.

\begin{equation}
\begin{split}
s_k &= \frac{1}{2 \sqrt{\Delta t} z} (\bar{r}_k - \underline{r}_k) \\
&= \frac{\bar{r}_k - r}{z}
\end{split}
\label{eq:sk}
\end{equation}

Where the $\Delta t$ has been dispensed with because it is defined as the unit ($\Delta t = 1$). In this way, then, $s_k$ can be deduced from ex-ante impact analysis. Note that the same process applies if a minimum life of project arithmetic return $\underline{R}_k$ is elicited instead of $\bar{R}_k$. Having deduced $s_k$ and $\mu_k$, it is then straightforward to compute the variances $\sigma_k^2$ and coefficients of variation $c_k$ in Eqs \ref{eq:xVar} and \ref{eq:xCV}.
<!-- Now, note that the log return over $\Delta t$, i.e. $\ln(x_k(t + \Delta t) / x_k(t) )$, is related to the arithmetic return as follows. -->
<!-- \begin{equation} -->
<!-- \frac{\Delta x_k}{x_k} &= \frac{x_k(t + \Delta t) - x_k(t)}{x_k(t)} = \frac{x_k(t + \Delta t)}{x_k(t)} - 1 \\ -->
<!-- &\Rightarrow \: \ln \left( \frac{x_k(t + \Delta t)}{x_k(t)} \right) = \ln \left( \frac{\Delta x_k}{x_k} + 1 \right) -->
<!-- \end{equation} -->
<!-- The upper and lower quarterly arithmetic returns $\bar{r}_k$ and $\underline{r}_k$ can thus be converted to upper and lower log quarterly returns (call these $\bar{\ell}_k$ and $\underline{\ell}_k$) as follows. -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \bar{\ell}_k &= \ln( \bar{r}_k + 1) \\ -->
<!-- \underline{\ell}_k &= \ln( \underline{r}_k + 1) -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- These are the upper and lower bounds of the 95$%$ confidence interval about the mean log quarterly return, which is given by -->
<!-- \begin{equation} -->
<!-- E \left[ \ln \left( \frac{x_k(t + \Delta t)}{x_k(t)} \right) \right] = \left(r - \frac{s_k^2}{2} \right) -->
<!-- \end{equation} -->
<!-- Alternatively, an analogous statement regarding the minimum percentage return can likewise be elicited. ; or "in the very worst of cases, the minimum arithmetic return to project $k$ is probably about $-50\%$". Maybe it's good to do both.Either way, the volatility $s_k$ can be deduced from the elicited information as follows. -->

Deduction of the covariance coefficients $s_{kj}$ in the absence of data is considerably more complicated, but can be achieved using a method recently developed by Schiek [-@]. To briefly summarize the method: 1) correlations between project returns ($\Delta x_k / x_k(\hat{t})$) and policy returns ($\Delta S_j / S_j(\hat{t})$) are elicited from expert opinion. 2) Policies are interpreted as the $m$ retained principal components of the unobserved project returns data (where $m < n$), orthonormally rotated for clarity. 3) Given this interpretation, and letting the matrix of project-policy correlations be denoted $L_{\circlearrowright}$, an approximate project returns correlation matrix $\tilde{K}_{XX}$ can be obtained as the outer product

\begin{equation}
\tilde{K}_{XX} = L_{\circlearrowright} L_{\circlearrowright} ' \;\;;\;\;\; [\tilde{K}_{XX}]_{kj} \approx \rho_{kj} = \text{cor}(\Delta x_k / x_k(\hat{t}), \Delta x_j / x_j(\hat{t}))$
\end{equation}

This matrix is approximate insofar as it diverges from the data-based correlation matrix $K_{XX}$---i.e., the correlation matrix calculated from the (unobserved) data. $\tilde{K}_{XX}$ may prove to be more accurate than $K_{XX}$ with respect to the true process generating the data insofar as the data are contaminated. Schiek's hypothetical example of a project-policy correlation matrix elicited from experts, and the corresponding approximate project returns correlation matrix [-@], are reproduced in Fig \ref{fig:egProjCorr}.
<!-- See Schiek [-@] for details. -->

```{r, fig.show = 'hold', fig.width=3, fig.height=2, fig.align='left', fig.cap="\\label{fig:Lrot}Bar chart of hypothetical project-policy correlations elicited from experts, reproduced from Schiek (2021)."}
#====================================================================
# Define function(s)
# Barchart of variable-signal correlations
plot_corrXS_barchart <- function(mat_L, group_info = NULL, fig_title = NULL, sigNames = NULL){
  
  nSig <- ncol(mat_L)
  df_plot <- data.frame(Item = row.names(mat_L), mat_L)
  df_plot$Item <- as.character(df_plot$Item)
  #-------------------------------------------------------
  if(is.null(sigNames)){
    signal_id <- paste("Signal", 1:nSig)
  }else{
    #signal_id <- paste("Signal", 1:nSig, "\n", sigNames)
    signal_id <- sigNames
  }
  colnames(df_plot)[2:(nSig + 1)] <- signal_id
  #-------------------------------------------------------
  gathercols <- as.character(signal_id)
  df_plot <- gather_(df_plot, "Signal", "Correlation", gathercols)
  df_plot <- transform(df_plot,
                       Signal = factor(Signal, levels = gathercols))
  
  if(!is.null(group_info)){
    outlist <- group_fn(group_info)
    cols_ordered_by_group <- outlist[[1]]
    group_color_vec <- outlist[[2]]
    group_vec_ordered <- outlist[[3]]
    df_match_group <- data.frame(Item = cols_ordered_by_group, Group = group_vec_ordered)
    df_plot <- merge(df_plot, df_match_group, by = "Item")
    df_plot <- df_plot[order(df_plot$Group), ]
    df_plot$Item <- factor(df_plot$Item, levels = unique(df_plot$Item))
    gg <- ggplot(df_plot, aes(x = Item, y = Correlation, fill = Group))
    gg <- gg + scale_fill_manual(values = unique(group_color_vec))
  }else{
    df_plot$Item <- factor(df_plot$Item,
                           levels = rev(unique(df_plot$Item)))
    gg <- ggplot(df_plot, aes(x = Item, y = Correlation))
  }
  gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
  gg <- gg + ylim(limits = c(-1, 1))
  gg <- gg + facet_wrap(~ Signal, nrow = 1)
  if(!is.null(fig_title)){
    gg <- gg + labs(title = fig_title)
  }
  gg <- gg + theme(axis.text = element_text(size = 7),
                   axis.title.x = element_text(size = 7),
                   axis.title.y = element_blank(),
                   legend.title = element_blank(),
                   legend.text = element_text(size = 7),
                   legend.position = "bottom",
                   strip.text = element_text(size = 7),
                   plot.title = element_text(face = "bold", size = 8))
  gg <- gg + coord_equal()
  gg <- gg + coord_flip()
  gg
  
}
#--------------------------------------------------------------

# Define function to order data by group
group_fn <- function(group_info){
  list_groups <- group_info[[1]]
  group_names <- group_info[[2]]
  group_colors <- group_info[[3]]
  varNames_ordered <- do.call(c, list_groups)
  n_groups <- length(group_names)
  n_items <- length(varNames_ordered)
  if(is.na(group_colors)){
    bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_groups)
    group_colors <- sample(bag_of_colors, n_groups)
    #group_colors <- viridis::viridis_pal(option = "D")(length(group_names))
  }
  #if(reverse_order){group_colors <- rev(group_colors)}
  #varNames_ordered <- colnames(mat_pctDiff)
  group_vec <- rep(NA, n_items)
  group_color_vec <- rep(NA, n_items)
  for(i in 1:n_groups){
    this_group_vec <- list_groups[[i]]
    this_group_name <- group_names[i]
    this_group_color <- group_colors[i]
    group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
    group_color_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_color
  }
  ind_ordered_cols <- order(factor(group_vec))
  cols_ordered_by_group <- as.character(varNames_ordered[ind_ordered_cols])
  group_color_vec <- group_color_vec[ind_ordered_cols]
  group_vec_ordered <- group_vec[ind_ordered_cols]
  out_list <- list(cols_ordered_by_group, group_color_vec, group_vec_ordered, ind_ordered_cols, group_vec)
  return(out_list)
}

#--------------------------------------------------------------
# Define function to plot covariance/correlation matrices
plot_covmat <- function(covmat, fig_title = "Covariance Matrix", round_to = 2, graph_on = T, legend_position = NULL, num_size = NULL){
  covmat[upper.tri(covmat)] <- NA
  df_plot <- covmat %>% tbl_df()
  these_levels <- colnames(df_plot)
  df_plot$ItemX <- colnames(df_plot)
  gathercols <- colnames(df_plot)[-ncol(df_plot)]
  df_plot <- df_plot %>% gather_("ItemY", "Value", gathercols)
  df_plot$ItemX <- factor(df_plot$ItemX, levels = these_levels)
  df_plot$ItemY <- factor(df_plot$ItemY, levels = these_levels)
  
  #midpoint <- min(covmat) + (max(covmat) - min(covmat)) / 2
  midpoint <- 0
  gg <- ggplot(df_plot, aes((ItemX), (ItemY)))
  gg <- gg + geom_tile(aes(fill = Value))#, width = 4, height = 4)
  if(!is.null(num_size)){
    gg <- gg + geom_text(aes(label = round(Value, round_to)), size = num_size)
  }else{
    gg <- gg + geom_text(aes(label = round(Value, round_to)), size = 2.5)
  }
  if(!is.null(fig_title)){
    gg <- gg + labs(title = fig_title)
  }
  gg <- gg + theme_bw()
  gg <- gg + theme(axis.text.x = element_text(size = 7, angle = 60, hjust = 1),
                   axis.text.y = element_text(size = 7),
                   axis.title = element_blank(),
                   legend.title = element_blank(),
                   plot.title = element_text(face = "bold", size = 8))
  if(!is.null(legend_position)){
    gg <- gg + theme(legend.position = legend_position)
  }
  gg <- gg + scale_fill_gradient2(low = "magenta", mid = "khaki", high = "cyan", midpoint, na.value = "white")
  if(graph_on){print(gg)}
  return(gg)
  
}
#====================================================================
prop_CSA <- c("Heat Tolerant\nBeans", "Coffee\nAgroforestry", "Digital\nAgriculture", "Low Emission\nSilvopastoral")
econGrowth_CSA <- c(0.11, 0.38, 0.4, -0.35)
econEquality_CSA <- c(0.7, 0.32, 0.71, 0.27)
envSust_CSA <- c(0.6, 0.42, 0.6, 0.8)
nutrition_CSA <- c(0.65, 0.06, 0.01, 0.04)
#---------------------------------------------
# prop_socialCap <- c("Micro Finance\nand Insurance", "Multi-stakeholder\nPlatforms", "Market Access")
# econGrowth_socialCap <- c(0.2, 0.45, 0.34)
# econEquality_socialCap <- c(0.87, 0.62, 0.79)
# envSust_socialCap <- c(0.1, 0.1, 0)
#---------------------------------------------
prop_smallHolder <- c("Cassava for\nBio-ethanol", "Triple Purpose\nSweet Potato", "Dairy\nCooperative", "Multi-stakeholder\nPlatforms")
econGrowth_smallHolder <- c(0.67, -0.31, 0.15, 0.4)
econEquality_smallHolder <- c(0.6, 0.75, 0.83, 0.81)
envSust_smallHolder <- c(0.32, 0.4, -0.52, 0.1)
nutrition_smallHolder <- c(0, 0.76, 0.67, 0.11)
#---------------------------------------------
prop_highYcommod <- c("Mega Maize", "Hyper Rice", "Ultra Cow")#, "Uber Palm")
econGrowth_highYcommod <- c(0.82, 0.86, 0.76)#, 0.88)
econEquality_highYcommod <- c(-0.34, -0.42, -0.63)#, -0.58)
envSust_highYcommod <- c(-0.6, -0.52, -0.67)#, -0.83)
nutrition_highYcommod <- c(0.4, 0.45, 0.53)
#---------------------------------------------
df_CSA <- data.frame(Proposal = prop_CSA, `Economic Growth` = econGrowth_CSA, `Economic Equality` = econEquality_CSA, `Environmental Sustainability` = envSust_CSA, `Nutritional Security` = nutrition_CSA, Group = "Climate Smart\nAgriculture")

# df_socialCap <- data.frame(Proposal = prop_socialCap, `Economic Growth` = econGrowth_socialCap, `Economic Equality` = econEquality_socialCap, `Environmental Sustainability` = envSust_socialCap, Group = "Social Capital")

df_smallHolder <- data.frame(Proposal = prop_smallHolder, `Economic Growth` = econGrowth_smallHolder, `Economic Equality` = econEquality_smallHolder, `Environmental Sustainability` = envSust_smallHolder, `Nutritional Security` = nutrition_smallHolder, Group = "Smallholder\nResilience")

df_highYcommod <- data.frame(Proposal = prop_highYcommod, `Economic Growth` = econGrowth_highYcommod, `Economic Equality` = econEquality_highYcommod, `Environmental Sustainability` = envSust_highYcommod, `Nutritional Security` = nutrition_highYcommod, Group = "High Value Yield\nEnhancement")

list_df <- list(df_highYcommod, df_smallHolder, df_CSA)#, df_socialCap)

df_Lrot <- as.data.frame(do.call(rbind, list_df))
colnames(df_Lrot)[2:5] <- gsub("\\.", " ", colnames(df_Lrot)[2:(ncol(df_Lrot) - 1)])

df_table <- df_Lrot[, c("Proposal", "Group")]
colnames(df_table)[1] <- "Project"

df_table$Project <- gsub("\n", " ", df_table$Project)
df_table$Group <- gsub("\n", " ", df_table$Group)
#========================================================================
#========================================================================
# Have to turn off table to fit PLOS ONE submission guidelines? Otherwise, turn back on.
# kable(df_table,
#       format = "pandoc",
#       caption = "**Hypothetical list of AR4D projects**\\label{tab:exampLoadings}") %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
#========================================================================
#========================================================================
#========================================================================
group_names <- as.character(unique(df_Lrot$Group))
n_groups <- length(group_names)
bag_of_colors <- randomcoloR::distinctColorPalette(k = 2 * n_groups)
group_colors_arb <- sample(bag_of_colors, n_groups)
# # Randomly assign an expected pct. return to each AR4D proposal
# n_prop <- nrow(df_Lrot)
# nSig <- ncol(df_Lrot) - 2
# # inv_scalability_ar4d <- exp(rnorm(n_prop))
# # inv_scalability_ar4d <- round(inv_scalability_ar4d / sum(inv_scalability_ar4d), 3) * 100
# #inv_scalability_ar4d <- runif(n_prop)
# inv_scalability_ar4d <- 10^-2 * c(15.9, 12.7, 17.6, 8.8, 7.4 , 9.0, 10.7, 10.8, 7.8 , 6.8, 9.4) # per unit investment
# #inv_scalability_ar4d <- 1 / scalability_ar4d 
# #nab_decRet_ar4d <- 10^-2 * inv_scalability_ar4d
# #inv_scalability_ar4d <- round(inv_scalability_ar4d / sum(inv_scalability_ar4d), 3) * 100
# #inv_scalability_ar4d <- 10^-2 * c(15.9, 12.7, 17.6, 8.8, 7.4 , 9.0, 10.7, 10.8, 7.8 , 6.8, 9.4)
# scalability_ar4d <- 1 / inv_scalability_ar4d
# #sdX_vec <- 1 / 2 * c(abs(as.matrix(inv_scalability_ar4d) + 2 * as.matrix(rnorm(n_prop))))
# #sdX_vec <- 1.61 * c(abs(as.matrix(inv_scalability_ar4d) * as.matrix(runif(n_prop))))
# sdX_vec <- 10^-2 * c(21.2785349, 9.5972656, 18.4114015, 9.4742876, 2.7923296, 0.6912369, 5.9851785, 12.0813232, 7.0012194, 6.2880289, 7.1941543)
# #sdX_vec <- 1 / 2 * c(12.868125, 12.490252, 11.344356, 11.007095, 4.783202, 10.734870, 9.144469, 8.444207, 5.876558, 7.155542, 12.647645)
# df_pctRet <- data.frame(inv_scalability_ar4d, Item = df_Lrot$Proposal, Group = df_Lrot$Group)
# names(inv_scalability_ar4d) <- df_Lrot$Proposal
# # Plot expected returns for each AR4D proposal
# group_colors <- group_colors_arb
# #plot_returns_barchart(df_pctRet, group_colors)
# list_graph_options <- list()
# list_graph_options[["fig_title"]] <- "AR4D project expected return"
# list_graph_options[["ylab"]] <- NULL
# list_graph_options[["legend_position"]] <- "none"
# list_graph_options[["axisTextX_off"]] <- T
# gg_perRet <- plot_returns_barchart(df_pctRet, group_colors, list_graph_options, graph_on = F)
# 
# df_sd <- data.frame(sdX_vec, Item = df_Lrot$Proposal, Group = df_Lrot$Group)
# list_graph_options[["fig_title"]] <- "Risk (standard deviation)"
# list_graph_options[["legend_position"]] <- NULL
# list_graph_options[["axisTextX_off"]] <- NULL
# 
# gg_perSd <- plot_returns_barchart(df_sd, group_colors, list_graph_options, graph_on = F)
# 
# gg_perRet / gg_perSd
#==============================================================
#==============================================================
#==============================================================
#==============================================================
df_x <- df_Lrot
df_x$Group <- NULL
projNames <- df_x$Proposal
matLcolNames <- colnames(df_x)
df_x$Proposal <- NULL
mat_Lrot <- as.matrix(df_x)
rownames(mat_Lrot) <- projNames
list_groups <- list(prop_highYcommod, prop_smallHolder, prop_CSA)
n_groups <- length(list_groups)
bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_groups)
group_colors <- sample(bag_of_colors, n_groups)
group_info <- list(list_groups, group_names, group_colors)
sigNames <- c("GDP Growth", "Economic\nEquality", "Environmental\nSustainability", "Nutritional\nSecurity")
nSig <- length(sigNames)
sigNames <- paste("Policy", 1:nSig, "\n", sigNames)
fig_title <- "Project-policy correlations elicited from experts"
gg_bar <- plot_corrXS_barchart(mat_Lrot, group_info, fig_title, sigNames)
ggsave("Fig 2.tiff", width = 5, height = 4)
gg_bar
#==============================================================
#D_sdX <- diag(sdX_vec)
KdxdxTilde <- mat_Lrot %*% t(mat_Lrot)
# SigdxdxTilde <- diag(sVec) %*% KdxdxTilde %*% diag(sVec)
# D_sdCorrect <- diag(1 / sqrt(diag(KxxTilde)))
# KxxTilde <- D_sdCorrect %*% KxxTilde %*% D_sdCorrect
colnames(KdxdxTilde) <- row.names(mat_Lrot)
fig_title <- "Approximate AR4D project correlation matrix"
gg_Kdxdx <- plot_covmat(KdxdxTilde, fig_title, round_to = 2, graph_on = F)
#ggsave("Fig 3.tiff")
#==============================================================
# gg_bar + gg_Kdxdx + plot_layout(nrow = 1)
#==============================================================
nProj <- nrow(df_table)

```

```{r, fig.show = 'hold', out.width="10cm", fig.cap="\\label{fig:egProjCorr}(Left) Bar chart of hypothetical project-policy correlations elicited from experts; (Right) approximate project returns correlation matrix deduced as the outer product of the project-policy correlation matrix with itself, reproduced from Schiek (2021).", fig.align='center'}

knitr::include_graphics("Fig2ProjCorr.png")

```

An approximate project returns covariance matrix $\tilde{\Sigma}_{XX}$ then follows as

\begin{equation}
\tilde{\Sigma}_{XX} = D(\mathbf{s}) D(\boldsymbol{\tau}) \tilde{K}_{XX} D(\mathbf{s}) D(\boldsymbol{\tau}) \;\;;\;\;\; [\tilde{\Sigma}_{XX}]_{kj} \approx s_{kj} \sqrt{\tau_k \tau_j}
\end{equation}

Where the elements of the vector $\mathbf{s}$ are the project volatility parameters $s_k$ deduced in Eq \ref{eq:sk}, and the elements of $\boldsymbol{\tau}$ are the project time horizons $\tau_k$. Having deduced the project return covariances, it is then straightforward to compute the project NPV covariances and coefficients of covariation in Eqs \ref{eq:xCov} and \ref{eq:xCcoV}.

## Modeling diminishing returns to investment

Generally speaking, project NPV scales with investment. That is to say, beyond a certain minimum level of funding required to generate, release, and support the diffusion and uptake of the project's research product(s) on a pilot level, further increments in project investment have the effect of broadening the scope of impact---i.e. extending the size and number of populations or markets where the research product is released and has an impact. For reasons well known to economists, this scaling is not linear, but rather marginally diminishing. Project NPV $x_k(\hat{t})$ may thus be expressed as a marginally diminishing function of the funding allocation $w_k$, as follows.

\begin{equation}
x_k(\hat{t}) = \tilde{x}_k(\hat{t}) \left(\frac{w_k}{C} \right)^{\alpha_k}
\label{eq:npvFnW}
\end{equation}

Where $C$ is the portfolio budget, $\tilde{x}_k(\hat{t})$ is the maximum financially feasible NPV---i.e., the NPV if the entire budget were invested in the project (the case where $w_k = C$)---and the exponent $\alpha_k$ is the elasticity of project NPV with respect to funding. That is,

\begin{equation}
\frac{\partial \ln(x_k(\hat{t}))}{\partial \ln(w_k)} = \alpha_k
\end{equation}

And $\alpha_k$ lies between $0$ and $1$, reflecting marginally diminishing returns to investment.

Substituting the right hand side of Eq \ref{eq:npvFnW} for $x_k(\hat{t})$, expected project NPV $\mu_k$ becomes a function of investment.

\begin{equation}
\mu_k(\hat{t}) = \tilde{x}_k(\hat{t}) \left( \frac{w_k}{C} \right)^{\alpha_k} e^{r \tau_k}
\label{eq:muFnw}
\end{equation}

Note that project NPV variances and covariances $\sigma_{jk}$ are also functions of investment, since $\mu_k$ appears in their definition.

Eq \ref{eq:npvFnW} is an ansatz. Practitioners may design other functional forms more suitable to their particular context; but, in general, the ansatz should be restricted to the domain $(0,\infty)$ and be marginally diminishing in the investment $w_k$. In some contexts, it may make sense to use a sigmoidal ansatz that exhibits increasing returns to investment up to a point of constant returns, and decreasing returns thereafter---such as $x_k(0) = \tilde{x}_k(0) e^{-C/w_k}$.
<!-- ... -->
<!-- This may be rearranged into an instructive expression... -->
<!-- \begin{equation} -->
<!-- \frac{\mu_k(\hat{t})}{\tilde{x}_k(\hat{t})} =  \left(\frac{w_k}{C} \right)^{\alpha_k} e^{r \tau} -->
<!-- \end{equation} -->
<!-- ...the expected program value as a fraction of its full (fiscally feasible) potential value...pctg of potentiality realized -->
<!-- or -->
<!-- \begin{equation} -->
<!-- \frac{x_k(\hat{t})}{\tilde{x}_k(\hat{t})} =  \left(\frac{w_k}{C} \right)^{\alpha_k} -->
<!-- \end{equation} -->

# Project kinematics and vis viva

In the broadest terms, a project may be said to intervene in, or operate upon, an existing net benefit stream $\Delta u/\Delta t$ with the aim of raising it to a higher value by the end of the project, such that

\begin{equation}
\frac{\Delta u}{\Delta t}\bigr_{t = T} > \frac{\Delta u}{\Delta t}\bigr_{t = 0}
\end{equation}

This implies an acceleration $\Delta^2 u / \Delta t^2$ which comes as a result of the skilled, concerted, applied efforts that comprise the project intervention. Letting $F$ stand for these applied efforts and $a$ for the acceleration, this axiomatic relation can be expressed $F \sim a$, or

\begin{equation}
F = \lambda a
\label{eq:Newt2}
\end{equation}

Where $\lambda$ is a proportionality constant to be defined based on context.

For sufficiently small time intervals $\Delta t$, it may be said that $F$ is effectively constant, resulting in a similarly constant acceleration of the net benefit stream. This is illustrated in Figure \ref{fig:kinema}, where the shorthand $v = \Delta u/\Delta t$ has been introduced to reduce clutter.

```{r, fig.show = "hold", fig.width = 3, fig.height=3, fig.align="center", fig.cap="\\label{fig:kinema}Generic example of an acceleration of a net benefit stream.", echo = FALSE}

t <- seq(0, 1, length.out = 10)
slope <- 1
yint <- 1 / 3
v <- slope * t + yint
df <- data.frame(t, v)
ymax <- max(v)

gg <- ggplot(df, aes(x = t, y = v))
gg <- gg + geom_line()
gg <- gg + geom_segment(aes(x = 0, xend = 1, y = yint, yend = yint), linetype = 2)
gg <- gg + geom_segment(aes(x = 0, xend = 1, y = ymax, yend = ymax), linetype = 2)
gg <- gg + geom_segment(aes(x = 1, xend = 1, y = 0, yend = ymax), linetype = 1)
gg <- gg + geom_segment(aes(x = 0, xend = 1.2, y = 0, yend = 0),
                        arrow=arrow(length=unit(0.2,"cm")), linetype = 1)
gg <- gg + geom_segment(aes(x = 0, xend = 0, y = 0, yend = 2),
                        arrow=arrow(length=unit(0.2,"cm")), linetype = 1)
gg <- gg + geom_ribbon(aes(ymin = yint, ymax = v), fill = "blue", alpha = .5)
gg <- gg + geom_ribbon(aes(ymin = 0, ymax = yint), fill = "orange", alpha = .5)
gg <- gg + annotate("text", x = 2 / 3, y = yint * 1.75,
                    label = expression(italic(Delta*A)))
gg <- gg + annotate("text", x = 2 / 3, y = yint * 0.5,
                    label = expression(italic(A["0"])))
gg <- gg + annotate("text", x = -0.06, y = yint,
                    label = expression(italic("v(t)")), size = label_size)
gg <- gg + annotate("text", x = -0.06, y = ymax,
                    label = expression(italic("v(t + "*Delta*t*")")), size = label_size)
gg <- gg + annotate("text", x = 0, y = -0.07,
                    label = "0", size = label_size)
gg <- gg + annotate("text", x = 1, y = -0.07,
                    label = expression(italic(T)), size = label_size)
#gg <- gg + coord_cartesian(ylim = c(0, 2))
# gg <- gg + labs(x = expression(Delta~t),
#                 y = expression(Delta*u~"/"~Delta*t))
gg <- gg + theme_bw()
gg <- gg + theme(axis.text = element_blank(),
                 axis.ticks = element_blank(),
                 axis.title = element_text(size = axisTitle_size))
gg


```

Note that the slope of the line in this plot is just the acceleration. That is,

\begin{equation}
\frac{\Delta v}{\Delta t} = \frac{\Delta^2 u}{\Delta t^2} = a
\end{equation}

Which can be rearranged as follows,

\begin{equation}
a \Delta t = v(T) - v(0)
\end{equation}

Note, moreover, that the cumulative net benefit over the life of the project ($\Delta u$) is the sum of the areas $A_0 + \Delta A$.

\begin{equation}
\begin{split}
\Delta u &= v(0) \Delta t + \int_0^T v dt \\
&= A_0 + \Delta A \\
&= \frac{1}{2} (v(T) + v(0)) \Delta t
\end{equation}

And a third "kinematic" expression, soon to come in handy, can be obtained by multiplying the previous two as follows.

\begin{equation}
\begin{split}
a \Delta t \Delta u &= \frac{1}{2} (v(T) - v(0)) (v(T) + v(0)) \Delta t \\
a \Delta u &= \frac{1}{2} (v(T)^2 - v(0)^2)
\end{split}
\label{eq:kinema3}
\end{equation}

<!-- Now, since the project has a limited budget, the amount of skilled, concerted, applied effort ($F$) it can exert along the net benefit displacement $\Delta u$ is also limited. That is, the quantity $F \Delta u$ is limited. Let this quantity be denoted $\mathcal{E}$. By equation \ref{eq:Newt2}, $\mathcal{E}$ can be rewritten as follows. -->
Now, consider that, for a given budget, the project aims to maximize the amount of skilled, concerted, applied effort ($F$) exerted by the project along the "displacement" $\Delta u$. That is to say, the aim is to maximize the quantity $F \Delta u$, (subject to constraints which will be addressed farther below). By equation \ref{eq:Newt2}, $\mathcal{E}$ can be rewritten as follows.
<!-- maximize bang for buck -->

\begin{equation}
\mathcal{E} = \lambda a \Delta u 
\end{equation}

But, by equation \ref{eq:kinema3}, this may again be rewritten

\begin{equation}
\mathcal{E} = \frac{\lambda}{2} (v(T)^2 - v(0)^2)
\label{eq:Eraw}
\end{equation}

Finally, note that the same analysis can be reformulated in terms of project NPV---that is to say, in terms of the difference between the net benefit stream _with_ the project intervention versus the net benefit stream _without_ it (the counter-factual). Formally, this means $v$ is redefined

\begin{equation}
v = \frac{\Delta u}{\Delta t}\bigr_{*} - \frac{\Delta u}{\Delta t}
\end{equation}

(Where the "$*$" indicates the "with project intervention" scenario.)

Then $v(0) = 0$, and so equation \ref{eq:Eraw} becomes

\begin{equation}
\mathcal{E} = \frac{\lambda}{2} v(T)^2
\label{eq:Enpv}
\end{equation}

In physics, this is analogous to the expression for the kinetic energy of a moving object with mass $\lambda$, where the $u$ stands for a spatial dimension. To discourage practitioners from getting too cozy with this analogy, $\mathcal{E}$ is henceforth referred to using Gottfried Leibniz's more archaic, and now largely abandoned, term "vis viva" (which means "living force" in Latin). Pragmatically, the vis viva may be interpreted as a quadratic expression of expected program impact.

In the motivating context of the present article, the displacement $\Delta u_k$ corresponds to the $k^{th}$ expected project NPV at $t = \hat{t}$. More precisely,

\begin{equation}
\begin{split}
\Delta u_k = \int_{\hat{t}}^T_k v_k dt &= u_k(\tau_k)|_{*} - u_k(\tau_k) \\
&= E[x_k(T)]\bigr|_{t = \hat{t}}
\end{split}
\end{equation}

Assuming $x_k(t)$ follows a gBm,

\begin{equation}
\Delta u_k |_{t = \hat{t}} = \mu_k(\hat{t}) = x_k(\hat{t}) e^{r \tau_k} \bigr|_{t = \hat{t}}
\end{equation}

The corresponding velocity thus works out to

\begin{equation}
\frac{\Delta u_k}{\Delta t} \bigr|_{t = \hat{t}} = -r x_k(\hat{t}) e^{r \tau_k}
\end{equation}

With starting and finishing values of

\begin{equation}
\frac{\Delta u_k}{\Delta t} \bigr|_{t = 0} = -r x_k(0) e^{r \tau_k}
\end{equation}

and

\begin{equation}
\frac{\Delta u_k}{\Delta t} \bigr|_{t = T_k} = -r x_k(T_k)
\end{equation}

The velocity is negative because time progression in the definition of $\mu_k$ is expressed not in terms of the number of time steps transpired, but rather in terms of the number of time steps that have yet to transpire until the time horizon $T_k$. Nonetheless, the velocity is monotonically increasing in time (Fig \ref{fig:kinema2}).

```{r, fig.show = "hold", fig.width = 3, fig.height=3, fig.align="center", fig.cap="\\label{fig:kinema2}Acceleration of a net benefit stream.", echo = FALSE}

t <- seq(0, 1, length.out = 10)
slope <- 1
yint <- -4 / 3
v <- slope * t + yint
df <- data.frame(t, v)
ymax <- max(v)
ymin <- min(v)

gg <- ggplot(df, aes(x = t, y = v))
gg <- gg + geom_line()
gg <- gg + geom_segment(aes(x = 0, xend = 1, y = ymax, yend = ymax), linetype = 1)
# gg <- gg + geom_segment(aes(x = 0, xend = 1, y = ymax, yend = ymax), linetype = 2)
gg <- gg + geom_segment(aes(x = 1, xend = 1, y = ymax, yend = 0), linetype = 2)
gg <- gg + geom_segment(aes(x = 0, xend = 1.2, y = 0, yend = 0),
                        arrow=arrow(length=unit(0.2,"cm")), linetype = 1)
gg <- gg + geom_segment(aes(x = 0, xend = 0, y = 0, yend = -2),
                        arrow=arrow(length=unit(0.2,"cm")), linetype = 1)
gg <- gg + geom_ribbon(aes(ymin = v, ymax = ymax), fill = "blue", alpha = .5)
# gg <- gg + geom_ribbon(aes(ymin = 0, ymax = yint), fill = "orange", alpha = .5)
gg <- gg + annotate("text", x = 1 / 3, y = yint * 0.5,
                    label = expression(italic(Delta*A)))
# gg <- gg + annotate("text", x = 2 / 3, y = yint * 0.5,
#                     label = expression(italic(A["0"])))
gg <- gg + annotate("text", x = -0.06, y = yint,
                    label = expression(italic("v(t)")), size = label_size)
gg <- gg + annotate("text", x = -0.06, y = ymax,
                    label = expression(italic("v(t + "*Delta*t*")")), size = label_size)
gg <- gg + annotate("text", x = 0, y = 0.07,
                    label = "t", size = label_size)
gg <- gg + annotate("text", x = 1, y = 0.07,
                    label = expression(italic("t + "*Delta*t)), size = label_size)
#gg <- gg + coord_cartesian(ylim = c(0, 2))
# gg <- gg + labs(x = expression(Delta~t),
#                 y = expression(Delta*u~"/"~Delta*t))
gg <- gg + theme_bw()
gg <- gg + theme(axis.text = element_blank(),
                 axis.ticks = element_blank(),
                 axis.title = element_text(size = axisTitle_size))
gg


```

The vis viva, meanwhile, works out to

\begin{equation}
\begin{split}
\mathcal{E}_k(\hat{t}) &= \frac{\lambda}{2} r^2 x_k(\hat{t})^2 e^{2r \tau_k} \\
&= \frac{\lambda}{2} r e^{r \tau} \mu_k(\hat{t})^2
\end{split}
\end{equation}

The velocity and vis viva of an entire portfolio may be expressed

\begin{equation}
\begin{split}
v_p = \sqrt{\Sigma_k \left( \frac{\Delta u_k}{\Delta t}} \right)^2 &= r \sqrt{ \Sigma_k \mu_k^2} \\
&= r \sqrt{\boldsymbol{\mu} ' \boldsymbol{\mu}} \\
\end{split}
\end{equation}

and

\begin{equation}
\mathcal{E} = \frac{v_p^2}{2} = \frac{r^2}{2} \boldsymbol{\mu} ' \boldsymbol{\mu}
\end{equation}
<!-- Note, finally, that the vis viva per unit project expenditure may be expressed -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \frac{\mathcal{E}_k}{w_k} &= r^2 x_k(\hat{t})^2 e^{2r \tau} \\ -->
<!-- &= r^2 k_k^2 \left( \frac{w_k}{C} \right)^{2 (\alpha_k - 1)} e^{2r \tau} -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- ...at portfolio level: -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \mathbf{\mathcal{E}} ' \mathbf{w}^{-1} &= r^2 x_k(\hat{t})^2 e^{2r \tau} \\ -->
<!-- &= r^2 k_k^2 \left( \frac{w_k^{\alpha_k - 1}}{C} \right)^2 e^{2r \tau} -->
<!-- \end{split} -->
<!-- \end{equation} -->

# Cyclically real expected project NPV \label{sec:cycReal}

## The funding cycle

The project funding cycle may be broadly defined as the process by which value is transformed from one tangibly or demonstrably real form to another. More specifically, in the AR4D context, it is the process by which a certain amount of funds is transformed into measurable project impacts. Project impacts at the end of a funding cycle may take the form of a demonstrable change in key development indicators; or they may be tangible impacts of a more intermediate nature, such as completed experiments, pilot implementations, proofs of concept, and/or empirical studies that resolve a prerequisite research question---with a view to demonstrable changes in key development indicators farther down the road, at the end of subsequent funding cycles.

## Project time horizons are measured in terms of funding cycles

The duration of a funding cycle is thus, by definition, the time elapsed between the disbursement of funds and the transformation of those funds into the realization of an agreed upon, tangible set of project impacts or "deliverables". Projects are usually designed so that this transformation occurs in a specific period of time. Let $T$ stand for the duration of the funding cycle expressed in terms of the judiciously chosen time step $\Delta t$. A typical AR4D funding cycle currently lasts 1-5 years. For the sake of argument, say it is 3 years. If the judicious time step $\Delta t$ is defined as an annual quarter, then, $T = 4$, in this case.

Assuming that projects cannot begin or end in the middle of a funding cycle, it follows that AR4D project time horizons must be measured as a positive integer multiple of $T$. That is, $T_k = T\nu_k \;;\;\; \nu_k \in \mathbb{Z}^+$.

## Project NPV is cyclically real

In the course of the funding cycle, project funds are productively consumed to generate project deliverables. During this process of transformation from one tangible form to another, the "project value" that undergoes the transformation exists in a limbo state where it is a complex amalgam of sunk cost, deliverables in formation, and funds not yet spent, still in their monetary form.

In other words, the value of the investment during the transformative process is not real, at least not in any immediately tangible or demonstrable sense. One speaks of "project value" at any point in the funding cycle, but one understands that this refers not to any palpable value the project might have at that point, but rather _either_ the value of the monetary investment at the start of the cycle, _or_ the expected value of project deliverables at the end of the cycle.

In this sense, project NPV is "cyclically real". That is, it is cyclically tangible or demonstrable, particularly at the start and end of funding cycles. One may also discern a sunk cost point, where the value is real but negative. At other points in the cycle, project NPV exists, but not in a way that intersects with our reality in any tangible or demonstrable sense. Mathematically, this means that the expected NPV model in equation \ref{eq:muFnw} is effectively restricted to the domain of $\hat{t}$ that are integer multiples of $T$.

## Modeling project NPV in the complex plane

The same can be said of complex numbers. Recall that a complex number $e^{i \theta}$ is defined

\begin{equation}
e^{i \theta} = \cos(\theta) + i \sin(\theta)
\label{eq:compNum}
\end{equation}

Where $i = \sqrt{-1}$ and $\theta \in (0, 2\kappa \pi)$ for any positive integer $\kappa$. Complex numbers are thus cyclically real---at $\theta = \kappa \pi$. Elsewhere, they have no physically meaningful interpretation (leave quantum mechanics aside for now), but nonetheless may be expressed in terms of a real part, $\cos(\theta)$, and an imaginary part, $i\sin(\theta)$. The complex plane thus presents a framework by which to extend $\mu_k(\hat{t})$ (Eq \ref{eq:muFnw}) to time points between the integer multiples of the funding cycle $T$.

To build up to this extension intuitively, note first of all that expected project NPV evaluated at the start of the funding cycle---that is to say $\mu_k(0) = x_k(0)e^{r T \nu_k}$---has the following equivalent expression in the complex plane.

\begin{equation}
\mu_k(0) = x_k(0) e^{\omega 2 \pi \nu_k}
\end{equation}

Where the funding cycle duration $T$ is mapped to $2 \pi$, so that $\omega 2 \pi = r T$; and the time step $\Delta t$ maps to $\Delta \theta$ such that $\Delta \theta = 2 \pi / T$. Now, consider that, by Eq \ref{eq:compNum}, $e^{2i \pi} = 1$. Hence, $\mu_k(0)$ may also be written

\begin{equation}
\mu_k(0) = x_k(0) e^{2 \pi \nu_k (\omega + i)}
\end{equation}

Extension to all angles $\theta \in (0, 2 \kappa \pi)$ in the complex plane then results in the following expression.

\begin{equation}
\begin{split}
\mu_k(\theta) &= x_k(\theta) e^{(2 \nu_k \pi - \theta)(\omega + i)} \\
&= \tilde{x}_k(\theta) \left(\frac{w_k}{C} \right)^{\alpha_k} e^{(T_k - \theta)(\omega + i)}
\label{eq:muFnwComp}
\end{equation}
<!-- [graphic? $\mu_k(\hat{\theta}) / x_k(\hat{\theta})$ decay---ratio of E[x(T)]|_t / x(t)] -->

## Negative project NPV and time horizon adjustments to ensure real valued budget allocations

This extension of expected project NPV to all points on the complex plane encompasses the sunk cost point occurring halfway through the funding cycle ($\theta = (2 \kappa - 1) \pi$), when the investment is real valued, but negative.

\begin{equation}
\begin{split}
\mu_k((2\kappa - 1) \pi) &= x_k((2\kappa - 1) \pi) e^{(T_k - (2\kappa - 1) \pi) (\omega + i)} \\
&= -x_k((2\kappa - 1) \pi) e^{(2 (\nu_k - \kappa) + 1) \pi \omega}
\end{split}
\end{equation}

<!-- By definition, the funding cycle begins and ends on a positive real value. A project investment that is negative at the start of the funding cycle thus implies that it is out of phase with the funding cycle by an odd integer multiple of $\pi$. -->
Without loss of generality, consideration may be limited to the case where the odd integer multiple is $1$. That is, if $\mu_k(0) < 0$, this implies

\begin{equation}
\begin{split}
\mu_k(0) &= -x_k(0) e^{T_k (\omega + i)} \;\;\;;\;\; T_k = 2 \pi \nu_k \\
&= x_k(0) e^{T_k (\omega + i) \pm i \pi}
\end{split}
\end{equation}

Negative project NPV is problematic for portfolio optimization since it implies a complex valued budget allocation. To see this, note that an expression for the project budget allocation is obtained by solving Eq \ref{eq:muFnwComp} for $w_k$ at $\theta = 0$.

\begin{equation}
w_k = C \left( \frac{\mu_k(0)}{\tilde{x}_k(0)} e^{-T_k (\omega + i)} \right)^{\frac{1}{\alpha_k}}
\label{eq:wFnmuComp}
\end{equation}

Whereby it is evident that $w_k$ is complex valued if $\mu_k(0) < 0$ and $T_k = 2 \pi \nu_k$. To ensure that budget allocations are always real valued, the time horizon is henceforth redefined

\begin{equation}
T_k = 2 \pi \left(\nu_k \mp \frac{1}{2} \kappa_k \right)
\label{eq:Tk}
\end{equation}

Where
\begin{equation}
\kappa_k =
\begin{cases}
0, & \text{if $\mu_k(0) > 0$} \\
1, & \text{if $\mu_k(0) < 0$}
\end{cases}
\end{equation}

In this modified definition of the time horizon, a half cycle phase correction is automatically triggered if expected project NPV is negative. The phase correction amounts to a half cycle extension or contraction of the project time horizon. (The decision of whether to extend or contract the horizon is made by the investor and/or research project managers.) This in turn guarantees that the budget allocation is real valued. To see this, consider a case where $\mu_k(0) < 0$ and apply the modified definition of $T_k$.

\begin{equation}
\begin{split}
w_k &= C \left( \frac{\mu_k(0)}{\tilde{x}_k(0)} e^{-T_k (\omega + i)} \right)^{\frac{1}{\alpha_k}} \\
&= C \left( \frac{e^{ \pm i \pi} |\mu_k(0)|}{\tilde{x}_k(0)} e^{-2 \pi \left(\nu_k \mp \frac{1}{2} \right) (\omega + i)} \right)^{\frac{1}{\alpha_k}} \\
&= C \left(\frac{|\mu_k(0)|}{\tilde{x}_k(0)} e^{-2 \pi \left(\nu_k \mp \frac{1}{2} \right) (\omega + i) \pm i \pi} \right)^{\frac{1}{\alpha_k}} \\
&= C \left(\frac{|\mu_k(0)|}{\tilde{x}_k(0)} e^{-2 \pi \omega \left( \nu_k \mp \frac{1}{2} \right) + 2 i \pi \left( \nu_k \mp \frac{1}{2} \right) \pm i \pi} \right)^{\frac{1}{\alpha_k}} \\
&= C \left(\frac{|\mu_k(0)|}{\tilde{x}_k(0)} e^{-2 \pi \omega \left( \nu_k \mp \frac{1}{2} \right) + 2 i \pi \nu_k \mp i \pi  \pm i \pi} \right)^{\frac{1}{\alpha_k}} \\
&= C \left(\frac{|\mu_k(0)|}{\tilde{x}_k(0)} e^{-2 \pi \omega \left( \nu_k \mp \frac{1}{2} \right)} \right)^{\frac{1}{\alpha_k}}
\end{split}
\end{equation}
<!-- # ```{r, fig.show = "hold", fig.width = 6, fig.height=3, fig.align="center", fig.cap="\\label{fig:logSpiral}Log spiral in (left) polar and (right) complex coordinates over 0 \\geq \\theta \\geq 2 \\pi.", echo = FALSE} -->
<!-- theta <- seq(0, 2 * pi, length.out = 50) -->
<!-- r <- 0.01 -->
<!-- Tt <- 40 -->
<!-- omega <- r * Tt / (2 * pi) -->
<!-- a <- 1 -->
<!-- x <- a * exp(omega * theta) * cos(theta) -->
<!-- y <- a * exp(omega * theta) * sin(theta) -->
<!-- df_plot <- data.frame(x, y) -->
<!-- thisTheta <- pi / 3 -->
<!-- thisX <- a * exp(omega * thisTheta) * cos(thisTheta) -->
<!-- thisY <- a * exp(omega * thisTheta) * sin(thisTheta) -->
<!-- theta <- seq(0, thisTheta, length.out = 15) -->
<!-- x <- 0.25 * cos(theta) -->
<!-- y <- 0.25 * sin(theta) -->
<!-- df_angle <- data.frame(x, y) -->
<!-- gg <- ggplot() -->
<!-- gg <- gg + geom_path(data = df_plot, aes(x, y, group = 1)) -->
<!-- gg <- gg + geom_point(data = data.frame(x = thisX, y = thisY), -->
<!--                       aes(x, y), size = 2) -->
<!-- gg <- gg + geom_segment(aes(x = 0, xend = thisX, y = 0, yend = thisY)) -->
<!-- gg <- gg + geom_path(data = df_angle, aes(x, y, group = 1)) -->
<!-- gg <- gg + geom_hline(yintercept = 0) -->
<!-- gg <- gg + geom_vline(xintercept = 0) -->
<!--                         # arrow=arrow(length=unit(0.2,"cm")), linetype = 1) -->
<!-- gg <- gg + annotate( -->
<!--     geom = "text", x = thisX, y = thisY,  -->
<!--     label = expression(italic(mu(theta)==x(0)~e^{omega*theta})), hjust = -0.1, vjust = -0.1, size = 3.5 -->
<!--   ) -->
<!-- gg <- gg + annotate( -->
<!--     geom = "text", x = -0.1, y = 1.7,  -->
<!--     label = expression(italic(y)), size = 4 -->
<!--   ) -->
<!-- gg <- gg + annotate( -->
<!--     geom = "text", x = 1.7, y = -0.1,  -->
<!--     label = expression(italic(x)), size = 4 -->
<!--   ) -->
<!-- gg <- gg + annotate( -->
<!--     geom = "text", x = 0.15, y = 0.1,  -->
<!--     label = expression(theta), size = 3.5 -->
<!--   ) -->
<!-- gg <- gg + coord_cartesian(ylim = c(-1.7, 1.7), xlim = c(-1.7, 1.7)) -->
<!-- gg <- gg + theme_bw() -->
<!-- gg <- gg + theme(axis.title = element_blank(), -->
<!--                  axis.text = element_blank(), -->
<!--                  axis.ticks = element_blank(), -->
<!--                  panel.grid = element_blank()) -->
<!-- ggPolar <- gg -->
<!-- gg <- ggplot() -->
<!-- gg <- gg + geom_path(data = df_plot, aes(x, y, group = 1)) -->
<!-- gg <- gg + geom_point(data = data.frame(x = thisX, y = thisY), -->
<!--                       aes(x, y), size = 2) -->
<!-- gg <- gg + geom_segment(aes(x = 0, xend = thisX, y = 0, yend = thisY)) -->
<!-- gg <- gg + geom_path(data = df_angle, aes(x, y, group = 1)) -->
<!-- gg <- gg + geom_hline(yintercept = 0) -->
<!-- gg <- gg + geom_vline(xintercept = 0) -->
<!--                         # arrow=arrow(length=unit(0.2,"cm")), linetype = 1) -->
<!-- gg <- gg + annotate( -->
<!--     geom = "text", x = thisX, y = thisY,  -->
<!--     label = expression(mu==x(0)~e^{(omega+i)*theta}), hjust = -0.1, vjust = -0.1, size = 3.5 -->
<!--   ) -->
<!-- gg <- gg + annotate( -->
<!--     geom = "text", x = -0.1, y = 1.7,  -->
<!--     label = expression(italic(iy)), size = 4 -->
<!--   ) -->
<!-- gg <- gg + annotate( -->
<!--     geom = "text", x = 1.7, y = -0.1,  -->
<!--     label = expression(italic(x)), size = 4 -->
<!--   ) -->
<!-- gg <- gg + annotate( -->
<!--     geom = "text", x = 0.15, y = 0.1,  -->
<!--     label = expression(theta), size = 3.5 -->
<!--   ) -->
<!-- gg <- gg + coord_cartesian(ylim = c(-1.7, 1.7), xlim = c(-1.7, 1.7)) -->
<!-- gg <- gg + theme_bw() -->
<!-- gg <- gg + theme(axis.title = element_blank(), -->
<!--                  axis.text = element_blank(), -->
<!--                  axis.ticks = element_blank(), -->
<!--                  panel.grid = element_blank()) -->
<!-- ggComp <- gg -->
<!-- ggPolar + ggComp + plot_layout(ncol = 2) -->
<!-- ``` -->

# Risk adjusted portfolio optimization in terms of vis viva

## First order conditions

With the groundwork laid above, the portfolio optimization problem may now be formulated and solved in terms of vis viva maximization subject to a risk tolerance threshold.

\begin{equation}
\max_{\mathbf{w}}{\mathcal{E}} \:\:\: s.t. \:\:\: \sigma = \overline{\sigma}
\end{equation}

Where the portfolio risk $\sigma$ is defined as follows.

\begin{equation}
\sigma = \sqrt{\mathbf{1} ' \Sigma \mathbf{1}} = \sqrt{\boldsymbol{\mu} ' G \boldsymbol{\mu}}
\end{equation}

<!-- Because $\mu_k(w_k)$ is monotonically increasing in $w_k$ -->
Without loss of generality, the control variables $\mathbf{w}$ may be swapped for $\boldsymbol{\mu}$.

\begin{equation}
\max_{\boldsymbol{\mu}}{\mathcal{E}} \:\:\: s.t. \:\:\: \sigma = \overline{\sigma}
\label{eq:probStat}
\end{equation}

Note that this formulation of the portfolio optimization problem does not include a budget constraint. Nonetheless, it will soon become clear that solutions to this problem are valid up to scaling, such that the budget constraint may be arbitrarily enforced.

The objective function or lagrangian is

\begin{equation}
\begin{split}
\mathcal{L} &= \mathcal{E} + \lambda (\sigma - \overline{\sigma}) \\
&= \frac{r^2}{2} \boldsymbol{\mu} ' \boldsymbol{\mu} - \lambda (\sqrt{\boldsymbol{\mu} ' G \boldsymbol{\mu}} - \overline{\sigma})
\end{split}
\end{equation}

The lagrangian multiplier $\lambda$ may be interpreted as the risk shadow price, that is to say, the marginal cost of a marginal increment in risk above the tolerance threshold $\bar{\sigma}$. Formally,

\begin{equation}
\frac{\partial \mathcal{L}}{\partial (\sigma - \overline{\sigma})} = \lambda
\end{equation}

Alternatively, $\lambda$ can be thought of as the marginal benefit of a marginal decrease in risk below the tolerance threshold $\overline{\sigma}$.

The first order conditions with respect to $\boldsymbol{\mu}$ are
<!-- [The project vis viva is proportional to the risk gradient.] -->

\begin{equation}
\nabla_{\boldsymbol{\mu}} \mathcal{L} = r^2 \boldsymbol{\mu} - \frac{\lambda}{\sigma} G \boldsymbol{\mu} = \mathbf{0}
\end{equation}

Where $\mathbf{0}$ is a vector of zeros. This can then be rearranged as follows.

\begin{equation}
G \boldsymbol{\mu} - \frac{r^2 \sigma}{\lambda} \boldsymbol{\mu} = \mathbf{0}
\end{equation}

The program NPV vector $\boldsymbol{\mu}^*$ satisfying the first order conditions is thus given by any of the eigenvectors of $G$, with corresponding risk shadow price given in terms of the respective eigenvalue. More specifically, letting $\mathbf{q}_j$ and $\upsilon_j$ stand for the $j^{th}$ eigenvector and eigenvalue of $G$, respectively, then the $j^{th}$ solution $\boldsymbol{\mu}_j^*$ can be written
<!-- $\frac{r^2 \sigma}{\gamma_j}$ $\lambda$ given by the inverse of the respective eigenvalues, scaled by $r^2$.]  -->

\begin{equation}
\boldsymbol{\mu}_j^* = \mathbf{q}_j
\label{eq:NPVstar}
\end{equation}

With risk shadow price $\lambda_j$

\begin{equation}
\lambda_j = r^2 \frac{\sigma_j^*}{\upsilon_j}
\end{equation}

Where $\sigma_j^*$ is the portfolio risk corresponding to the $j^{th}$ solution,

\begin{equation}
\begin{split}
\sigma_j^* &= \sqrt{\mathbf{q}_j ' G \mathbf{q}_j} \\
&= \sqrt{\upsilon_j}
\end{split}
\label{eq:riskJraw}
\end{equation}

Such that $\lambda_j$ reduces to

\begin{equation}
\lambda_j = \frac{r^2}{\sqrt{\upsilon_j}} = \frac{r^2}{\sigma_j^*}
\label{eq:lamJraw}
\end{equation}

The risk shadow price is thus inversely proportional to portfolio risk. As risk increases, the marginal benefit of additional increments in risk decrease.

### The candidate optimal budget allocations

The budget allocations satisfying the first order conditions ($w_{kj}^*$) are given by Eq \ref{eq:wFnmu} evaluated at $\mu_{kj}^*$.

\begin{equation}
\mathbf{w}_{kj}^* = C \left(\frac{\mu_{kj}^*}{\tilde{x_k(0)}}e^{- r T_k } \right)^{\frac{1}{\alpha_k}}
\end{equation}

But recall that this has the following equivalent expression in the complex plane.

\begin{equation}
\mathbf{w}_{kj}^* = C \left(\frac{\mu_{kj}^*}{\tilde{x_k(0)}}e^{- T_k (\omega + i)} \right)^{\frac{1}{\alpha_k}}
\label{eq:wStar}
\end{equation}
<!-- \begin{equation} -->
<!-- \mathbf{w}_{kj}^* = C \left(\frac{q_{kj}}{\tilde{x_k(0)}} (-1)^{2 \kappa_k + 1} e^{-r T_k} \right)^{\frac{1}{\alpha_k}} -->
<!-- \label{eq:wStar} -->
<!-- \end{equation} -->

Generally speaking, the eigenvectors $\mathbf{q}_j$ may include negative elements, which in turn implies negative project NPV $\mu_{kj}^*$. As explained in \ref{sec:cycReal}, a negative expected NPV may be interpreted as the project value at the sunk cost point in the funding cycle. Negative project NPV is problematic for portfolio optimization since it implies a complex valued budget allocation (Eq \ref{eq:wStar}). The project time horizon $T_k$ is defined in Eq \ref{eq:Tk} so as to automatically extend or contract the time horizon by half a funding cycle, such that the budget allocations $\mathbf{w}_{kj}^*$ are guaranteed to be real valued. Note, moreover, that all of the terms in Eq \ref{eq:wStar} are positive, whereby it is guaranteed that $\mathbf{w}_{kj}^*$ is also positive.
<!-- And in vector notation, -->
<!-- \begin{equation} -->
<!-- \hat{\mathbf{w}}_j^* = \Sigma_{j = 1}^m C_j (D(\tilde{\mathbf{x}}(0))^{-1} \mathbf{q}_j (-1)^{2 \kappa - 1} e^{-r T \left(\frac{1}{2} + \kappa \right)})^{\frac{1}{\boldsymbol{\alpha}}} -->
<!-- \label{eq:wStarPolicy} -->
<!-- \end{equation} -->
<!-- \begin{equation} -->
<!-- \hat{\boldsymbol{\mu}}_j^* = (-1)^{1 - 2 \kappa} e^{rT \left(\kappa - \frac{1}{2}\right)} \beta_j \mathbf{q}_j -->
<!-- \end{equation} -->

### Scaling the solution to enforce constraints

Because eigenvectors are defined up to scaling, the optimal solution can be arbitrarily scaled in accordance with any portfolio impact, risk, or budget target/constraint. The budget constraint is usually the most binding of these three. In order to arbitrarily enforce the budget constraint $C$, then, note that the candidate optimal budget allocations may be rewritten

\begin{equation}
\mathbf{w}_{kj}^* = C \left(\frac{\beta_j \mu_{kj}^*}{\tilde{x_k(0)}} e^{-T_k (\omega + i)} \right)^{\frac{1}{\alpha_k}}
\label{eq:wStarPolicy}
\end{equation}
<!-- \begin{equation} -->
<!-- \hat{\mathbf{w}}_j^* = \Sigma_{j = 1}^m C_j (D(\tilde{\mathbf{x}}(0))^{-1} \beta_j \mathbf{q}_j (-1)^{2 \kappa - 1} e^{-r T \left(\frac{1}{2} + \kappa \right)})^{\frac{1}{\boldsymbol{\alpha}}} -->
<!-- \label{eq:wStarPolicy} -->
<!-- \end{equation} -->

Where $\mathbf{q}_j {'} \mathbf{q}_j = 1$ and $\beta_j$ is an arbitrary scaling constant, which may be chosen such that $\hat{\mathbf{w}}_j^* {'} \mathbf{1} = C$.

When choosing the scaling constant $\beta_j$ to enforce the budget constraint, the corresponding expected portfolio impact (vis viva) $\hat{\mathcal{E}}_j^*$ is also thereby defined.

\begin{equation}
\begin{split}
\hat{\mathcal{E}}_j^* &= \frac{1}{2} \hat{\mathbf{v}}_j^* ' \hat{\mathbf{v}}_j^* = \frac{r^2}{2} \hat{\boldsymbol{\mu}}_j^* ' \hat{\boldsymbol{\mu}}_j^* \\
&= \frac{r^2}{2} \beta_j \mathbf{q}_j ' \beta_j \mathbf{q}_j = \frac{r^2}{2} \beta_j^2
\end{split}
\end{equation}

Moreover, the portfolio risk $\hat{\sigma}_j^*$ (Eq \ref{eq:riskJraw}) and risk shadow price $\lambda_j$ (Eq {eq:lamJraw}) must also be redefined---or rather defined more carefully---as follows.

\begin{equation}
\begin{split}
\sigma_j^* &= \sqrt{\boldsymbol{\mu}_j^* ' G \boldsymbol{\mu}_j^*} = \sqrt{(\beta_j \mathbf{q}_j) ' G (\beta_j \mathbf{q}_j}) \\
&= \beta_j \sqrt{\upsilon_j}
\end{split}
\label{eq:riskJ}
\end{equation}

\begin{equation}
\lambda_j = \frac{r^2}{\beta_j \sqrt{\upsilon_j}}
\label{eq:lamJ}
\end{equation}

## Second order condition \label{sec:soc}

The second order condition guaranteeing that the solution is a maximizer of $\mathcal{L}$ is

\begin{equation}
\mathbf{a} ' H \mathbf{a} < 0 \:|\: \mathbf{a} ' \nabla \mathcal{E} = 0
\label{eq:soc}
\end{equation}

Where $H$ is the Hessian of $\mathcal{L}$ evaluated at the solution $\boldsymbol{\mu}^*$, that is,

\begin{equation}
H = \nabla_{\boldsymbol{\mu}}^2 \mathcal{L}|_{\boldsymbol{\mu}^*}
\end{equation}

And $a$ is any nonzero, real valued vector in the null space of the constraint gradient evaluated at the solution, i.e. $\nabla_{\boldsymbol{\mu}} \sigma |_{\boldsymbol{\mu} = \boldsymbol{\mu}^*}$.

In other words, the second order condition is fulfilled if and only if $H$ is negative definite.

To resolve this condition into a more intelligible form, note that $\mathbf{a}$ can be rewritten in terms of any arbitrary, nonzero, real valued vector $\mathbf{y}$ multiplied by a basis $Z$ whose columns are in the null space of $\nabla_{\boldsymbol{\mu}} \sigma |_{\boldsymbol{\mu} = \boldsymbol{\mu}^*}$.

\begin{equation}
Z \mathbf{y} = \mathbf{a} \:|\: \nabla_{\boldsymbol{\mu}} \sigma |_{\boldsymbol{\mu} = \boldsymbol{\mu}^*} ' Z = \mathbf{0}
\end{equation}

Since $\nabla_{\boldsymbol{\mu}} \sigma |_{\boldsymbol{\mu} = \boldsymbol{\mu}^*}$ is any eigenvector of $G$, say the $j^{th}$ eigenvector, then the other eigenvectors constitute a valid basis $Z$. Letting the notation $Q_j$ stand for the matrix of eigenvectors of $G$ excluding the $j^{th}$ column, and $\boldsymbol{\beta}_j$ stand for the vector of scaling constants $\boldsymbol{\beta}$ excluding the corresponding $j^{th}$ element, then $Z = D(\boldsymbol{\beta}) Q_j$. The second order condition can thus be rewritten

\begin{equation}
\mathbf{y} ' H_j \mathbf{y} &> 0
\end{equation}

Where $H_j$ is the "projected" Hessian of $\mathcal{L}$ evaluated at the $j^{th}$ solution $\boldsymbol{\mu}_j^*$.

\begin{equation}
H_j = D(\boldsymbol{\beta}) Q_j ' \nabla_{\boldsymbol{\mu}}^2 \mathcal{L}|_{\boldsymbol{\mu}^*} Q_j D(\boldsymbol{\beta})
\end{equation}

The second order condition is thus fulfilled if and only if $H_j$ is negative definite. This is easy to determine since $H_j$ is a diagonal matrix. To see this, note that

\begin{equation}
\begin{split}
\nabla_{\boldsymbol{\mu}}^2 \mathcal{L} &= r^2 I - \frac{\lambda}{\sigma} G \\
\Rightarrow \nabla_{\boldsymbol{\mu}}^2 \mathcal{L}|_{\boldsymbol{\mu}^*} &= r^2 I - \frac{\lambda_j}{\sigma^*_j} G \\
&= r^2 \left(I - \frac{1}{\beta_j^2 \upsilon_j} G \right)
\end{split}
\end{equation}

Substituting the right hand side of this equation for $\nabla_{\boldsymbol{\mu}}^2 \mathcal{L}|_{\boldsymbol{\mu}^*}$ in the previous equation thus gives

\begin{equation}
\begin{split}
H_j &= r^2 D(\boldsymbol{\beta}_j) Q_j ' \left(I - \frac{1}{\beta_j^2 \upsilon_j} G \right) Q_j D(\boldsymbol{\beta}_j) \\
&= r^2 \left( D(\boldsymbol{\beta}_j) Q_j ' Q_j D(\boldsymbol{\beta}_j) - \frac{1}{\beta_j^2 \upsilon_j} D(\boldsymbol{\beta}_j) Q_j ' G Q_j D(\boldsymbol{\beta}_j) \right) \\
&= r^2 \left( D(\boldsymbol{\beta}_j)^2 - \frac{1}{\beta_j^2 \upsilon_j} D(\boldsymbol{\beta}_j)^4 D(\boldsymbol{\upsilon}_j) \right) \\
&= r^2 D(\boldsymbol{\beta}_j)^2 \left( I - \frac{1}{\beta_j^2 \upsilon_j} D(\boldsymbol{\beta}_j)^2 D(\boldsymbol{\upsilon}_j) \right)
\end{split}
\end{equation}

Where $\boldsymbol{\upsilon}_j$ is the vector of eigenvalues of $G$ excluding the $j^{th}$ eigenvalue. Keeping in mind that the eigenvalues of $G$ are positive (because $G$ is positive definite), it follows that the solution $\{\mathbf{q}_a, \upsilon_a, \beta_a \}$ such that

\begin{equation}
\frac{\beta_j^2 \upsilon_j}{\beta_a^2 \upsilon_a} > 1 \;\;;\;\;\; \text{for all $j \neq a$}
\end{equation}

is that for which $H_a$ is negative definite, and is hence the solution that satisfies the second order condition (Eq \ref{eq:soc}), and thus maximizes $\mathcal{L}$. By the same token, the solution $\{\mathbf{q}_b, \upsilon_b, \beta_b \}$ such that

\begin{equation}
\frac{\beta_j^2 \upsilon_j}{\beta_b^2 \upsilon_b} < 1 \;\;;\;\;\; \text{for all $j \neq b$}
\end{equation}

is that for which $H_{b}$ is positive definite, and hence the solution that minimizes $\mathcal{L}$; and all other solutions $\gamma \notin \{a, b \}$ produce indefinite $H_{\gamma}$, corresponding to saddle point solutions.
<!-- trailing eigenvector of $\mathbf{q}_n$ is the solution that minimizes $\mathcal{L}$, and the intermediate eigenvectors $(1 < k < n)$ represent saddle point solutions of $\mathcal{L}$ between these two extremes. -->
<!-- ...Since $G$ is positive definite, its eigenvalues are positive. And since $\upsilon_1 > \upsilon_2 > ... > \upsilon_n$ for a portfolio of $n$ items, it is straightforward to see that $H_1$ is negative definite, $H_n$ is positive definite, and the matrices $H_{1 < k < n}$ are indefinite. -->
<!-- (i.e., the case where $\boldsymbol{\mu}^*$ equals the leading eigenvector of $K$) -->
<!-- (i.e., the case where \boldsymbol{\mu}^* equals the trailing eigenvector of $G$) -->
<!-- The leading eigenvector $\mathbf{q}_1$ is thus the solution that satisfies the second order condition (Eq \ref{eq:soc}), and thus maximizes $\mathcal{L}$. By the same token, the trailing eigenvector of $\mathbf{q}_n$ is the solution that minimizes $\mathcal{L}$, and the intermediate eigenvectors $(1 < k < n)$ represent saddle point solutions of $\mathcal{L}$ between these two extremes. -->

# Post-solution considerations

## Keeping only meaningful solutions

The trailing eigenvectors of a covariance matrix tend to represent noise in the data. The budget allocations associated with these eigenvectors are thus effectively random, and should be discarded from the menu of eligible solutions. To ensure that solutions are meaningful, only the leading $m$ eigenvectors required to explain a high fraction of total portfolio variance should be retained. This is tantamount to the "spectral cut-off" method used in the financial context [@guo2018eigen; @carrasco2011optimal].

The fraction of portfolio variance explained by any given eigenvector $\mathbf{q}_j$ (call this $k_j$) is the corresponding eigenvalue $\upsilon_j$ divided by the sum of all the eigenvalues (See Abdi [-@] for details).

\begin{equation}
k_j = \frac{\upsilon_j}{\sum_{k = 1}^n \upsilon_k}
\end{equation}

The cumulative variance captured by a group of $m < n$ leading eigenvectors is then

\begin{equation}
\zeta_m = \sum_{j=1}^m k_j
\end{equation}

Such that $\zeta_n = 1$.

Again, the number of retained eigenvectors $m$ should be high enough to explain a high fraction of the portfolio variance---such that, for example, $\zeta_m \geq 0.95$---but the exact number is left to the practitioner's discretion.

## A menu of solutions catering to a range of risk appetites

The "optimal" solution ($\boldsymbol{\mu}_1^*$) is optimal in the sense that it maximizes expected portfolio impact subject to a risk constraint. However, this is the solution that also corresponds to the highest eigenvalue ($\upsilon_1$), and is hence also the riskiest of the candidate solutions (recall equation \ref{eq:gammaRisk}). The other candidate solutions ($\boldsymbol{\mu}_2^*, \dots, \boldsymbol{\mu}_m^*$) may correspond to lower expected impacts, but also successively decreasing eigenvalues ($\upsilon_2, \dots, \upsilon_m$), and hence successively decreasing risk levels.

The AR4D investor is thus presented not with a single optimal solution, but an optimal menu of solutions corresponding to a range of risk tolerances. Typically, AR4D investors are guided by a pro-poor mandate which requires them to operate in the gaps left by market failures, and hence to incline towards ambitious research portfolios. Ambition implies risk, which in turn implies that AR4D donors should endeavor to select from among the riskier solutions on the optimal menu. It should be kept in mind, moreover, that higher portfolio risk is not unambiguously bad, as it exposes the investor to an increased probability of both lower and higher portfolio return.

Nonetheless, the expected portfolio impact (vis viva) $\mathcal{E}_j$ must also be taken into consideration when selecting a solution from the menu. Obviously, a lower risk solution with higher expected impact should be selected over a higher risk solution with relatively lower vis viva.

Finally, political considerations likely come into play. Solutions that allocate most of the budget to just one or two projects are bound to create a toxic work environment.
<!-- The leading eigenvector of $G$ is the portfolio budget allocation that maximizes portfolio impact subject to a risk constraint. $\overline{\mathcal{E}}$. The other solutions are not necessarily sub-optimal, but rather correspond to lower investor risk tolerances. Here it is important to keep in mind that higher portfolio risk is not unambiguously bad, but rather a matter of ambition. Higher risk exposes the investor to an increased probability of both lower and higher portfolio return. Moreover, if the portfolio items follow a gBm, as assumed here, then returns are lognormally distributed; and so the risk is primarily upside. Moreover, let's not forget that AR4D institutions have a pro-poor mandate to redress market failures; and this can often mean taking on ambitious, higher risk portfolios. -->
<!-- Note that this is reflected in the corresponding eigenvalues. The leading eigenvalue---which corresponds to the risk maximizing solution---is the largest, indicating the highest risk for a given vis viva target $\overline{\mathcal{E}}$ (recall equation \ref{eq:lambDef}), while the remaining eigenvalues are successively smaller, indicating successively lower risk levels. -->
```{r, fig.show = 'hold', fig.width=5, fig.height=2, fig.align='left', fig.cap="\\label{fig:varExplained_barchart}Plot of the individual and cumulative portions of variance explained by each signal in the financial dataset.", echo=F}
c_vec <- eig_values / sum(eig_values)
ck_vec <- cumsum(c_vec)
df_plot <- data.frame(Signal = paste("Signal", 1:length(eig_values)), Portion_explained = c_vec, Portion_explained_cumul = ck_vec)
colnames(df_plot)[2:3] <- c("Individually", "Cumulatively")
gathercols <- colnames(df_plot)[2:3]
df_plot <- gather_(df_plot, "Portion explained", "Value", gathercols)
df_plot$Signal <- factor(df_plot$Signal,
                         levels = unique(df_plot$Signal),
                         ordered = T)
gg <- ggplot(df_plot, aes(x = Signal, y = Value, fill = `Portion explained`))
gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
gg <- gg + scale_fill_manual(values = c("wheat3", "paleturquoise"))
gg <- gg + geom_hline(yintercept = 0.9, color = "coral", size = 1, linetype = "dashed")
gg <- gg + theme(axis.text.y = element_text(size = 7),
                 axis.text.x = element_text(size = 7, angle = 60, hjust = 1),
                 axis.title = element_blank(),
                 legend.title = element_text(size = 8),
                 legend.text = element_text(size = 8))
gg
ggsave("Fig 4.tiff")
#====================================================
nSig <- which(ck_vec > 0.9)[1]
mat_S <- mat_S_all[, 1:nSig]
mat_L <- cormat_XS[, 1:nSig]
eig_values_sigs <- eig_values[1:nSig]
#====================================================
```

**Fig 4. Plot of the individual and cumulative portions of variance** explained by each signal in the financial dataset.

&nbsp;

The plot shows that, for the financial data set, the leading `r nSig` signals are sufficient to meet this criterion.

# Hypothetical example

To illustrate the portfolio optimization workflow presented above, here I walk through a step-by-step example of how the method might be applied in an AR4D fund allocation decision.

In this hypothetical example, a donor wants to know the optimal distribution of limited funds over a five year funding cycle across a portfolio of eight AR4D programs. The programs might be defined in terms of major commodity categories (cereals, roots & tubers, legumes, livestock, fruit & vegetables, fish, etc.); or they might be organized around thematic areas (climate smart agriculture, smallholder resilience, digital agriculture, etc.); or they might be some combination thereof. The donor wants to maximize impact for a given risk tolerance ...such that risk is minimized for a given impact target (as represented by the vis viva). As explained in section ..., there are $m$ possible budget allocations that solve the problem, corresponding to the $m$ leading eigenvectors of the program cross coefficient of variation matrix $G$. Each of these 

The $m$ leading eigenvectors of the project correlation matrix $K_{X, X}$ can be deduced from the crowdsourced rotated project-policy correlation matrix $K_{X,S}^{\circlearrowright}$ as follows.

\begin{equation}
\tilde{P} = K_{X,S}^{\circlearrowright} B ' \tilde{Gamma}^{-\frac{1}{2}}
\end{equation}

(See Schiek [-@] for details.)

\ref{fig:rotCorrXSnParams}

$\tilde{K}$ -> $\tilde{G}$

the exected program NPVs and their corresponding variances are estimated following the method described in section \ref{sec:volEst}

...may want to focus on a specific strategic objective for the funding cycle.

...multi-objective... 

<!-- [so as to minimize risk while maximizing impact across three strategic objectives.] The eight programs are listed vertically in Fig ..., while the three strategic objectives appear across the top of the figure. As usual in the AR4D context, there are no program NPV time series data by which to calculate variances and covariances  The bars in Fig ... represent each program's correlation with each strategic objective, elicited from a survey of experts and stakeholders. -->

<!--  each of the form -->

<!-- \begin{equation} -->
<!-- \mathbf{w}_j^* = C (D(\tilde{\mathbf{x}})^{-1} \mathbf{p}_j e^{-rT})^{\frac{1}{\boldsymbol{\alpha}}} \:\:;\:\:\: j \in (1, 2,..., m) -->
<!-- \end{equation} -->



where $m$ is the number of leading eigenvectors sufficient to explain $90\%$ of the variation in the program NPV. of the cross coefficient of variation matrix $G$.  corresponding to   set of possible budget allocations that solve the portfolio optimization problem are the eigenvectors of the cross coefficient of variation matrix $G$

Following the method explained in section ..., these correlations are interpreted as the leading, orthonormally rotated loadings $L_{\circlearrowleft}$ explaining $90\%$ of variation in the program NPV time series. The spectrally reduced correlation matrix can be calculated as the outer product $\tilde{K} = L_{\circlearrowleft} L_{\circlearrowleft}'$. The leading eigenvectors of this matrix $\tilde{P}$ 

The matrix of leading eigenvectors of the project correlation matrix $\tilde{P}$ is deduced from equation ... Schiek [-@].

The strategic objective covariance matrix can be calculated as the inner product $\Sigma_{SS}^{\circlearrowleft} = L_{\circlearrowleft} ' L_{\circlearrowleft}$. This, in turn, has the eigendecompsition $\Sigma_{SS}^{\circlearrowleft} = B \tilde{\Gamma} B'$, where $B$ is the orthonormal rotation matrix. The eigenvectors of $\tilde{K}$ can then be  can thus be deduced from the elicited information $L_{\circlearrowleft}$.

time adjustments

```{r, fig.show = 'hold', fig.width=3, fig.height=2, fig.align='left', fig.cap="\\label{fig:rotCorrXSnParams}(Left) Bar chart of hypothetical project-policy correlations, (right) table of hypothetical project parameters."}
# RupLoP <- 1.5
# Tk <- 12 * 4
# #RupLoP = (1 + rUp)^Tk - 1
# rUp <- (RupLoP + 1)^(1 / Tk) - 1
#--------------------------------------------------------------
# Arithmetic and log return over judicious time step (quarterly)
rYrlyArith <- 0.04 #0.035 # Equal to the discount rate
tStep <- 4 #T step per year
rQrtrlyArith <- (1 + rYrlyArith)^(1 / tStep) - 1
# Convert arithmetic return to log return
r <- round(log(1 + rQrtrlyArith), 3)
#--------------------------------------------------------------
# Project params
alpha <- c(0.65, 0.65, 0.6, 0.75, 0.65, 0.6, 0.65, 0.8, 0.7, 0.75, 0.7)
xTilde <- exp(rnorm(nProj, 6, 0.75))
xTilde <- round(xTilde, 4)
TvecYrs <- c(18, 14, 8, 15, 12, 14, 10, 5, 4, 7, 12)
TvecTstep <- TvecYrs * tStep
tFundCyc <- 3 * tStep # Say 3 year funding cycle
omega <- r * tFundCyc / (2 * pi)
TvecFcyc <- round(TvecTstep / tFundCyc, 2) # nu_k
Tt <- paste(TvecYrs, TvecTstep, TvecFcyc, sep = "/")
#--------------------------------------------------------------
# Deduce project risk
# Arithmetic return upper and lower bounds (95% conf interval)
# Elicit max return over life of project from ex-ante impact assessment
ErTkArith <- (1 + rQrtrlyArith)^(TvecTstep) - 1
rTkArith_max <- ErTkArith + exp(rnorm(nProj, 0.15, 0.75))
# Covert to max return over the judicious time step (quarterly)
rQrtrlyArith_max <- (1 + rTkArith_max)^(1 / TvecTstep) - 1
# Deduce min quarterly return based on symmetry of normal distribution
rQrtrlyArith_min <- 2 * rQrtrlyArith - rQrtrlyArith_max
rQrtrlyArith_max <- round(rQrtrlyArith_max, 4)
# Deduce s
zBound <- 1.96
sVec <- (rQrtrlyArith_max - rQrtrlyArith) / zBound
sVec <- round(sVec, 4)
# Display conf interval
confInt95 <- paste(round(rQrtrlyArith_min, 4), round(rQrtrlyArith_max, 4), sep = "/")
#--------------------------------------------------------------
projNames <- df_table$Project
df_param <- data.frame(proj = projNames, alpha, xTilde,
                       Tt, confInt95, sVec = sVec)
df_param <- df_param %>% gather(var, val, alpha:sVec)
#df_param$val <- factor(df_param$val)
cNames <- c(expression("Scale\nelasticity\n("*alpha*")"),
            "Stand. dev.\narithmetic ret.",
            "Arithmetic return\n95% conf. interval",
            "Time horizon\n(Years/Quarters/Fund. Cyc.)",
            expression(tilde(x))
            )
gg <- ggplot(df_param, aes(x = var, y = proj,
                           label = val))
gg <- gg + geom_tile(alpha = 0)
gg <- gg + scale_x_discrete("var", labels = cNames, position = "top")
# gg <- gg + facet_wrap(~var, nrow = 1, labeller = labeller(var = facetTitles))
gg <- gg + geom_text(size = cellText_size)
#gg <- gg + theme_bw()
gg <- gg + theme(axis.title = element_blank(),
                 axis.text.y = element_text(size = axisText_size),
                 axis.text.x = element_text(size = axisText_size),
                 panel.grid = element_blank(),
                 strip.text = element_text(size = facetTitle_size))
gg_tab <- gg
#--------------------------------------------------------------
gg_tab
#--------------------------------------------------------------

```



\ref{fig:wStarVecs}

```{r, fig.show = "hold", fig.width = 5, fig.height=3, fig.align="center", fig.cap="\\label{fig:wStarVecs}Optimal budget allocations for each policy/risk tolerance level.", echo = FALSE}

G <- exp(SigdxdxTilde) - 1
ups <- eigen(G)$values
xx <- cumsum(ups) / sum(ups)
thresh <- 0.95
nSigG <- which(xx > thresh)[1]
upsTilde <- ups[1:nSigG]
Qtilde <- eigen(G)$vectors[, 1:nSigG]
#-------------------------------------------------------------------
# SigdSdS <- t(mat_Lrot) %*% mat_Lrot
# qP <- colSums(Qtilde)
# #SigP <- diag(qP) %*% (exp(SigdSdS) - 1) %*% diag(qP)
# # diag((mat_Lrot) %*% t(mat_Lrot))
# uSigP <- eigen(SigP)$values
# eVecSigP <- eigen(SigP)$vectors
# #B <- eigen(SigdSdS)$vectors
# # L <- mat_Lrot %*% B
# # Ptilde <- L %*% diag(1 / sqrt(gammaSigdSdS))
#--------------------------------------------------------------------
# Single objective optimal budget allocation
TkHat <- 2 * pi * TvecFcyc
mat_wShareStar <- matrix(NA, nProj, nSigG)
#mat_muStar <- matrix(NA, nProj, nSig)
visVivVec <- c()
betaStarVec <- c()
wSlack <- function(beta, Tk, omega, xTilde, muStar, alpha){
  wShare <- ((diag(exp(-Tk * omega)) %*% diag(1 / xTilde) %*% abs(muStar)) * beta)^(1 / alpha)
  wSlack <- sum(wShare) - 1
  return(wSlack)
}
thisInt <- c(0, 10000)
for(j in 1:nSigG){
  muStar <- Qtilde[, j]
  ind <- which(muStar < 0)
  kappaK <- rep(0, length(TkHat))
  kappaK[ind] <- 1
  Tk <- TkHat - kappaK * pi
  out <- uniroot(wSlack, interval = thisInt,
                 Tk = Tk, omega = omega, xTilde = xTilde,
                 muStar = muStar, alpha = alpha,
                 lower = min(thisInt), upper = max(thisInt))
  betaStar <- out$root
  print(out$f.root)
  wShareStar <- ((diag(exp(-Tk * omega)) %*% diag(1 / xTilde) %*% abs(muStar)) * betaStar)^(1 / alpha)
  print(sum(wShareStar))
  #mat_muStar[, j] <- muStar * betaStar
  mat_wShareStar[, j] <- wShareStar
  visVivVec[j] <- r^2 / 2 * betaStar^2
  betaStarVec[j] <- betaStar
}
mat_muStar <- round(Qtilde, 4)
mat_wShareStar <- round(mat_wShareStar, 4)
#colSums(mat_wShareStar)
visVivVec <- round(visVivVec, 2)
muStarVec <- colSums(mat_muStar)
sigVec <- round(upsTilde, 2)
betaStarVec <- round(betaStarVec, 4)
row.names(mat_muStar) <- projNames
colnames(mat_muStar) <- paste(sigVec, "/", visVivVec)
row.names(mat_wShareStar) <- projNames
colnames(mat_wShareStar) <- paste(sigVec, "/", visVivVec)
#--------------------------------------------------------------------
#visVivVec / sigVec
#--------------------------------------------------------------------
df_plot <- as.data.frame(mat_wShareStar)
gathercols <- colnames(df_plot)
df_plot$Project <- projNames
df_plot <- df_plot %>% gather_("Policy", "Budget share", gathercols)

gg <- ggplot(df_plot, aes(x = Project, y = `Budget share`))
gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
#gg <- gg + ylim(limits = c(0, 1))
gg <- gg + facet_wrap(~ Policy, nrow = 1)
#gg <- gg + labs(y = xAxis_title)
gg <- gg + theme(axis.text = element_text(size = 7),
                 axis.title.x = element_text(size = 7),
                 axis.title.y = element_blank(),
                 legend.title = element_blank(),
                 legend.text = element_text(size = 7),
                 strip.text = element_text(size = 7))
gg <- gg + coord_equal()
gg <- gg + coord_flip()
gg
#=====================================================================
muStar <- mat_muStar[, 1]
SigStar <- diag(muStar) %*% G %*% diag(muStar)
t(muStar) %*% G %*% muStar
Kstar <- diag(1 / sqrt(diag(SigStar))) %*% SigStar %*% diag(1 / sqrt(diag(SigStar)))
e <- eigen(Kstar)$values
xx <- cumsum(e) / sum(e)
thresh <- 0.95
eTilde <- e[1:nSigKstar]
nSigKstar <- which(xx > thresh)[1]
Etilde <- eigen(Kstar)$vectors[, 1:nSigKstar] 

mat_L <- Etilde %*% diag(sqrt(eTilde))

mat_Lrot <- varimax(mat_L)[[1]]
mat_Lrot <- matrix(as.numeric(mat_Lrot),
                   attributes(mat_Lrot)$dim,
                   dimnames = attributes(mat_Lrot)$dimnames)
mat_R <- varimax(mat_L)[[2]]
mat_R <- matrix(as.numeric(mat_R),
                attributes(mat_R)$dim,
                dimnames = attributes(mat_R)$dimnames)
bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_groups)
group_colors <- sample(bag_of_colors, n_groups)
group_info <- list(list_groups, group_names, group_colors)
sigNames <- c("GDP Growth", "Economic\nEquality", "Environmental\nSustainability", "Nutritional\nSecurity")
nSig <- length(sigNames)
sigNames <- paste("Policy", 1:nSig, "\n", sigNames)
fig_title <- "Project-policy correlations elicited from experts"
colnames(mat_Lrot) <- matLcolNames
row.names(mat_Lrot) <- projNames
colnames(mat_L) <- matLcolNames
row.names(mat_L) <- projNames
gg_bar <- plot_corrXS_barchart(mat_Lrot, group_info, fig_title, sigNames)
#ggsave("x.png", width = 5, height = 4)
gg_bar
#=====================================================================


```





\ref{fig:wStarVecMO}

```{r, fig.show = "hold", fig.width = 5, fig.height=3, fig.align="center", fig.cap="\\label{fig:wStarVecMO}Multi-objective optimal budget allocations corresponding to each risk tolerance.", echo = FALSE}
# Multi-objective optimal budget allocation
#gammaSigSS
mat_wShareStarMO <- matrix(NA, nProj, nSig)
mat_muStarMO <- matrix(NA, nProj, nSig)
visVivMOvec <- c()
betaStarMOvec <- c()
for(l in 1:nSig){
  matWgts <- eVecSigP[, l]
  muStarMO <- Q %*% matWgts
  ind <- which(muStar < 0)
  kappaK <- rep(0, length(TkHat))
  kappaK[ind] <- 1
  Tk <- TkHat - kappaK * pi
  out <- uniroot(wSlack, interval = thisInt,
                 Tk = Tk, omega = omega, xTilde = xTilde,
                 muStar = muStarMO, alpha = alpha,
                 lower = min(thisInt), upper = max(thisInt))
  betaStarMO <- out$root
  print(out$f.root)
  wShareStarMO <- ((diag(exp(-Tk * omega)) %*% diag(1 / xTilde) %*% abs(muStarMO)) * betaStarMO)^(1 / alpha)
  print(sum(wShareStar))
  visVivMOvec[l] <- r^2 / 2 * betaStar^2
  mat_wShareStarMO[, l] <- wShareStarMO
  mat_muStarMO[, l] <- muStarMO
  visVivMOvec[l] <- r^2 / 2 * betaStarMO^2
  betaStarMOvec[l] <- betaStarMO
}
mat_muStarMO <- round(mat_muStarMO, 4)
mat_wShareStarMO <- round(mat_wShareStarMO, 4)
#colSums(mat_wShareStar)
visVivMOvec <- round(visVivMOvec, 2)
muStarMOvec <- colSums(mat_muStarMO)
sigVecMO <- round(uSigP, 2)
betaStarMOvec <- round(betaStarMOvec, 2)
row.names(mat_muStarMO) <- projNames
colnames(mat_muStarMO) <- paste(sigVecMO, "/", visVivMOvec)
row.names(mat_wShareStarMO) <- projNames
colnames(mat_wShareStarMO) <- paste(sigVecMO, "/", visVivMOvec)
#--------------------------------------------------------------------
#visVivMOvec / sigVecMO
#--------------------------------------------------------------------
df_plot <- as.data.frame(mat_wShareStarMO)
gathercols <- colnames(df_plot)
df_plot$Project <- projNames
df_plot <- df_plot %>% gather_("Policy", "Budget share", gathercols)

gg <- ggplot(df_plot, aes(x = Project, y = `Budget share`))
gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
#gg <- gg + ylim(limits = c(0, 1))
gg <- gg + facet_wrap(~ Policy, nrow = 1)
#gg <- gg + labs(y = xAxis_title)
gg <- gg + theme(axis.text = element_text(size = 7),
                 axis.title.x = element_text(size = 7),
                 axis.title.y = element_blank(),
                 legend.title = element_blank(),
                 legend.text = element_text(size = 7),
                 strip.text = element_text(size = 7))
gg <- gg + coord_equal()
gg <- gg + coord_flip()
gg

```


# Discussion and conclusion

* Valid insofar as projects are scaleable, etc.... (key assumptions)

* ...there was no straightforward explanation or expectation of/for conservation of vis viva (energy)...In an analogous development, financial analysts find that eigenvectors give the best solutions, even though the conventional theory does not .

The task of ... all of which are in one way or another vitally important, will never be an easy one. ...Here I have attempted to reduce the methodological basis for this limited success by introducing a portfolio optimization workflow.... not limited to the AR4D...but applicable to any multi-objective portfolio optimization workflow.

Above I have presented a novel portfolio optimization method which... offers a menu of budget allocations depending upon risk tolerance... multiple strategic objectives or policies ... novelty comes from the novel inclusion of the vis viva constraint....parallel development in physics...positive budget allocations.

* In physics one might also apply the naive budget constraint approach---can only move so far in the three spatial dimensions.---it was only through empirical observation that physicists realized that a much less intuitive constraint was the one they should pay attention to, that of energy, one which practitioners still puzzle over to this day.---In finance, the naive approach has been pretty useless, led mostly to frustration. Leibniz did not reason that the quantity $v^2$ was conserved, but rather arrived at this conclusion through empirical observation...even today the rationale is not clear---ask your local physicist what energy is. In a parallel manner, portfolio managers sense that the eigenvectors of the covariance matrix are the proper solutions to the portfolio optimization problem, and have moved ahead with this idea even though the rationale for doing so is not clear.

* Can also get these results using the Euler condition -> equations of motion. Acceleration is proportional to the gradient of the potential energy

\begin{equation}
\nabla_{\boldsymbol{\mu}} \mathcal{L} = \frac{d}{dt} \nabla_{\dot{\boldsymbol{\mu}}} \mathcal{L}
\end{equation}

\begin{equation}
\nabla \sigma = \lambda \ddot{\boldsymbol{\mu}}
\end{equation}

* Might also formulate as a synergy max problem (using correlation matrix)












* Negative weights problem. Different approaches: linear combination of portfolios; scaling of eigenvectors; shrinkage estimate; lastly, figure out a practical interpretation of negative weights (and/or get comfortable with complex weights).


* No problem if not invertible

* Connection with my PCA method of deducing eigenvectors

* Can explore $\Delta u / \Delta l$ or $\Delta u / \Delta(investment)$ instead of $\Delta u / \Delta t$

* "Imaginary" (cyclic) interest rate -- log spiral

* The data by which to calculate the covariance matrix are non-existent. Elsewhere I have shown how the leading eigenvectors of the project covariance matrix effectively encode each project's contribution to distinct policy objectives (the principle component scores); and how these can be deduced from expert knowledge when data are not available [@]. The approximate covariance matrix is approximate in the sense... but insofar as it filters out noise, may actually be more accurate with respect to the true process that generates the data. Better to call it the filtered covariance matrix, then.  The eigenvectors of $K$ are just the eigenvectors of the covariance matrix premultiplied by a diagonal matrix of the inverse means $\mu_k$. ...This also precludes the problem of using too small an eigenvalue. ...It would not be possible to use this approch in the conventional approach as that way involves inverting the covariance matrix, which would no be possible since the filtered covariance matrix has $n - m$ eigenvalues equal to zero. 

* If expected growth rates differ, then positive weights cannot be guaranteed. Likewise, time horizons can differ from project to project, but this may introduce negative budget allocations.






























<!-- ## A menu of solutions catering to a range of strategic objectives -->
<!-- Generally speaking, each of the $m$ leading eigenvectors of a covariance matrix of a complex system of variables encode information about a specific aspect of the system. In the financial context, for example, Gopikrishnan et al. [-@gopik2001quant] show that each of the leading eigenvectors of a covariance matrix of hundreds of stock price series is associated with movements in a particular industry or sector, such that each element in the eigenvectors reflects a particular stock's contribution to, or participation in, movements in the respective sector. -->
<!-- [The authors even suggest this method might be used to classify firms into industries.] -->
<!-- Schiek extends the logic of Gopikrishnan et al.'s analysis to AR4D correlation matrices [-@schiek2021reverse]. In that extension, Schiek shows that the collective movements associated with each eigenvector may be interpreted as the AR4D institution's (and/or donor's) strategic objectives or policies, such that the elements of the eigenvectors may be interpreted as reflecting each research program's contribution towards the respective policy. -->
<!-- In technical language, this association between macro and micro level movements results because empirical collective movements (for e.g., sectorial movements) tend to align with the leading principal components of the system. The leading eigenvectors reflect the participation of individual variables (for e.g., stocks) in a particular collective movement because they are very nearly the covariances of variables ($X$) with leading principal components ($S$). -->
<!-- The exact covariances of variables with leading principal components ($\tilde{\Sigma}_{X, S}$) are given by -->
<!-- \begin{equation} -->
<!-- \tilde{\Sigma}_{X, S} = \tilde{P} \tilde{\Gamma} -->
<!-- \end{equation} -->
<!-- (Where the tilde "~" indicates that the expression is restricted to the $m$ leading eigenvector-eigenvalue pairs.) -->
<!-- Correlations between variables and leading principal components ($K_{X, S}$), it follows, are given by  -->
<!-- \begin{equation} -->
<!-- K_{X, S} = \tilde{P} \tilde{\Gamma}^{\frac{1}{2}} -->
<!-- \end{equation} -->
<!-- (See Schiek [-@schiek2021reverse] or Abdi [-@] for details.) -->
<!-- Pre-multiplying the correlations by an orthonormal (typically varimax) rotation matrix $B$ can be useful in clarifying which variables are associated with which collective movements. -->
<!-- <!-- (Note that this is done without loss of generality, since covariance matrices are defined up to an orthonormal rotation of their eigenvectors.) -->
<!-- \begin{equation} -->
<!-- K_{X, S}^{\circlearrowright} = B \tilde{P} \tilde{\Gamma}^{\frac{1}{2}} -->
<!-- \label{eq:Lrot} -->
<!-- \end{equation} -->
<!-- In the AR4D context, this may be interpreted as the matrix of correlations between projects and policies, and elicited from expert opinion as such [@schiek2021reverse]. This matrix will become very useful in the next section. -->
<!-- Because eigenvectors are associated with strategic objectives, by selecting a particular portfolio solution from the optimal menu, the investor effectively commits themselves to a particular strategic objective for the funding cycle. When selecting a budget allocation from the optimal menu, then, the investor must consider not only their risk tolerance, but also the strategic objective they wish to focus on for the funding cycle. -->
<!-- ## Multi-objective portfolio optimization -->
<!-- Alternatively, the investor may want to optimize across multiple strategic objectives. In other words, they may want an optimal linear combination of the solution menu. -->
<!-- \begin{equation} -->
<!-- \boldsymbol{\mu}^* = Q \boldsymbol{\beta}^* -->
<!-- \end{equation} -->
<!-- Where the optimal scaling parameters $\beta_j^*$ are defined as those which maximize policy impact subject to a policy risk tolerance. Formally, this problem can be stated as follows. -->
<!-- <!-- $\mathcal{E}^p = r^2/2\; \boldsymbol{\mu} {'} \boldsymbol{\mu}$ -->
<!-- <!-- $\sigma^p = \sqrt{\boldsymbol{\mu}^p ' G_{p,p} \boldsymbol{\mu}^p}$ -->
<!-- <!-- Which can then be solved for the optimal multi-objective budget allocation.  -->
<!-- \begin{equation} -->
<!-- \max_{\boldsymbol{\beta}} \mathcal{E}^p \:\:\:\:s.t.\:\:\: \sigma^p = \bar{\sigma}^p -->
<!-- \end{equation} -->
<!-- Where $\mathcal{E}^p$ and $\sigma^p$ are the policy vis viva and risk, defined as the policy level aggregation of the project level solution menu. -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \mathcal{E}^p &= \Sigma_{j = 1}^m \mathcal{E}_j^* \;\;;\;\;\; \mathcal{E}_j^* = \frac{1}{2} \mathbf{v}_j^* ' \mathbf{v}_j^* = \frac{r^2}{2} \boldsymbol{\mu}_j^* ' \boldsymbol{\mu}_j^* =  \frac{r^2}{2} \beta_j^2 \\ -->
<!-- &= \frac{r^2}{2} \boldsymbol{\beta} ' \boldsymbol{\beta} \\ -->
<!-- \sigma^p &= \mathbf{1} ' \Sigma^p \mathbf{1} = \boldsymbol{\beta} ' G^p \boldsymbol{\beta} -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- Where $G^p$ is the policy coefficient of covariation matrix, which can be deduced from the policy NPV covariance matrix $\Sigma^p$, which in turn is obtained as the inner product of the crowdsourced project-policy correlation matrix (Eq \ref{eq:Lrot}) with itself (see Schiek [-@schiek2021reverse] for details). -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- K_{X, S}^{\circlearrowright} ' K_{X, S}^{\circlearrowright} &= B \tilde{\Gamma} B ' \\ -->
<!-- &= \Sigma_{S,S} -->
<!-- \end{split} -->
<!-- \label{eq:SigSSeig} -->
<!-- \end{equation} -->
<!-- <!-- , and where the control variables $\boldsymbol{\mu}^p$ are the expected policy NPVs, whose elements are defined -->
<!-- <!-- \begin{equation} -->
<!-- <!-- \mu_j^p = \boldsymbol{\mu}_j^* ' \mathbf{1} -->
<!-- <!-- \end{equation} -->
<!-- <!-- But note that, since $\boldsymbol{\mu}_j^* = \mathbf{q}_j$, then $\boldsymbol{\mu}_j^* {'} \mathbf{1}$ equals the eigenvalue scaling constant $\beta_j$ (recall equation \ref{eq:wStarScaled}). Therefore, the control variables can be rewritten -->
<!-- <!-- \begin{equation} -->
<!-- <!-- \boldsymbol{\mu}^p = \boldsymbol{\beta} -->
<!-- <!-- \end{equation} -->
<!-- <!-- And so the policy optimization problem can be rewritten -->
<!-- <!-- \begin{equation} -->
<!-- <!-- \max_{\boldsymbol{\beta}} \mathcal{E}^p \:\:\:\:s.t.\:\:\: \sigma^p = \bar{\sigma}^p -->
<!-- <!-- \end{equation} -->
<!-- <!-- Where -->
<!-- <!-- \begin{equation} -->
<!-- <!-- \begin{split} -->
<!-- <!-- \mathcal{E}^p &= \frac{r^2}{2} \boldsymbol{\beta} ' \boldsymbol{\beta} \\ -->
<!-- <!-- \sigma^p &= \boldsymbol{\beta} ' G_{p,p} \boldsymbol{\beta} -->
<!-- <!-- \end{split} -->
<!-- <!-- \end{equation} -->
<!-- <!-- $K_{X, S}^{\circlearrowright} K_{X, S}^{\circlearrowright}{'}$ gives the approximate program correlation matrix $\tilde{K}_{x, x}$, while the inner product  gives the policy covariance matrix (call this $\Sigma_{S, S}^{\circlearrowright}$). In the absence of time series data by which to estimate program and policy correlations, then, the matrix $K_{X, S}^{\circlearrowright} = B \tilde{P}$ provides a means of deducing them from expert opinion. (See Schiek [-@schiek2021reverse] for details.) -->
<!-- This is just the project portfolio problem (equation \ref{eq:probStat}) stated at the policy level. The corresponding lagrangian is then -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \mathcal{L} &= \mathcal{E}^p - \lambda^p (\sigma^p - \bar{\sigma}^p) \\ -->
<!-- &= \frac{r^2}{2} \boldsymbol{\beta} ' \boldsymbol{\beta} - \lambda^p (\boldsymbol{\beta} ' G^p \boldsymbol{\beta} - \bar{\sigma}^p) -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- And the first order conditions then follow as -->
<!-- \begin{equation} -->
<!-- \nabla_{\boldsymbol{\beta}} \mathcal{L} = r^2 \boldsymbol{\beta} - \frac{\lambda^p}{\sigma^p} G^p \boldsymbol{\beta} = \mathbf{0} -->
<!-- \label{eq:focS} -->
<!-- \end{equation} -->
<!-- <!-- Which can be rearranged -->
<!-- <!-- \begin{equation} -->
<!-- <!-- \boldsymbol{\beta} - \frac{\lambda^S}{r^2 \sigma^S} G_{S, S} \boldsymbol{\beta} = \mathbf{0} -->
<!-- <!-- \end{equation} -->
<!-- The candidate solution $\boldsymbol{\beta}^*$ may thus be any of the eigenvectors of $G^p$, with corresponding policy risk shadow price ($\lambda^p$) given in terms of the respective eigenvalue. Note from equation \ref{eq:SigSSeig} that the matrix of eigenvectors of $\Sigma^p$ is just the orthonormal rotation matrix $B$ that is applied to clarify project-policy correlations; and the respective eigenvalues are just the $m$ leading eigenvalues of the project correlation matrix $K$ (or coefficient of covariation matrix $G$). (See Schiek [-@schiek2021reverse] for details.) -->
<!-- By definition, $G_{S, S} = D(\boldsymbol{\beta})^{-1} \Sigma_{S, S} D(\boldsymbol{\beta})^{-1}$; and so it follows that the eigenvalues of $G_{S, S}$ are likewise $\tilde{\Gamma}$. Combining this information with the first order conditions (equation \ref{eq:focS}), it follows that $\sigma_j^{p*} = \sqrt{\gamma_j} = \sigma_j^*$, and -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \lambda^p_j &= r^2 \frac{\sigma_j^{S*}}{\gamma_j} \\ -->
<!-- &= \frac{r^2}{\sqrt{\gamma_j}} -->
<!-- &= \lambda_j -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- That is, the optimal policy portfolio risk and risk shadow price are equal to the optimal project portfolio risk and risk shadow price. -->
<!-- Moreover, the eigenvectors of $G_{S, S}$ and $\Sigma_{S, S}$ are trivially related as follows. Letting $\boldsymbol{\varepsilon}_j$ and $\mathbf{b}_j$ stand for the $j^{th}$ eigenvectors of $G_{S, S}$ and $\Sigma_{S, S}^{\circlearrowright}$, respectively, -->
<!-- \begin{equation} -->
<!-- \boldsymbol{\varepsilon}_j = D(\boldsymbol{\beta})^{-1} \mathbf{b}_j -->
<!-- \end{equation} -->
<!-- But, when evaluated at the $j^{th}$ solution $\boldsymbol{\beta}_j^* = \boldsymbol{\varepsilon}_j$, this becomes -->
<!-- \begin{equation} -->
<!-- \boldsymbol{\varepsilon}_j = \mathbf{b}_j^{\frac{1}{2}} -->
<!-- \end{equation} -->
<!-- The $\ell^{th}$ candidate policy budget allocation $\boldsymbol{\beta}_{\ell}^*$ is thus given by -->
<!-- \begin{equation} -->
<!-- \boldsymbol{\beta}_{\ell}^* = \mathbf{b}_{\ell}^{\frac{1}{2}} \:\:;\:\:\: \ell \in (1, m) -->
<!-- \end{equation} -->
<!-- To avoid complex valued elements in $\boldsymbol{\beta}_{\ell}^*$, recall that negative elements in $\mathbf{b}_{\ell}$ can be made positive by adjusting the program time horizon, as explained in section \ref{sec:cycReal}. -->
<!-- Deduction of the second order condition for a maximum follows the same procedure as in the project level problem (section \ref{sec:soc}), and results in an analogous conclusion: the investor is presented not with a single optimal linear combination, but with an optimal menu of linear combinations, each corresponding to a particular portfolio risk tolerance (\sqrt{\gamma_{\ell}}). The investor should choose the solution $\boldsymbol{\beta}_{\ell}^*$ with the risk shadow price $\lambda_{\ell}$ that best matches their risk tolerance. -->
<!-- With the optimal policy weights ($\boldsymbol{\beta}_{\ell}^*$) in hand, the optimal multi-objective impact $\mathcal{E}_{\ell}^{p*}$ then follows as -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \mathcal{E}_{\ell}^{p*} &= \frac{r^2}{2} \boldsymbol{\beta}_{\ell}^* ' \boldsymbol{\beta}_{\ell}^* \\ -->
<!-- &= \mathbf{b}_{\ell}^{\frac{1}{2}} ' \mathbf{b}_{\ell}^{\frac{1}{2}} -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- <!-- &= \frac{r^2}{2} \boldsymbol{\mu}_{\ell}^* ' \boldsymbol{\mu}_{\ell}^* \\ -->
<!-- <!-- &= \frac{r^2}{2} \boldsymbol{\beta}_{\ell}^* Q ' Q \boldsymbol{\beta}_{\ell}^* \\ -->
<!-- The optimal multi-objective project NPV vector $\boldsymbol{\mu}_{\ell}^*$ follows as the optimal linear combination of the optimal project NPV menu. -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \boldsymbol{\mu}_{\ell}^* &= \Sigma_{j = 1}^m \mu_j^* \beta_{j \ell}^* \\ -->
<!-- \boldsymbol{\mu}_{\ell}^* &= Q \boldsymbol{\beta}_{\ell}^* \\ -->
<!-- &= D(\tilde{\mathbf{c}}) \tilde{P} \boldsymbol{\beta}_{\ell}^* -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- And the optimal multi-objective budget allocation is -->
<!-- \begin{equation} -->
<!-- \mathbf{w}_{\ell}^* = C (D(\tilde{\mathbf{x}}(0))^{-1} D(e^{-r \mathbf{T}}) \boldsymbol{\mu}_{\ell}^* )^{\frac{1}{\boldsymbol{\alpha}}} -->
<!-- \label{eq:wStarPolicy} -->
<!-- \end{equation} -->
<!-- Again, the elements in $\mathbf{T}$ are so defined that... (recall Eq \ref{eq:Tk}), complex valued budget allocations, negative elements in $\boldsymbol{\mu}_{\ell}^*$ can be made positive by adjusting the program time horizon, as explained in section \ref{sec:cycReal}. To enforce the budget constraint, the eigenvector matrix $B$ may be scaled by a parameter $\chi_{\ell}$ such that $\mathbf{w}_{\ell}^* {'} \mathbf{1} = C$. The  -->
<!-- \begin{equation} -->
<!-- \boldsymbol{\mu}_{\ell}^* = \chi_{\ell} D(\tilde{\mathbf{c}}) \tilde{P} \boldsymbol{\beta}_{\ell}^* -->
<!-- \end{equation} -->
<!-- Note that adjustments to $\chi_{\ell}$ also scale the optimal policy portfolio risk $\sigma^p$ and impact $\mathcal{E}^p$. -->
<!-- The CGIAR is said to "have a long history of good intentions but limited success in developing appropriate approaches for priority setting" [@Birner2016]. In this paper, I have attempted to reduce, in some measure, the methodological premise for this "limited success" by adaptating financial risk-adjusted portfolio optimization to the AR4D context. The proposed method can also be viewed as an AR4D tradoff/synergy assessment tool. -->
<!-- The particular set of equations that constitute the proposed mechanism may vary considerably with assumptions regarding the shape of the AR4D impact probability density and the derivation of the diminishing returns function. One might also explore what happens to optimal solutions if the mean portfolio return is replaced by the mode of the portfolio return. The hypothetical example given above takes place at the program level, but could have also been given at the project, or some other, level, so long as the proposals are scaleable. -->
<!-- Further pursuit of this inquiry implies a more nuanced approach to the ex-ante impact evaluation of individual AR4D proposals in Step 3 of Mills' resource allocation workflow. In particular, to be of real use in risk adjusted optimization of resource allocations, ex-ante impact assessment methods must focus on how AR4D proposal impacts _scale_ with investment, rather than the impact they might have at a given level of funding. They must also include a mature risk assessment method. -->
<!-- The AR4D investor's budget and return shadow prices are revealed as a byproduct of the method proposed here. These shadow prices essentially constitute the investor's bid price for research capital and impact, respectively. If publicized, they could thus help guide investors in search of suitable partners. More broadly, they might also serve to lay some of the demand side groundwork for an AR4D marketplace. At the same time, however, it is important to keep in mind that research proposals are _not_ stocks and bonds. Adaptation of financial tools to the AR4D context requires an open mind, unconditioned by financial tropes. -->
<!-- *Exploration of the risk adjusted portfolio optimization apparatus introduced in this article suggests that the shape of the risk-reward frontier and budget allocation can vary widely from one set of input values to another. In the hypothetical example above, I arbitrarily set proposal risk to be more or less proportional to proposal return. The shape and extent of the solution frontier vary considerably when risk is assigned in a more random fashion. -->
<!-- In particular, I have introduced adjustments to the existing portfolio optimization method so as to accommodate the diminishing returns, lack of data, and strictly positive budget shares that distinguish the AR4D context from the financial context. -->
<!-- In AR4D resource allocation procedures there arises what is known in game theory as a "credible commitment problem" -->
<!-- The problem is not unique to the AR4D context. Researchers in physics and medicine, in particular,   is faced by research institutes across disciplines and scales. -->
<!-- * May not be exactly "accurate", but generates an optimal solution that is consistent with what we think we know about the domain. Moreover, it should kept in mind that data are not infallible sources of information. Data of the sort considered here, in particular, are inherently noisy. Under such circumstances, arguably, it may be preferible to build up the covariance matrix from crowdsourced domain knowledge, in the manner shown above, rather than calculate it from data. Hence, domain knowledge of the key dimensions of the problem space, and of how the various moving parts of the problem (i.e. the portfolio items) map onto these dimensions, is a sufficient---and, in noisy contexts, possibly superior---substitute for data. -->
<!-- "long history of good intentions but limited success in developing appropriate approaches for priority setting". An historic effort begun in 2011 to restructure CGIAR financing around Core Research Programs (CRPs) was ad hoc, thereby reinforcing the very institutional inertia it was designed to disrupt [@Birner2016]. ReDonor attempts to establish criteria and methods for a merit-based allocation of ever-scarcer research funds are seen as invasive characterized as  -->
<!-- "One of the geniuses" of such institutes is said to be their ability to consistently attract "aid funding of long-term research" while keeping "aid professionals from setting research agendas and hiring scientists" (McCalla, 2014). Donor attempts to establish criteria and methods for a merit-based allocation of ever-scarcer research funds are characterized as -->
<!-- "development at the expense of research" (Birner &amp; Byerlee, 2016) or even the "Balkanization" of research (Petsko, 2011) . -->
<!-- ## Separating signals from noise, identifying main assets contributing to market movements -->
<!-- MV analysis is, in many ways, still a work in progress, with unresolved methodological issues severely limiting its usefulness even in its native financial context. In particular, covariance matrices estimated from price data tend to contain a lot of noise; and the optimal solutions then tend to diverge considerably from reality as a result of this [@michaud1989markowitz]. In my adaptation below, I propose a new approach to this problem by first dimensionally reducing the portfolio from several dozen assets to just a handful of "signals" constructed from the eigenvectors of the correlation matrix that can be meaningfully distinguished from noise. -->
\pagebreak

# References {-}

<div id="refs"></div>

# Appendix: Securities appearing in the financial example {-}

# Figures and tables

Figure \ref{fig2} is generated using an R chunk.

```{r fig2, fig.width = 5, fig.height = 5, fig.align='center', out.width="50%", fig.cap = "\\label{fig2}A meaningless scatterplot.", echo = FALSE}
plot(runif(25), runif(25))
```

# Tables coming from R

Tables can also be generated using R chunks, as shown in Table \ref{tab1} for example.

```{r tab1, echo = TRUE}
knitr::kable(head(mtcars)[,1:4], 
             caption = "\\label{tab1}Caption centered above table"
)
```

# References {-}
