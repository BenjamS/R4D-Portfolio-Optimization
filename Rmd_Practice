---
title: Risk-adjusted optimal balancing of investments across staple crop research
  programs
author:
  - name: Ben Schiek
    email: b.schiek@cgiar.org
    affiliation: International Center for Tropical Agriculture (CIAT)
address:
  - code: International Center for Tropical Agriculture
    address: Km 17 Recta Cali-Palmira, Valle del Cauca, Colombia
abstract: |
  Conventional priority setting exercises generate valuable discussion and stakeholder consensus around individual agricultural research proposals, but a subsequent, currently missing, step is necessary to translate these exercises into final budget shares optimally allocated across a portfolio of several such proposals. The continued absence of a rigorous assessment tool at the portfolio level, combined with increasingly austere financial outlooks, has contributed to deteriorating relations between donors and research organizations.  Here I explore the possibility of adapting Mean-Variance Analysis, a portfolio optimization technique developed in stock market contexts, to the agricultural research portfolio optimization problem. In the process, I present new approaches to the old problems of negative budget weights and inaccuracy due to noisy data. I walk through an example application using stock market data, and then an example in the agricultural research context using FAO farmgate price data for staple crops. The adapted tool also addresses risk.    left research budget allocation processes vulnerable to undue political pressures, opaque, ad hoc procedures, institutional inertia, and other forms of subjective whim. This, in turn, further aggravated by increasingly austere fiscal outlooks, has contributed to unprecedented levels of toxicity in relations between donors and research institutes.
  
  The urgency of filling this methodological gap was recognized more than twenty years ago by Bradford Mills. However, attention has instead focused on the development of individual project ex-ante impact assessment tools. The continued absence of a rigorous assessment tool at the portfolio level has left research budget allocation processes vulnerable to undue political pressures, ad hoc procedures, institutional inertia, and other forms of subjective whim. This, in turn, situation, further aggravated by increasingly austere fiscal outlooks, has contributed to unprecedented levels of toxicity in relations between donors and research institutes.  making it increasingly difficult to devise a budget appropriation acceptable to all stakeholders. keywords: "Ex-ante impact assessment, Risk-adjusted portfolio optmization, Mean-Variance Analysis, agricultural research for development"
journal: "European Journal of Operational Research"
date: "`r Sys.Date()`"
bibliography: R4D Portfolio Optimization.bib
biblio-style: apalike
#linenumbers: true
#numbersections: true
output:
  rticles::elsevier_article:
    latex_engine: xelatex
mainfont: Garamond
---
<!-- https://tug.org/FontCatalogue/ -->
<!-- Source Serif Pro Light -->
<!-- rmarkdown::draft("Test.Rmd", template = "elsevier_article", package = "rticles") -->


# Introduction

Agricultural research for development (AR4D) fund allocation processes are increasingly fraught with contentious choices between many worthwhile alternatives. How to fairly and optimally allocate an ever-shrinking budget across a portfolio of research programs that are all, in one way or another, vitally important? Here I examine the possibility of adapting portfolio optimization tools developed in the financial context in order to address this issue.

In particular, I walk through an adaptation of Mean-Variance (MV) Analysis, first conceptualized by Markowitz () and later formalized by Merton (). The principal outputs of MV Analysis are 1) the MV frontier, which indicates the expected portfolio return associated with any level of risk tolerance, and 2) the vector of budget shares that must be allocated to each portfolio item in order to achieve a given risk-reward position on the MV frontier.

It is worth emphasizing from the outset that this goes far beyond the "priority setting exercises", with which many donors, development agencies, and AR4D institutions may be familiar. Whereas priority setting exercises typically culminate in the ranking of research alternatives into research priorities, the final output of the method proposed here are the precise budget shares that must be allocated to each research program in order to maximize portfolio net benefit, given a certain risk tolerance specified by the donor. The explicit accounting of risk involved in MV Analysis also merits special emphasis, as risk accounting in the AR4D context is, by comparison, quite rudimentary.

After reviewing some of the cultural and methodological issues that motivate and constrain this work, I first walk through an application of MV Analysis in its native financial context using daily price data downloaded from yahoo finance. Then, I move to an application in the AR4D context using yearly farmgate price data for staple crops downloaded from the Food and Agriculture Organization (FAO), for various geographical regions.

In both applications, I first decompose the data into a loadings matrix in order to isolate and interpret key trends or "signals" in the data. I separate signals from noise using an old but little-utilized technique developed in the study of physical systems. Once signals have been isolated, they are interpreted in concrete terms based on how much the different portfolio items load onto them. A varimax rotation of the loadings matrix turns out to be of great assistance in this task. The interpretation of the signals is then further confirmed and illustrated by constructing the signal time series and then plotting them against their respective highest loading price series. The highest loading price series are seen to track their respective signals closely.

This exploratory market analysis is a quick and inexpensive way to examine how prices are interacting with each other, and to identify which price series appear to be moving the market. This is important information to take into consideration could serve to inform and orient policy discussions, and to prepare the ground for the subsequent question of optimal funding allocations is addressed. Given enough data, it could also be used to 

The AR4D and financial planning contexts differ in many important ways. The AR4D donor wants the price of staples to go down.

MV analysis is, in many ways, still a work in progress.
Negative weights... A number of unresolved methodological issues severely limit the usefulness of MV Analysis. In a nutshell, noise in the data makes accurate estimation of the returns vector and correlations matrix difficult.

IMPACT future backtest


There is an increasingly toxic environment between research institutions and donors. Much of this is arguably attributable to current priority setting practices, which leave plenty of room for politics, institutional inertia, and other forms of subjective whim to creep in and exert undue influence over fund allocation decisions. The MV Analysis adaption may contribute towards reducing that 

I am not convinced that the MV Analysis adaptation is a perfect fit. But I am convinced and hope to convince others that this adaptation is better than the often ad hoc and/or politically driven processes in place now.  could go a long way towards inoculating AR4D decision making pipelines from some of the more egregious ad hoc and/or politically driven tendencies exerting an undue influence over the process today. The proposed method forces decisionmakers to think carefully about how funding aligns with AR4D pro-poor mandates, geographic focus, etc., and increases transparency around these issues. These tools also bring risk into the picture.

The aim of investment here is not merely to share in the fortunes of the portfolio items, but rather to play a role in their shaping. High commodity price is an indication of demand outstripping supply. The aim of the CBOE speculator is to prosper from that scarcity and to aggravate it by removing from circulation as much of the commodity as possible, thereby raising the price further, the aim of the AR4D donor is the exact opposite of this. The donor invests in AR4D so as to increase production and quality of the crops and thereby to bring down their price.

Interpretation of "price" is different from the financial context. This could reflect 1) higher kcals per unit weight, 2) higher quality kcals, 3) higher demand, or 4) some combination of all of the above.


```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = FALSE,
    message = FALSE,
    warning = FALSE

)

```


```{r, fig.align='center', echo=FALSE}

#fig.width=14, fig.height=14,

#setwd("D:/OneDrive - CGIAR/Documents")
#options(warn = -1); options(scipen = 999)
#-------------------------------------------------------------
#devtools::install_github("thomasp85/patchwork")
library(plyr)
# library(tidyverse)
# library(ggplot2)
library(zoo)
# library(FactoMineR)
# library(factoextra)
# library(Hmisc)
# library(corrplot)
library(tidyquant)
library(patchwork)
#=======================================================================
# Resources:
# Correlation matrices:
# http://www.sthda.com/english/wiki/correlation-matrix-a-quick-start-guide-to-analyze-format-and-visualize-a-correlation-matrix-using-r-software
# https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html
#=======================================================================
# Define functions
#=======================================================================
signals_from_noise <- function(mat_pctDiff,
                               fig_title_eigDens = NULL,
                               fun_env = NULL){
  
  if(is.null(fun_env)){
    eigenvalue_density_plot = T
    pca_var_plot = T
    pca_ind_plot = T
    group_info = NULL
    quietly = F
  }else{
    eigenvalue_density_plot = fun_env[[1]]
    pca_var_plot = fun_env[[2]]
    pca_ind_plot = fun_env[[3]]
    group_info = fun_env[[4]]
    quietly = fun_env[[5]]
    
  }
  #---------------------------------------------------------
  # Separate signals from noise
  #---------------------------------------------------------
  res <- FactoMineR::PCA(mat_pctDiff, ncp = ncol(mat_pctDiff), graph = F)
  eigvals <- as.data.frame(res$eig)$eigenvalue
  eigval_max <- max(eigvals)
  mat_loads <- res$var$coord
  mat_loads_rot <- varimax(mat_loads)[[1]]
  mat_eigvecs <- mat_loads %*% diag(1 / sqrt(eigvals))
  #---------------------------------------------------------
  # Apply random matrix theory () to determine eigenvalue distribution of a 
  # correlation matrix of random data.
  n_obs <- nrow(mat_pctDiff)
  n_items <- ncol(mat_pctDiff)
  Q <- n_obs / n_items
  s_sq <- 1 - eigval_max / n_items
  #s_sq <- 1
  eigvals_rand_max <- s_sq * (1 + 1 / Q + 2 / sqrt(Q))
  eigvals_rand_min <- s_sq * (1 + 1 / Q - 2 / sqrt(Q))
  eigvals_rand <- seq(eigvals_rand_min, eigvals_rand_max, length.out = n_items)
  eigvals_rand_density <- Q / (2 * pi * s_sq) * sqrt((eigvals_rand_max - eigvals_rand) * (eigvals_rand - eigvals_rand_min)) / eigvals_rand
  #---------------------------------------------------------
  # Plot eigenvalue density vs. random matrix eigenvalue density
  df_plot_data <- data.frame(Eigenvalue = eigvals, Type = "Data")
  df_plot_rand <- data.frame(Eigenvalue = eigvals_rand, Type = "Random")
  df_plot <- rbind(df_plot_data, df_plot_rand)
  
  if(eigenvalue_density_plot){
    if(is.null(fig_title_eigDens)){fig_title_eigDens <- "Eigenvalue density"}
    gg <- ggplot(df_plot, aes(x = Eigenvalue, fill = Type))
    gg <- gg + geom_density(alpha = .3)
    gg <- gg + theme(axis.title.y = element_blank(),
                     axis.text.y = element_blank(),
                     axis.title.x = element_text(size = 10),
                     plot.caption = element_text(size = 10, hjust = 0),
                     legend.title = element_blank())
    gg <- gg + labs(caption = fig_title_eigDens)
    print(gg)
    
    # gg <- ggplot()
    # gg <- gg + geom_density(data = df_plot, aes(x = Eigenvalues, color = "Correlation Matrix"), lwd = 1.1)
    # gg <- gg + geom_line(data = data.frame(x = eigvals_rand, y = eigvals_rand_density), aes(x = x, y = y, color = "Random matrix"), lwd = 1.1)
    # gg <- gg + scale_colour_manual(name = "density", 
    #                                values = c(`Correlation Matrix` = "blue", `Random matrix` = "magenta"))
  }
  #---------------------------------------------------------
  # Which data eigenvalues can be meaningfully distinguished from noise?
  ind_deviating_from_noise <- which(eigvals > eigvals_rand_max) # (eigvals_rand_max + 5 * 10^-1))
  #---------------------------------------------------------
  # Extract signal loadings matrix from noise
  mat_loads_sig <- mat_loads[, ind_deviating_from_noise]
  eigvals_sig <- eigvals[ind_deviating_from_noise]
  mat_loads_rot_sig <- mat_loads_rot[, ind_deviating_from_noise]
  #---------------------------------------------------------
  n_signals <- length(eigvals_sig)
  if(!quietly){print(paste("Number of signals: ", n_signals))}
  #---------------------------------------------------------
  # Get dimensionally reduced version of original input data
  mat_eigvecs_sig <- mat_eigvecs[, ind_deviating_from_noise]
  mat_inData_sig <- mat_pctDiff %*% mat_eigvecs_sig
  if(n_signals == 1){
    mat_inData_sig <- mat_inData_sig / eigvals_sig
  }else{
    mat_inData_sig <- mat_inData_sig %*% diag(1 / eigvals_sig)
  }
  #---------------------------------------------------------
  # Set sign of eigenvectors such that they
  # best conform to the input time series
  inData_avg <- rowMeans(mat_pctDiff)
  # if(n_signals == 1){
  #   mse <- mean((mat_inData_sig - inData_avg)^2)
  #   mse_neg <- mean((-mat_inData_sig - inData_avg)^2)
  #   if(mse_neg < mse){
  #     mat_eigvecs <- -mat_eigvecs
  #     mat_inData_sig <- -mat_inData_sig
  #     mat_loads_rot_sig <- -mat_loads_rot_sig
  #   }
  # }else{
  #   for(i in 1:n_signals){
  #     mse <- mean((mat_inData_sig[, i] - inData_avg)^2)
  #     mse_neg <- mean((-mat_inData_sig[, i] - inData_avg)^2)
  #     if(mse_neg < mse){
  #       mat_eigvecs_sig[, i] <- -mat_eigvecs_sig[, i]
  #       mat_inData_sig[, i] <- -mat_inData_sig[, i]
  #       mat_loads_rot_sig[, i] <- -mat_loads_rot_sig[, i]
  #     }
  #   }
  #   
  # }
  
  #---------------------------------------------------------
  # PCA cluster plots to examine natural grouping in the data
  #---------------------------------------------------------
  # By variable
  if(pca_var_plot){
    if(n_signals == 1){
      print("Only 1 signal, can't do PCA plots.")
    }else{
      gg <- factoextra::fviz_pca_var(res, habillage = factor(group_vec))
      print(gg)
    }
    
  }
  #---------------------------------------------------------
  # By individual
  if(pca_ind_plot){
    if(n_signals == 1){
      print("Only 1 signal, can't do PCA plots.")
    }else{
      res <- FactoMineR::PCA(t(mat_pctDiff), graph = F)
      gg <- factoextra::fviz_pca_ind(res, habillage = factor(group_vec), addEllipses = T)
      print(gg)
    }
    
  }
  #---------------------------------------------------------
  # Cluster plot using Mclust()
  # mc <- mclust::Mclust(t(mat_pctDiff))
  # summary(mc)
  # View(mc$classification)
  # factoextra::fviz_cluster(mc, frame.type = "norm", geom = "text")
  #---------------------------------------------------------
  
  
  list_out <- list(mat_loads_sig, mat_loads_rot_sig, mat_loads, mat_loads_rot, mat_inData_sig, eigvals_sig, mat_eigvecs_sig, eigvals, mat_eigvecs)
  return(list_out)
}
#=======================================================================
plot_signals_against_avg <- function(mat_inData_sig, mat_inData,
                                     fig_title = NULL, facet_ncol = 1){
  # (mat_inData = mat_pctDiff)
  #---------------------------------------------------------
  # Dimensionally reduced plot of data (signal plots)
  #---------------------------------------------------------
  n_signals <- ncol(mat_inData_sig)
  if(is.null(fig_title)){fig_title <- "Signals"}
  #---------------------------------------------------------
  # Plot signal data against average
  inData_avg <- rowMeans(mat_inData)
  date_vec <- row.names(mat_inData)
  df_plot1 <- data.frame(Date = date_vec, inData_avg)
  df_plot2 <- data.frame(Date = date_vec, mat_inData_sig)
  df_plot1$Date <- factor(format(df_plot1$Date, format = "%y-%m-%d%"), ordered = T)
  df_plot2$Date <- factor(format(df_plot2$Date, format = "%y-%m-%d%"), ordered = T)
  xAxis_labels <- df_plot1$Date[seq(1, nrow(df_plot1), length.out = 5)]
  signal_id <- paste("Signal", c(1:n_signals))
  colnames(df_plot2)[2:(n_signals + 1)] <- signal_id
  gathercols <- signal_id
  df_plot2 <- df_plot2 %>% gather_("Signal", "Value", gathercols)
  gg <- ggplot()
  gg <- gg + geom_line(data = df_plot1, aes(x = Date, y = inData_avg, group = 1), color = "orange", lwd = 2)
  gg <- gg + geom_line(data = df_plot2, aes(x = Date, y = Value, group = 1))
  gg <- gg + scale_x_discrete(breaks = xAxis_labels)
  gg <- gg + facet_wrap(~ Signal, ncol = facet_ncol)
  gg <- gg + theme(axis.title.y = element_blank(),
                   #axis.text.x = element_text(angle = 60, hjust = 1),
                   plot.caption = element_text(size = 10, hjust = 0)
  )
  gg <- gg + labs(caption = fig_title)
  print(gg)
  
  
}
#=======================================================================
interpret_loadings <- function(mat_loads_rot_sig, fig_title = NULL, fun_env = NULL){
  #---------------------------------------------------------
  if(is.null(fig_title)){fig_title = "Each item's contribution to each signal"}
  if(is.null(fun_env)){
    group_info = NULL
    signal_names = NULL
    group_colors = NULL
  }else{
    group_info = fun_env[[1]]
    signal_names = fun_env[[2]]
    group_colors = fun_env[[3]]
  }
  #---------------------------------------------------------
  # Handle case where there's just 1 signal
  # (In such cases, mat_loads_rot_sig will be of class "numeric")
  if(class(mat_loads_rot_sig) == "numeric"){
    n_items <- length(mat_loads_rot_sig)
    n_signals <- 1
    varNames_ordered <- names(mat_loads_rot_sig)
  }
  if(class(mat_loads_rot_sig) == "matrix"){
    n_items <- nrow(mat_loads_rot_sig)
    n_signals <- ncol(mat_loads_rot_sig)
    varNames_ordered <- row.names(mat_loads_rot_sig)
  }
  #------------------------------------------------------------
  # Plot loadings barcharts
  df_plot <- data.frame(id = varNames_ordered, mat_loads_rot_sig)
  #--------------
  # Name the signals, if names provided
  if(is.null(signal_names)){
    signal_id <- paste("Signal", c(1:n_signals))
  }else{
    signal_id <- signal_names
  }
  #--------------
  colnames(df_plot)[2:(n_signals + 1)] <- signal_id
  gathercols <- as.character(signal_id) 
  df_plot <- gather_(df_plot, "Signal", "Loading", gathercols)
  df_plot <- transform(df_plot,
                       Signal = factor(Signal, levels = gathercols))
  #--------------
  # Group the vars if group info is provided
  if(!is.null(group_info)){
    list_groups <- group_info[[1]]
    group_names <- group_info[[2]]
    group_vec <- rep(NA, n_items)
    for(i in 1:length(list_groups)){
      this_group_vec <- list_groups[[i]]
      this_group_name <- group_names[i]
      group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
    }
    #--------------
    df_plot$Type <- factor(group_vec)
    xx <- df_plot$Type
    df_plot$id <- factor(df_plot$id, levels = unique(df_plot$id[order(xx)]))
    gg <- ggplot(df_plot, aes(x = id, y = Loading, fill = Type))
    gg <- gg + scale_fill_manual(values = group_colors)
  }else{
    gg <- ggplot(df_plot, aes(x = id, y = Loading))
  }
  gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
  gg <- gg + facet_wrap(~ Signal, nrow = 1)
  gg <- gg + theme(axis.text.y = element_text(face = "bold", size = 10),
                   axis.text.x = element_text(face = "bold", size = 10),
                   axis.title.y = element_blank(),
                   axis.title.x = element_text(face = "bold", size = 10),
                   plot.caption = element_text(size = 10, hjust = 0))
  gg <- gg + labs(caption = fig_title)
  gg <- gg + coord_equal()
  gg <- gg + coord_flip()
  print(gg)
  
}
#=======================================================================

historical_returns_and_corr_plot <- function(mat_pctDiff,
                                             mat_pctDiff_test = NULL,
                                             group_info = NULL,
                                             returns_plot = F,
                                             corr_plot = F,
                                             group_colors = NULL,
                                             fig_title_returns = NULL,
                                             fig_title_corrplot = NULL,
                                             fig_title_corrplot_test = NULL,
                                             returns_plot_range = NULL,
                                             corrplot_options = list(
                                               plot_with_pvals = F,
                                               plot_with_corrCoefs = F,
                                               corr_coef_size = 0.75)
){
  #------------------------------------------------------------
  #if(is.null(fig_title_returns)){fig_title_returns <- "Historical Returns"}
  #------------------------------------------------------------
  # Group the vars if group info is provided
  if(!is.null(group_info)){
    if(is.null(group_colors)){
      group_colors <- randomcoloR::distinctColorPalette(k = length(group_names))
      #group_colors <- viridis::viridis_pal(option = "D")(length(group_names))
    }
    n_groups <- length(list_groups)
    n_items <- ncol(mat_pctDiff)
    varNames_ordered <- colnames(mat_pctDiff)
    list_groups <- group_info[[1]]
    group_names <- group_info[[2]]
    group_vec <- rep(NA, n_items)
    group_color_vec <- rep(NA, n_items)
    for(i in 1:n_groups){
      this_group_vec <- list_groups[[i]]
      this_group_name <- group_names[i]
      this_group_color <- group_colors[i]
      group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
      group_color_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_color
    }
    xx <- factor(group_vec)
    cols_ordered_by_group <- as.character(colnames(mat_pctDiff)[order(xx)])
    group_color_vec <- group_color_vec[order(xx)]
  }
  
  
  # for(i in 1:length(list_groups)){
  #   this_group_vec <- list_groups[[i]]
  #   this_group_color <- group_colors[i]
  #   label_colors[which(cols_ordered_by_group %in% this_group_vec)] <- this_group_color
  # }
  
  
  #------------------------------------------------------------
  from_date <- row.names(mat_pctDiff)[1]
  to_date <- row.names(mat_pctDiff)[nrow(mat_pctDiff)]
  from_date <- gsub("-", "/", from_date)
  to_date <- gsub("-", "/", to_date)
  date_interval <- paste(from_date, to_date, sep = " - ")
  fig_subtitle_returns <- date_interval
  nab_pctRet <- apply(mat_pctDiff, 2, function(x) prod(1 + x)) - 1
  #------------------------------------------------------------
  df_plot <- data.frame(Returns = nab_pctRet)
  df_plot$id <- row.names(df_plot)
  if(!is.null(group_info)){
    df_plot$Type <- factor(group_vec)
    xx <- df_plot$Type
    df_plot$id <- factor(df_plot$id, levels = cols_ordered_by_group)
  }
  #------------------------------------------------------------
  if(!is.null(mat_pctDiff_test)){
    #------
    data_type_train <- paste("Train data ", date_interval)
    df_plot$Data <- data_type_train
    #------
    from_date <- row.names(mat_pctDiff_test)[1]
    to_date <- row.names(mat_pctDiff_test)[nrow(mat_pctDiff_test)]
    from_date <- gsub("-", "/", from_date)
    to_date <- gsub("-", "/", to_date)
    date_interval <- paste(from_date, to_date, sep = " - ")
    data_type_test <- paste("Backtest data ", date_interval)
    nab_pctRet_test <- apply(mat_pctDiff_test, 2, function(x) prod(1 + x)) - 1
    #------------------------------------------------------------
    df_plot_test <- data.frame(Returns = nab_pctRet_test, Data = data_type_test)
    df_plot_test$id <- row.names(df_plot_test)
    if(!is.null(group_info)){
      df_plot_test$Type <- factor(group_vec)
      xx <- df_plot_test$Type
      df_plot_test$id <- factor(df_plot_test$id, levels = cols_ordered_by_group)
    }
    df_plot <- as.data.frame(rbind(df_plot, df_plot_test))
    
    df_plot$Data <- factor(df_plot$Data, levels = c(data_type_train, data_type_test))
    
  }
  
  #------------------------------------------------------------
  # Historical returns plot
  if(returns_plot){
    
    if(!is.null(group_info)){
      gg <- ggplot(df_plot, aes(x = id, y = Returns, fill = Type))
      #gg <- gg + scale_color_brewer(palette = "Dark2")
      gg <- gg + scale_fill_manual(values = group_colors)
    }else{
      gg <- ggplot(df_plot, aes(x = id, y = Returns))
    }
    gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
    if(!is.null(mat_pctDiff_test)){
      gg <- gg + facet_wrap(~Data, ncol = 1)
      gg <- gg + labs(title = fig_title_returns)
    }else{
      gg <- gg + labs(title = fig_title_returns, subtitle = fig_subtitle_returns)
    }
    gg <- gg + theme(axis.text.x = element_text(angle = 60, hjust = 1),
                     #axis.text.x = element_text(face = "bold", size = 10, angle = 60, hjust = 1),
                     #axis.text.y = element_text(face = "bold", size = 10),
                     axis.title.x = element_blank(),
                     #axis.title.y = element_text(face = "bold", size = 10),
                     #plot.title = element_text(size = 11)
                     )
    if(!is.null(returns_plot_range)){
      gg <- gg + coord_cartesian(ylim = returns_plot_range)
    }
    #  gg <- gg + coord_equal()
    #  gg <- gg + coord_flip()
    print(gg)
    
  }
  #------------------------------------------------------------
  # Correlation matrix plot
  if(corr_plot){
    #if(is.null(fig_title_corrplot)){fig_title_corrplot <- "Correlation matrix"}
    plot_with_pvals <- corrplot_options[["plot_with_pvals"]]
    plot_with_corrCoefs <- corrplot_options[["plot_with_corrCoefs"]]
    corr_coef_size <- corrplot_options[["corr_coef_size"]]
    if(!is.null(group_info)){
      mat_pctDiff_corrplot <- mat_pctDiff[, cols_ordered_by_group]
      if(!is.null(mat_pctDiff_test)){
        mat_pctDiff_corrplot_test <- mat_pctDiff_test[, cols_ordered_by_group]
      }else{
        mat_pctDiff_corrplot_test <- NULL
      }
      
    }else{
      mat_pctDiff_corrplot <- mat_pctDiff
      if(!is.null(mat_pctDiff_test)){
        mat_pctDiff_corrplot_test <- mat_pctDiff_test
      }else{
        mat_pctDiff_corrplot_test <- NULL
      }
      
      group_color_vec <- "black"
    }
    #-------------------
    corr_colorRamp <- colorRampPalette(c("orange", "white", "deepskyblue"))(50)
    #-------------------
    xx <- Hmisc::rcorr(mat_pctDiff_corrplot)
    cormat <- xx$r
    if(plot_with_pvals){
      pvals <- xx$P
    }else{
      pvals <- NULL
    }
    
    if(plot_with_corrCoefs){
      corrplot::corrplot.mixed(cormat, tl.pos = "lt", tl.col = group_color_vec,
                               tl.srt = 45, p.mat = pvals, number.cex = corr_coef_size,
                               title = fig_title_corrplot,
                               col = corr_colorRamp)
    }else{
      corrplot::corrplot(cormat, type = "lower", tl.col = group_color_vec, tl.srt = 45,
                         p.mat = pvals, title = fig_title_corrplot,
                         col = corr_colorRamp)
    }
    #-------------------
    if(!is.null(mat_pctDiff_corrplot_test)){
      # if(is.null(fig_title_corrplot_test)){
      #   fig_title_corrplot_test <- "Correlation matrix, test data"
      # }
      xx <- Hmisc::rcorr(mat_pctDiff_corrplot_test)
      cormat <- xx$r
      if(plot_with_pvals){
        pvals <- xx$P
      }else{
        pvals <- NULL
      }
      if(plot_with_corrCoefs){
        corrplot::corrplot.mixed(cormat, tl.pos = "lt", tl.col = group_color_vec,
                                 tl.srt = 45, p.mat = pvals, number.cex = corr_coef_size,
                                 title = fig_title_corrplot_test,
                                 col = corr_colorRamp)
      }else{
        corrplot::corrplot(cormat, type = "lower", tl.col = group_color_vec, tl.srt = 45,
                           p.mat = pvals, title = fig_title_corrplot_test,
                           col = corr_colorRamp)
      }
      
    }
    
  }
  #------------------------------------------------------------
  
}
#=======================================================================
# End function definition
#=======================================================================
#=======================================================================

spy_sector_symbs <- c("XLF", "XLC", "XLY", "XLP", "XLV", "XLK", "RWR",
                      "XLU", "XLI", "XBI", "IYT") #"TTEK"
spy_sector_detail <- c("Financials", "Communications", "Luxury goods", "Staple goods",
                       "Healthcare", "Technology", "Real estate", "Utilities", "Industrial",
                       "Biotechnology", "Transportation") #"Gov. foreign aid"
minerals_symbs <- c("GLD", "SLV", "PPLT", "JJC", "JJM") #"XME"
minerals_detail <- c("Gold", "Silver", "Platinum", "Copper", "Industrial metals") #"US metals and mining"
agriculture_symbs <- c("JJG", "BAL", "SOYB", "SGG", "JO", "NIB", "DBA")
agriculture_detail <- c("Grains", "Cotton", "Soybean", "Sugar", "Coffee", "Cacao", "General agriculture")
energy_symbs <- c("WTI", "FUE", "WOOD", "ICLN", "KOL", "UNG")
energy_detail <- c("Oil (W&T Offshore Inc.)", "Biofuels", "Timber", "Clean energy", "Coal", "US natural gas")
#currency_symbs <- c("EMLC", "UUP", "FXE", "FXY", "FXF", "FXC", "FXB", "FXA")
#currency_detail <- c("Emerging mkt currencies", "USD", "EUR", "JPY", "CHF", "CND", "GBP", "AUD")
currency_symbs <- c("EURUSD=X", "JPY=X", "CHF=X", "CAD=X",
                    "GBPUSD=X", "AUDUSD=X", "INR=X")
currency_detail <- c("EUR/USD", "USD/JPY",
                     "USD/CHF", "USD/CAD", "GBP/USD", "AUD/USD", "USD/INR")
emerg_mkt_symbs <- c("EMLC", "ELD", "BKF", "VWOB")
emerg_mkt_detail <- c("Emerg mkts debt", "Emerg mkts gov. bonds", "BRIC countries", "Emerging mkt currencies")
crypto_symbs <- c("BLOK", "LEGR", "BCNA")
crypto_detail <- c("Blockchain tech.", "Blockchain companies", "Blockchain in China")
Tbond_symbs <- c("IEI", "IEF", "TLT")#, "BIL"
Tbond_detail <- c("T-bond 3-7 yrs", "T-bond 7-10 yrs", "T-bond 20+ yrs") #"T-bond 1-3 months"

ts_symb_vec <- c(spy_sector_symbs, minerals_symbs, agriculture_symbs, energy_symbs,
                 currency_symbs, emerg_mkt_symbs, crypto_symbs, Tbond_symbs)
ts_detail_vec <- c(spy_sector_detail, minerals_detail, agriculture_detail, energy_detail,
                   currency_detail, emerg_mkt_detail, crypto_detail, Tbond_detail)
df_symb_detail <- data.frame(symbol = ts_symb_vec, detail = ts_detail_vec)
#--
#length(ts_symb_vec)
#--
# list_groups <- list(spy_sector_symbs, minerals_symbs, agriculture_symbs, energy_symbs,
#                      currency_symbs, emerg_mkt_symbs, crypto_symbs, Tbond_symbs)
#-------------------------------------------
# Create grouping info for plots (required input into functions)
list_groups <- list(spy_sector_detail, minerals_detail, agriculture_detail, energy_detail,
                    currency_detail, emerg_mkt_detail, crypto_detail, Tbond_detail)
group_names <- c("US Sectors", "Minerals", "Agriculture", "Energy", "Major Currency Pairs",
                 "Emerging Markets", "Cryptocurrencies/Blockchain", "T-Bonds")
group_info <- list(list_groups, group_names)
n_groups <- length(group_names)
#-------------------------------------------
#per_ema <- 13
#fromdate = Sys.Date() - 360
fromdate = "2018-06-20"
todate = "2019-08-09"
tbl_ohlcv <- tq_get(ts_symb_vec, get = "stock.prices", from = fromdate, to = todate)
df_ohlcv <- as.data.frame(tbl_ohlcv)
#--
# Crypto ts can have duplicate date entries sometimes (BTC-USD)
# Get rid of these duplicates, if any
df_ohlcv <- as.data.frame(df_ohlcv %>% group_by(symbol) %>% mutate(dup = duplicated(date)))
df_ohlcv <- subset(df_ohlcv, dup == F)
df_ohlcv$dup <- NULL
#--

#df_ohlcv$p <- rowSums(df_ohlcv[, c(4:6)]) / 3
#--
#df <- df_ohlcv[, c("date", "symbol", "p")]
# df <- df_ohlcv %>% group_by(symbol) %>% tq_transmute(select = adjusted, 
#                      mutate_fun = periodReturn, 
#                      period     = "monthly")
#--
df <- df_ohlcv[, c("date", "symbol", "adjusted")]
df <- df %>% spread(symbol, adjusted)
ind_shift <- which(colnames(df) %in% currency_symbs)
df[, ind_shift] <- rbind(rep(NA, length(ind_shift)), df[-nrow(df), ind_shift])
df <- df[-c(1, nrow(df)), ]
date_vec <- df$date
mat_ts_dy <- na.approx(df[, -1])

#o <- apply(mat_ts_dy, 2, function(x) length(which(is.na(x))))
#table(o)
#which(o==1)
#xts_ts_dy <- xts(mat_ts_dy, date_vec)
#date_vec <- index(xts_ts_dy)
#-----
mat_pctDiff_dy <- diff(mat_ts_dy) / mat_ts_dy[-nrow(mat_ts_dy), ]
#mat_pctDiff_dy <- diff(log(mat_ts_dy))
row.names(mat_pctDiff_dy) <- as.character(date_vec[-1])
ts_avg_dy <- rowMeans(mat_pctDiff_dy)
mu_ret_dy <- colMeans(mat_pctDiff_dy)
sd_ret_dy <- apply(mat_pctDiff_dy, 2, sd)

mat_ts_wk <- to.weekly(xts(mat_ts_dy, date_vec), OHLC = F)
mat_pctDiff_wk <- diff(mat_ts_wk) / mat_ts_wk[-nrow(mat_ts_wk), ]
mat_pctDiff_wk <- as.matrix(mat_pctDiff_wk[-1, ])
ts_avg_wk <- rowMeans(mat_pctDiff_wk)
mu_ret_wk <- colMeans(mat_pctDiff_wk)
sd_ret_wk <- apply(mat_pctDiff_wk, 2, sd)

mat_ts_mo <- to.monthly(xts(mat_ts_dy, date_vec), OHLC = F)
mat_pctDiff_mo <- diff(mat_ts_mo) / mat_ts_mo[-nrow(mat_ts_mo), ]
mat_pctDiff_mo <- as.matrix(mat_pctDiff_mo[-1, ])
ts_avg_mo <- rowMeans(mat_pctDiff_mo)
mu_ret_mo <- colMeans(mat_pctDiff_mo)
sd_ret_mo <- apply(mat_pctDiff_mo, 2, sd)
#----------------------------------------------
# Compare histograms of daily, weekly, and monthly returns
# df_plot <- data.frame(ts_id = names(mu_ret_dy), mu_ret_dy, mu_ret_wk, mu_ret_mo)
# colnames(df_plot)[-1] <- c("Mean Daily Return", "Mean Weekly Return", "Mean Monthly Return")
# gathercols <- colnames(df_plot)[-1]
# df_plot <- df_plot %>% gather_("Return Type", "Value", gathercols)
# #gg <- ggplot(df_plot, aes(Value, fill = `Return Type`))
# gg <- ggplot(df_plot, aes(Value))
# gg <- gg + geom_density(alpha = 0.4)
# gg <- gg + facet_wrap(~`Return Type`, nrow = 1, scales = "free")
# #gg <- gg + coord_cartesian(xlim = c(-0.02, 0.02))
# gg
#----------------------------------------------
#ind_rm_ema <- 1:(per_ema - 1)
# mat_pctDiff <- apply(mat_pctDiff, 2, function(x) x - EMA(x, per_ema))
# mat_pctDiff <- mat_pctDiff[-ind_rm_ema, ]
# date_vec <- df$date[-c(ind_rm_ema, ind_rm_na)]
#----------------------------------------------
mat_ts_in <- mat_ts_dy
ts_avg_in <- ts_avg_dy
# nab_pctRet_in <- mu_ret_dy
# sd_ret_in <- sd_ret_dy
#----------------------------------------------
mat_pctDiff <- mat_pctDiff_dy
date_vec <- row.names(mat_pctDiff)
#length(date_vec)
row.names(mat_pctDiff) <- as.character(date_vec)
colnames(mat_pctDiff) <- as.character(df_symb_detail$detail[order(df_symb_detail$symbol)])
#----------------------------------------------
check_on_data <- F
if(check_on_data){
  df_plot <- as.data.frame(mat_pctDiff)
  gathercols <- colnames(df_plot)
  df_plot$Date <- date_vec
  df_plot <- df_plot %>% gather_("Security", "Weekly change (%)", gathercols)
  df_plot$Type <- NA
  df_plot$Type[which(df_plot$Security %in% spy_sector_detail)] <- "US Sectors"
  df_plot$Type[which(df_plot$Security %in% minerals_detail)] <- "Minerals"
  df_plot$Type[which(df_plot$Security %in% agriculture_detail)] <- "Agriculture"
  df_plot$Type[which(df_plot$Security %in% energy_detail)] <- "Energy"
  df_plot$Type[which(df_plot$Security %in% currency_detail)] <- "Major Currency Pairs"
  df_plot$Type[which(df_plot$Security %in% emerg_mkt_detail)] <- "Emerging Markets"
  df_plot$Type[which(df_plot$Security %in% crypto_detail)] <- "Cryptocurrencies/ \nBlockchain"
  df_plot$Type[which(df_plot$Security %in% Tbond_detail)] <- "T-bonds"
  #------------------------------------------------------------
  # gg <- ggplot(df_plot, aes(x = Year, y = Value, group = Security, color = Security))
  # gg <- gg + geom_line()
  # gg <- gg + facet_wrap(~ Type, nrow = 3, scales = "free")
  # gg <- gg + theme(legend.position = "none")
  # gg
  #------------------------------------------------------------
  this_type <- "US Sectors"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Minerals"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Agriculture"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Energy"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Major Currency Pairs"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Emerging Markets"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Cryptocurrencies/Blockchain"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "T-bonds"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
}

```

# An example from the financial context

Before exploring the possibility of applying these tools to the AR4D context, it will be instructive to first apply them in the financial context where they have been developed. Below, daily financial data for 46 securities over the period 20 June 2018 to 8 August 2019 is downloaded directly into R from yahoo finance using the tidyquant package. The 46 securities have been chosen so as to be representative of the radar screen of a modern, broad minded investor (Table 1). They are mostly exchange traded funds tracking broad categories of stocks, bonds, markets, and commodities, plus the major currency pair exchange rates traded on the FOREX market.

[Table here detailing the ETF names, what they track, and my category name]


```{r returns, fig.cap="Historical returns, train and backtest datasets", fig.width=10, echo=FALSE}
#fig.width=11, fig.height=6,
ind_train <- 1:round(nrow(mat_pctDiff) * 2 / 3)
ind_test <- setdiff(1:nrow(mat_pctDiff), ind_train)
mat_pctDiff_train <- mat_pctDiff[ind_train, ]
mat_pctDiff_test <- mat_pctDiff[ind_test, ]
# ts_avg_test <- ts_avg_in[ind_test]
# date_vec_test <- date_vec[ind_test]
#group_colors <- RColorBrewer::brewer.pal(n = n_groups, "Dark2")
# "Darjeeling"
group_colors <- wesanderson::wes_palette("Darjeeling1", n = n_groups, type = "continuous")
#this_fig_title <- "Figure 1: Historical returns"
this_fig_title <- NULL
historical_returns_and_corr_plot(mat_pctDiff_train, mat_pctDiff_test, group_info,
                                 returns_plot = T,
                                 corr_plot = F,
                                 group_colors = group_colors,
                                 fig_title_returns = this_fig_title)


```

The dataset is large enough to allow for backtesting of the optimal portfolio. The portfolio is optimized using the first two thirds of the data (the "train" data), and then backtested against the remaining third. Historical returns and correlation matrices are presented for these two datasets in Figures \ref{fig:returns} and \ref{fig:corrplots}. 

```{r corrplots, fig.cap=c("Correlation matrix, train data", "Correlation matrix, test data"), fig.width=10, fig.height=10, fig.align='center', echo=FALSE}
#fig.width=12, fig.height=12,

# (mat_pctDiff,
#                                              mat_pctDiff_test = NULL,
#                                              group_info = NULL,
#                                              returns_plot = F,
#                                              corr_plot = F,
#                                              fig_title_returns = NULL,
#                                              fig_title_corrplot = NULL,
#                                              fig_title_corrplot_test = NULL,
#                                              returns_plot_range = NULL,
#                                              corrplot_options = list(
#                                                plot_with_pvals = F,
#                                                plot_with_corrCoefs = F,
#                                                corr_coef_size = 0.75)
# )

corrplot_options = list(plot_with_pvals = F, plot_with_corrCoefs = F, corr_coef_size = 0.75)
# fig_title_corrplot <- "Figure 2: Correlation matrix, train data"
# fig_title_corrplot_test <- "Figure 3: Correlation matrix, test data"
fig_title_corrplot <- NULL
fig_title_corrplot_test <- NULL
historical_returns_and_corr_plot(mat_pctDiff_train, mat_pctDiff_test, group_info,
                                 returns_plot = F, 
                                 corr_plot = T, 
                                 group_colors = group_colors, 
                                 fig_title_corrplot = fig_title_corrplot,
                                 fig_title_corrplot_test = fig_title_corrplot_test,
                                 corrplot_options = corrplot_options)



```


In Figure \ref{fig:eigdensplot}, a density plot of the correlation matrix eigenvalues is compared against one of eigenvalues derived from a random matrix. In this plot, it is evident that most eigenvalues are small and cannot be distinguished from noise, but six of them extend beyond the random matrix eigenvalue density plot. These correspond to the eigenvectors that can be meaningfully distinguished from noise.

```{r eigdensplot, fig.cap="Eigenvalue density plots for the financial data correlation matrix and a\ncorrelation matrix of a random dataset of the same dimensions.", fig.align='center'}
fun_env = list(eigenvalue_density_plot = T,
               pca_var_plot = F,
               pca_ind_plot = F,
               group_info,
               quietly = F)
# this_fig_title <- "Figure 4: Eigenvalue density plots for the financial data correlation matrix and a\ncorrelation matrix of a random dataset of the same dimensions."
this_fig_title <- NULL
list_out <- signals_from_noise(mat_pctDiff_train, fig_title_eigDens = this_fig_title, fun_env)
# list_out <- list(mat_loads_sig, mat_loads_rot_sig, mat_loads, mat_loads_rot, mat_inData_sig, eigvals_sig, mat_eigvecs_sig, eigvals, mat_eigvecs)
mat_loads_sig_train <- list_out[[1]]
mat_loads_rot_sig_train <- list_out[[2]]
mat_pctDiff_sig_train <- list_out[[5]]
mat_eigvecs_sig_train <- list_out[[7]]
```




















blah blah...

\begin{equation}
S = X \bar{P}
\label{eq:this_eq}
\end{equation}

For a long time now, AR4D centers have been under increasing pressure to "do a lot more with a lot less" [@Alston1995]

\begin{equation}
1+1=2
\end{equation}


## Later on
blah blah remember that in equation (\ref{eq:this_eq})



The Elsevier article class
==========================

#### Installation

If the document class *elsarticle* is not available on your computer,
you can download and install the system package *texlive-publishers*
(Linux) or install the LaTeX package *elsarticle* using the package
manager of your TeX installation, which is typically TeX Live or MikTeX.

#### Usage

Once the package is properly installed, you can use the document class
*elsarticle* to create a manuscript. Please make sure that your
manuscript follows the guidelines in the Guide for Authors of the
relevant journal. It is not necessary to typeset your manuscript in
exactly the same way as an article, unless you are submitting to a
camera-ready copy (CRC) journal.

#### Functionality

The Elsevier article class is based on the standard article class and
supports almost all of the functionality of that class. In addition, it
features commands and options to format the

-   document style

-   baselineskip

-   front matter

-   keywords and MSC codes

-   theorems, definitions and proofs

-   lables of enumerations

-   citation style and labeling.

Front matter
============

The author names and affiliations could be formatted in two ways:

(1) Group the authors per affiliation.

(2) Use footnotes to indicate the affiliations.

See the front matter of this document for examples. You are recommended
to conform your choice to the journal you are submitting to.

Bibliography styles
===================

There are various bibliography styles available. You can select the
style of your choice in the preamble of this document. These styles are
Elsevier styles based on standard styles like Harvard and Vancouver.
Please use BibTeXÂ to generate your bibliography and include DOIs
whenever available.


References {#references .unnumbered}
==========
